[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html",
    "href": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html",
    "title": "1 A Layered Grammar of Graphics: ggplot2 methods",
    "section": "",
    "text": "Published: 14-Apr-2023"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#learning-outcome",
    "href": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#learning-outcome",
    "title": "1 A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.1 Learning Outcome",
    "text": "1.1 Learning Outcome\nWe will:\n\nlearn the basic principles and components of ggplot2\ngain hands-on experience plotting functional graphs ‚úåÔ∏è"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#getting-started",
    "href": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#getting-started",
    "title": "1 A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.2 Getting Started",
    "text": "1.2 Getting Started\n\n1.2.1 Install and load the required r libraries\nLoad the tidyverse library. tidyverse is a collection of powerful and popular packages, such as ggplot2, dplyr, in R that are designed to help us work with and manipulate data in a consistent and efficient manner.\n\n\nShow the code\npacman::p_load(tidyverse)\n\n\n\n\n1.2.2 Import the data\nWe import the exam_data.csv data-set. This data-set contains the examination scores of a cohort of Primary 3 students from a local school.\nThere are a total of seven attributes.\n\nThe categorical attributes are: ID, CLASS, GENDER and RACE.\nThe continuous attributes are: MATHS, ENGLISH and SCIENCE.\n\n\n\nShow the code\nexam_data <- read_csv('data/Exam_data.csv', show_col_types = FALSE )"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#introduction-to-ggplot",
    "href": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#introduction-to-ggplot",
    "title": "1 A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.3 Introduction to ggplot",
    "text": "1.3 Introduction to ggplot\nggplot is an R package for decoratively creating data-driven graphics based on The Grammar of Graphics.\n\n1.3.1 R Base Graphics Vs ggplot\nFirst, let us compare how R Graphics, the core graphical functions of Base R and ggplot plot a simple histogram.\n\nR Base Graphicsggplot\n\n\n\n\nShow the code\nhist(exam_data$MATHS)\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(data=exam_data, aes(x = MATHS)) +\n  geom_histogram(bins=10, \n                 boundary = 100,\n                 color=\"black\", \n                 fill=\"grey\") +\n  ggtitle(\"Distribution of Maths scores\") + \n  ylab('Frequency') +\n  xlab('Maths score') +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nBase graphics has a pen on paper model: we can only draw on top of the plot, we cannot modify or delete existing content. There is no (user accessible) representation of the graphics, apart from their appearance on the screen. Base graphics includes both tools for drawing primitives and entire plots. Base graphics functions are generally fast, but have limited scope.\nOn the other hand, ggplot2 has an underlying grammar, based on the Grammar of Graphics (see sections below), that allows us to compose graphs by combining independent components. This makes ggplot2 powerful. Rather than being limited to sets of pre-defined graphics, we can create novel graphics that are tailored to our specific problem."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#grammar-of-graphics",
    "href": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#grammar-of-graphics",
    "title": "1 A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.4 Grammar of Graphics",
    "text": "1.4 Grammar of Graphics\nGrammar of Graphics is a general scheme for data visualization which breaks up graphs into semantic components such as scales and layers. It was introduced by Leland Wilkinson (1999) .\nIn brief, the grammar tells us that a graphic maps the data to the aesthetic attributes (colour, shape, size) of geometric objects (points, lines, bars). The plot may also include statistical transformations of the data and information about the plot‚Äôs coordinate system. Facetting can be used to plot for different subsets of the data. The combination of these independent components are what make up a graphic.\n\n1.4.1 A Layered Grammar of Graphics\nThe 7 layers are:\n\n\n\n\n\n\nData: Refers to the data-set being plotted\nAesthetics: Use the attributes of the data to influence visual characteristics, such as position, colours, size, shape, or transparency.\nGeometrics: Represent our data using visual elements such as points, bar or line.\nFacets: Split the data into subsets to create small multiples of the same graph (paneling, multiple plots).\nStatistics:Apply additional statistical transformations that summarise the data (e.g.¬†mean, confidence intervals).\nCoordinates: Define the pane on which data is mapped on the graphic.\nTheme: Modify all non-data components of a plot, such as main title, sub-title, y-aixs title, legend, and background.\n\nThe purpose of each layer (or component) is further discussed below."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#the-data-layer",
    "href": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#the-data-layer",
    "title": "1 A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.5 The Data Layer",
    "text": "1.5 The Data Layer\nLet us call the ggplot() function, with data argument pointing to the data-set to be used for plotting.\n\n\nShow the code\nggplot(data=exam_data)\n\n\n\n\n\nUnder the hood, a ggplot object is initialized using the data provided. We will need to include 2 other key layers - the aesthetic mappings and geometric layer - to see the plot.\n\n\n\n\n\n\nNote\n\n\n\nIf the data-set is not already a data.frame, it will be converted to one by using the fortify() function."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#the-aesthetic-layer",
    "href": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#the-aesthetic-layer",
    "title": "1 A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.6 The Aesthetic Layer",
    "text": "1.6 The Aesthetic Layer\nThe aesthetic mappings take attributes of the data and and use them to influence the visual characteristics, such as position, colour, size, shape, or transparency, of the plot.\nAll aesthetics of a plot are specified in the aes() function call. In the later part of this document, we will see that each geom layer can have its own aes specification.\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x= MATHS))\n\n\n\n\n\nThe tick marks and label for the x-axis are displayed."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#the-geometric-layer",
    "href": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#the-geometric-layer",
    "title": "1 A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.7 The Geometric Layer",
    "text": "1.7 The Geometric Layer\nGeometric objects are the actual marks we put on a plot. Examples include:\n\ngeom_point for drawing individual points (e.g., a scatter plot)\ngeom_line for drawing lines (e.g., for a line charts)\ngeom_smooth for drawing smoothed lines (e.g., for simple trends or approximations)\ngeom_bar for drawing bars (e.g., for bar, column charts)\ngeom_histogram for drawing binned values (e.g.¬†a histogram)\ngeom_polygon for drawing arbitrary shapes\ngeom_map for drawing polygons in the shape of a map! (we can access the data to use for these maps by using the map_data() function).\n\n A plot must have at least one geom; there is no upper limit. We can add a geom to a plot using the ‚Äô+‚Äô operator.\n\n1.7.1 Geometric Object: geom_bar\nYup, it‚Äôs a for a bar chart!\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\n\n\n\n1.7.2 Geometric Object: geom_dotplot\nIn a dot plot, the width of a dot corresponds to the bin width (or maximum width, depending on the binning algorithm), and dots are stacked.\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(dotsize = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nDanger\n\n\n\nThe y-axis is not very useful and can be misleading.\n\n\nTo address the above concern, we take the following steps:\n\nscale_y_continuous() is used to turn off the y-axis, and\nbinwidth argument is used to change the binwidth to 2.5.\n\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(binwidth=2.5,         \n               dotsize = 0.5) +      \n  scale_y_continuous(NULL,           \n                     breaks = NULL)  \n\n\n\n\n\n\n\n1.7.3 Geometric Object: geom_histogram\ngeom_histogram() is used to create a simple histogram by using values in MATHS field of exam_data.\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_histogram()       \n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the default bin is 30.\n\n\n\n\n1.7.4 Modify a geometric object by changing geom()\nThe following arguments of the geom() function can be used:\n\nbins argument is used to change the number of bins to 20,\nfill argument is used to shade the histogram with light blue color, and\ncolor argument is used to change the outline colour of the bars in black\n\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20,            \n                 color=\"black\",      \n                 fill=\"light blue\")  \n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThese changes are applied at the specific geom() layer and will not perpertuate when we include another geom() layer.\n\n\n\n\n\n1.7.5 Modify a geometric object by changing aes()\nWe can changes the interior colour of the histogram (i.e.¬†fill) by using sub-group of aes().\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x= MATHS, \n           fill = GENDER)) +\n  geom_histogram(bins=20, \n                 color=\"grey30\")\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nggplot2 takes care of the details of converting data into aesthetics (e.g., ‚Äòred‚Äô, ‚Äòyellow‚Äô, ‚Äògreen‚Äô) with a scale. There is one scale for each aesthetic mapping in a plot. The scale is also responsible for creating a guide, an axis or legend, that allows us to read the plot, converting aesthetic values back into data values.\nIf we want to set an aesthetic to a fixed value, without scaling it, do so in the individual layer outside of aes(). Refer to section 1.7.4.\n\n\n\n\n\n1.7.6 Geometric Object: geom_density\ngeom-density() computes and plots kernel density estimate, which is a smoothed version of the histogram.\nIt is a useful alternative to the histogram for continuous data that comes from an underlying smooth distribution.\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_density()           \n\n\n\n\n\nWe can plot two kernel density lines by using colour or fill arguments of aes().\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x = MATHS, \n           colour = GENDER)) +\n  geom_density()\n\n\n\n\n\n\n\n1.7.7 Geometric Object: geom_boxplot\ngeom_boxplot() displays continuous value list. It visualises five summary statistics (the median, two hinges and two whiskers), and all ‚Äúoutlying‚Äù points individually.\n\n\nShow the code\nggplot(data=exam_data, \n       aes(y = MATHS,       \n           x= GENDER)) +    \n  geom_boxplot()            \n\n\n\n\n\nNotches are indentation on the box-plot at the median value to help visually assess whether the medians of distributions differ. The notch indicates a confidence interval around the median, calculated using the median absolute deviation. If the notches of two box plots do not overlap, it suggests that the medians of the two groups are significantly different. We can show the indentation using the notch argument.\n\n\nShow the code\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot(notch=TRUE)\n\n\n\n\n\n\n\n1.7.8 Geometric Object: geom_violin\ngeom_violin is designed for creating violin plot. With ordinary density curves (see section 1.7.6), it is difficult to compare more than just a few distributions because the lines visually interfere with each other. With a violin plot, it‚Äôs easier to compare several distributions since they‚Äôre placed side by side.\n\n\nShow the code\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_violin()\n\n\n\n\n\n\n\n1.7.9 Geometric Object: geom_point\ngeom_point() is especially useful for creating scatterplot.\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point()            \n\n\n\n\n\n\n\n1.7.10 Combine several geom objects\nFor instance, we can plot the data points on the boxplots by using both geom_boxplot() and geom_point().\n\n\nShow the code\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() +                    \n  geom_point(position=\"jitter\", \n             size = 0.5)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#the-statistics-layer",
    "href": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#the-statistics-layer",
    "title": "1 A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.8 The Statistics Layer",
    "text": "1.8 The Statistics Layer\nThe Statistics functions statistically transform data, usually as some form of summary. For example:\n\nfrequency of values of a variable (bar graph)\n\na mean\na confidence limit\n\nThere are two ways to use these functions:\n\nadd a stat_() function and override the default geom, or\nadd a geom_() function and override the default stat.\n\n\n\n1.8.1 Working with the stat() function\nWe can use stat_summary() function to include the mean value on a boxplot.\n\n\nShow the code\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  stat_summary(geom = \"point\",       \n               fun =\"mean\",         \n               colour =\"red\",        \n               size=4)               \n\n\n\n\n\n\n\n1.8.2 Working with the geom() function\nWe can also use the geom() function to get the same result.\n\n\nShow the code\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\",        \n             fun=\"mean\",           \n             colour =\"red\",          \n             size=4)          \n\n\n\n\n\n\n\n1.8.3 Add a best fit curve on a scatter-plot\nThe scatter-plot below shows the relationship of Maths and English grades of pupils. The interpretability of this graph can be improved by adding a best fit curve using the geom_smooth() function.\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(linewidth=0.5)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe default smoothing method used is loess (short for ‚Äúlocally weighted scatter-plot smoothing‚Äù). The loess method involves fitting a smooth curve to a scatter-plot of data points, where the curve is weighted to give more emphasis to nearby points and less emphasis to points that are far away.\n\n\nThe default smoothing method can be overridden as shown below. The ‚Äúlm‚Äù method can be used to fit a straight line to a scatterplot of data points. This line represents the best linear approximation of the relationship between the variables and can be used to make predictions or estimate the effect of one variable on the other.\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#the-facet-layer",
    "href": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#the-facet-layer",
    "title": "1 A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.9 The Facet Layer",
    "text": "1.9 The Facet Layer\nFaceting generates small multiples (sometimes also called trellis plot), each displaying a different subset of the data. They are an alternative to aesthetics for displaying additional discrete variables. ggplot2 supports two types of factes, namely: facet_grid() and facet_wrap().\n\n1.9.1 Working with facet_wrap()\nfacet_wrap wraps a 1-d sequence of panels into 2-d.¬†This is generally a better use of screen space than facet_grid because most displays are roughly rectangular.\nLet‚Äôs do a trellis plot using facet-wrap() for the maths score of each class.\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_wrap(~ CLASS)\n\n\n\n\n\n\n\n1.9.1 Working with facet_grid()\nfacet_grid() forms a matrix of panels defined by row and column facetting variables. It is most useful when we have two discrete variables, and all combinations of the variables exist in the data.\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n  facet_grid(CLASS~.) +\n  theme(plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), \"cm\"))\n\n\n\n\n\nFrom the above, it‚Äôs now more apparent that the maths scores decrease as we move down from Class 3A to Class 3I."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#the-coordinates-layer",
    "href": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#the-coordinates-layer",
    "title": "1 A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.10 The Coordinates Layer",
    "text": "1.10 The Coordinates Layer\nThere are two types of coordinate systems. Linear coordinate systems preserve the shape of geoms:\n\ncoord_cartesian(): the default Cartesian coordinate system, where the 2-d position of an element is given by the combination of the x and y positions.\ncoord_flip(): Cartesian coordinate system with x and y axes flipped.\ncoord_fixed(): Cartesian coordinate system with a fixed aspect ratio.\n\nOn the other hand, non-linear coordinate systems can change the shapes: a straight line may no longer be straight. The closest distance between two points may no longer be a straight line.\n\ncoord_map()/coord_quickmap()/coord_sf(): Map projections.\ncoord_polar(): Polar coordinates.\ncoord_trans(): Apply arbitrary transformations to x and y positions, after the data has been processed by the stat.\n\n\n1.10.1 Working with Coordinates\nBy the default, the bar chart of ggplot2 is in vertical form (i.e.¬†column chart).\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\n\nWe can flip the horizontal bar chart into vertical bar chart by using coord_flip().\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\n\n1.10.2 Change the y- and x-axis range\nWe can use the coord_caatesian() function to fix both the y-axis and x-axis range from 0-100.\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#the-theme-layer",
    "href": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#the-theme-layer",
    "title": "1 A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.11 The Theme Layer",
    "text": "1.11 The Theme Layer\nThemes control elements of the graph not related to the data. For example:\n\nbackground colour\nsize of fonts\ngridlines\nlabels colour\n\nBuilt-in themes include: - theme_gray() (default) - theme_bw() - theme_classic()\nA list of theme can be found at this link. Each theme element can be conceived of as either a line (e.g.¬†x-axis), a rectangle (e.g.¬†graph background), or text (e.g.¬†axis title).\n\n1.11.1 Working with theme\nA horizontal bar chart plotted using various themes.\ntheme_classic() is my favourite! üòú\n\ntheme_gray()theme_bw()theme_classic()theme_minimal()\n\n\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_gray()\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal()"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#reference",
    "href": "Hands-On_Ex/Hands-On_Ex01/Hands-On_Ex1.html#reference",
    "title": "1 A Layered Grammar of Graphics: ggplot2 methods",
    "section": "1.12 Reference",
    "text": "1.12 Reference\n\nHadley Wickham (2023) ggplot2: Elegant Graphics for Data Analysis. Online 3rd edition.\nWinston Chang (2013) R Graphics Cookbook 2nd edition. Online version.\nHealy, Kieran (2019) Data Visualization: A practical introduction. Online version\nLearning ggplot2 on Paper ‚Äì Components\nLearning ggplot2 on Paper ‚Äì Layer\nLearning ggplot2 on Paper ‚Äì Scale\n\n\n\\(**That's\\) \\(all\\) \\(folks!**\\)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex02/Hands-On_Ex2.html",
    "href": "Hands-On_Ex/Hands-On_Ex02/Hands-On_Ex2.html",
    "title": "2 Beyond ggplot2 Fundamentals",
    "section": "",
    "text": "Published: 20-Apr-2023\nModified: 27-Apr-2023"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex02/Hands-On_Ex2.html#learning-outcome",
    "href": "Hands-On_Ex/Hands-On_Ex02/Hands-On_Ex2.html#learning-outcome",
    "title": "2 Beyond ggplot2 Fundamentals",
    "section": "2.1 Learning Outcome",
    "text": "2.1 Learning Outcome\nWe will learn to plot charts that are beyond the out-of-the-box offerings from ggplot2. We will explore how to customize and extend ggplot2 with new:\n\nAnnotations\nThemes\nComposite plots"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex02/Hands-On_Ex2.html#getting-started",
    "href": "Hands-On_Ex/Hands-On_Ex02/Hands-On_Ex2.html#getting-started",
    "title": "2 Beyond ggplot2 Fundamentals",
    "section": "2.2 Getting Started",
    "text": "2.2 Getting Started\n\n2.2.1 Install and load the required r libraries\nInstall and load the the required R packages. The name and function of the new packages that will be used for this exercise are as follow:\n\nggrepel: provides a way to prevent labels from overlapping in ggplot2 plots\nggthemes: provides a set of additional themes, geoms and scales for ggplot2\nhrbrthemesüëçüèæ: provides another set of visually appealing themes and formatting options for ggplot2\npatchworküëçüèæ: provides a way to combine multiple ggplot2 plots into a single figure\n\n\n\nShow the code\npacman::p_load(tidyverse, patchwork, \n               ggthemes, hrbrthemes,\n               ggrepel)\n\n\n\n\n2.2.2 Import the data\nWe will be using the same exam scores data-set that was featured in my Hands-On Ex 1.\n\n\nShow the code\nexam_data <- read_csv('data/Exam_data.csv', show_col_types = FALSE )"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex02/Hands-On_Ex2.html#beyond-ggplot2-annotation",
    "href": "Hands-On_Ex/Hands-On_Ex02/Hands-On_Ex2.html#beyond-ggplot2-annotation",
    "title": "2 Beyond ggplot2 Fundamentals",
    "section": "2.3 Beyond ggplot2 Annotation",
    "text": "2.3 Beyond ggplot2 Annotation\nOne challenge in plotting statistical graph is annotation, especially with large number of data points. The data points overlap and this leads to an ugly chart.\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\n2.3.1 Working with ggrepel package\nggrepel is an extension of ggplot2 package which provides geoms for ggplot2 to repel overlapping text. We simply replace geom_text() by geom_text_repel() and geom_label() by geom_label_repel().\ngeom_text_repel() adds text directly to the plot. geom_label_repel() draws a rectangle underneath the text, making it easier to read.\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\") +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex02/Hands-On_Ex2.html#beyond-ggplot2-themes",
    "href": "Hands-On_Ex/Hands-On_Ex02/Hands-On_Ex2.html#beyond-ggplot2-themes",
    "title": "2 Beyond ggplot2 Fundamentals",
    "section": "2.4 Beyond ggplot2 Themes",
    "text": "2.4 Beyond ggplot2 Themes\nggplot2 comes with eight built-in themes, they are: theme_gray(), theme_bw(), theme_classic(), theme_dark(), theme_light(), theme_linedraw(), theme_minimal(), and theme_void(). 4 of these themes were featured in my Hands-On-Ex1 page.\n\n\nShow the code\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_gray() +  \n  ggtitle(\"Distribution of Maths scores [theme_gray()]\") \n\n\n\n\n\n\n\n\n\n\n\nFor facet or small multiple plots\n\n\n\nConsider using theme_gray(), theme_bw() or theme_light() as they offer bounded axis which helps to compartmentalize the different plots.\n\n\n\n2.4.1 Working with ggthemes package\nggthemes provides ‚Äòggplot2‚Äô themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, ‚ÄòStata‚Äô, ‚ÄòExcel‚Äô, and The Wall Street Journal, among others.\nCheck out some of the available themes below üëáüèº.\n\nTufteEconomist(Stephen) Few üëçüèæFivethirtyeightGDocs üëçüèæHighcharts üëçüèæ\n\n\n\n\nShow the code\np1 <- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\np1 + theme_tufte()\n\n\n\n\n\n\n\n\n\nShow the code\np1 +theme_economist() + scale_colour_economist()\n\n\n\n\n\n\n\n\n\nShow the code\np1 + theme_few() + scale_colour_few()\n\n\n\n\n\n\n\n\n\nShow the code\np1 + scale_color_fivethirtyeight(\"cyl\") + theme_fivethirtyeight()\n\n\n\n\n\n\n\n\n\nShow the code\np1 + theme_gdocs() + scale_color_gdocs()\n\n\n\n\n\n\n\n\n\nShow the code\np1 + theme_hc() + scale_colour_hc()\n\n\n\n\n\n\n\n\nThe package also provides some extra geoms and scales for ‚Äòggplot2‚Äô. Consult this vignette to learn more.\n\n\n2.4.2 Working with hrbthems package\nhrbrthemes package provides a base theme that focuses on typographic elements, including where various labels are placed as well as the fonts that are used.\n\n\nShow the code\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\n\n\n\n\nThe second goal centers around productivity for a production workflow. In fact, this ‚Äúproduction workflow‚Äù is the context for where the elements of hrbrthemes should be used. It allows us, the data analysts, to focus on the analysis while the package works behind the scene to produce an elegant chart. Consult this vignette to learn more.\n\n\nShow the code\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18, \n              base_size = 15, \n              grid = \"Y\") \n\n\n\n\n\n\n\n\n\n\n\nTweaking the arguments in the theme_ipsum() function\n\n\n\n\naxis_title_size argument is used to increase the font size of the axis title to 18,\nbase_size argument is used to increase the default axis label to 15, and\ngrid argument is used to remove the x-axis grid lines."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex02/Hands-On_Ex2.html#beyond-ggplot2-facet",
    "href": "Hands-On_Ex/Hands-On_Ex02/Hands-On_Ex2.html#beyond-ggplot2-facet",
    "title": "2 Beyond ggplot2 Fundamentals",
    "section": "2.5 Beyond ggplot2 facet",
    "text": "2.5 Beyond ggplot2 facet\nIt is not unusual that multiple graphs are required to tell a compelling visual story. There are several ggplot2 extensions provide functions to compose a figure with multiple graphs. In this section, we will learn how to create a composite plot by combining multiple graphs. First, let us create three statistical graphs.\n\n\nShow the code\np1 <- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\np2 <- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\np3 <- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\np1\n\n\n\n\n\nShow the code\np2\n\n\n\n\n\nShow the code\np3\n\n\n\n\n\n\n2.5.1 Creating Composite Graphics with pathwork package\nThere are several ggplot2 extensions which provide functions to compose figure with multiple graphs. In this section, we are going to explore patchwork.\nThe patchwork package has a very simple syntax where we can create layouts super easily.\n\nBasic layout: Placing 2 plots side-by-sideDefault layout: Grid SquareChange layout: use plot_layout() function\n\n\n\n\nShow the code\np1 + p2\n\n\n\n\n\n\n\n\n\nShow the code\np1 + p2 + p3 + p1\n\n\n\n\n\n\n\n\n\nShow the code\np1 + p2 + p3 + plot_layout(nrow = 3, byrow = FALSE)\n\n\n\n\n\n\n\n\nWe can use | to place the plots beside each other, while / will stack them\n\n\nShow the code\np1 + p2 / p3\n\n\n\n\n\n\n\nShow the code\n(p1 / p2) | p3\n\n\n\n\n\npatchwork also provides auto-tagging capabilities, in order to identify subplots in text\n\n\nShow the code\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = '1')\n\n\n\n\n\nWe can apply themes to the charts\n\n\nShow the code\npatchwork <- (p1 / p2) | p3\npatchwork & theme_economist()\n\n\n\n\n\nBeside providing functions to place plots next to each other based on the provided layout. With inset_element() of patchwork, we can place one or several plots or graphic elements freely on top of another plot.\n\n\nShow the code\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1.)\n\n\n\n\n\nRefer to Plot Assembly to learn more about arranging charts using patchwork."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex02/Hands-On_Ex2.html#references",
    "href": "Hands-On_Ex/Hands-On_Ex02/Hands-On_Ex2.html#references",
    "title": "2 Beyond ggplot2 Fundamentals",
    "section": "2.6 References",
    "text": "2.6 References\n\nPatchwork R package goes nerd viral\nggrepel\nggthemes\nhrbrthemes\nggplot tips: Arranging plots\nggplot2 Theme Elements Demonstration\nggplot2 Theme Elements Reference Sheet\n\n\n\\(**That's\\) \\(all\\) \\(folks!**\\)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex03/Hands-On_Ex3a.html",
    "href": "Hands-On_Ex/Hands-On_Ex03/Hands-On_Ex3a.html",
    "title": "3 Interactive Data Visualisation with R",
    "section": "",
    "text": "Published: 25-Apr-2023"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex03/Hands-On_Ex3a.html#learning-outcome",
    "href": "Hands-On_Ex/Hands-On_Ex03/Hands-On_Ex3a.html#learning-outcome",
    "title": "3 Interactive Data Visualisation with R",
    "section": "3.1 Learning Outcome",
    "text": "3.1 Learning Outcome\nWe will learn to create interactive visuals"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex03/Hands-On_Ex3a.html#getting-started",
    "href": "Hands-On_Ex/Hands-On_Ex03/Hands-On_Ex3a.html#getting-started",
    "title": "3 Interactive Data Visualisation with R",
    "section": "3.2 Getting Started",
    "text": "3.2 Getting Started\n\n3.2.1 Install and load the required r libraries\nInstall and load the the required R packages. The name and function of the new packages that will be used for this exercise are as follow:\n\nggiraph: allows interactive graphics to be created using ggplot2\nplotly: creates interactive, web-based graphs using the Plotly.js JavaScript library\nDT: creates interactive tables using the DataTables JavaScript library. It allows data to be displayed in a table with features such as sorting, filtering, pagination, and searching.\n\n\n\nShow the code\npacman::p_load(tidyverse, patchwork, \n               ggiraph, plotly,\n              DT)\n\n\n\n\n3.2.2 Import the data\nWe will be using the same exam scores data-set that was featured in my Hands-On Ex 1.\n\n\nShow the code\nexam_data <- read_csv('data/Exam_data.csv', show_col_types = FALSE )"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex03/Hands-On_Ex3a.html#interactive-data-visualisation",
    "href": "Hands-On_Ex/Hands-On_Ex03/Hands-On_Ex3a.html#interactive-data-visualisation",
    "title": "3 Interactive Data Visualisation with R",
    "section": "3.3 Interactive Data Visualisation",
    "text": "3.3 Interactive Data Visualisation\n\n3.3.1 Working with ggiraph package\nggiraph is an htmlwidget and a ggplot2 extension. In ggplot2, interactivity can be added to a plot through the use of ggplot geometries. There are 3 arguments that can be used to enable interactivity in ggplot geometries:\n\ntooltip: To display a tooltip when the mouse is hovered over elements of the plot. Tooltips can be customized to include any information that is relevant to the plot.\nonclick: To specify an embedded JavaScript function that is executed when chart element is clicked.\ndata_id: To link a graph element to a record via a data_id. The data_id is a unique identifier for each record in the data, and it is used to enable interactivity between the plot and other components of the application.\nWhen used within a Shiny application, elements associated with an id (data_id) can be selected and manipulated on the client and server. The list of selected values will be stored in in a reactive value named [shiny_id]_selected.\n\n\n3.3.1.1 The tooltip argument\nFirst, an interactive version of ggplot2 geom (i.e.¬†geom_dotplot_interactive()) will be used to create the basic graph. Next, girafe() of ggiraph is used to create an interactive Scalable Vector Graphics (SVG) object to display the plot on a html page.\n\n\n\n\n\n\nWhat is a SVG?\n\n\n\nSVG is an XML-based format that is commonly used for web graphics and interactive visualizations because it allows graphics to be resized without losing quality.\n\n\nüëáInteractivity: By hovering the mouse pointer on an data point of interest, the student‚Äôs ID will be displayed.\n\n\nShow the code\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    #  Specifies that the dots will be stacked on top of each other when they overlap\n    stackgroups = TRUE, \n    binwidth = 1,\n    # Specifies the method for plotting the dot plot\n    method = \"histodot\") +\n  # NULL: Specifies that the y-axis label will be blank.\n  # breaks = NULL: Specifies that no tick marks will be displayed on the y-axis\n  scale_y_continuous(NULL, \n                     breaks = NULL)\n\ngirafe(\n  # Specifies the ggplot2 plot p that will be converted to an interactive plot using girafe\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\n\nDisplay more information on tooltip\nHere, we included the student‚Äôs class in the tooltip.\nThe first three lines of codes below create a new field called tooltip. Text data in the ID and CLASS fields are assigned to the newly created field. Next, this newly created field is used as tooltip field as shown in the code of line 8.\nüëáInteractivity: The student‚Äôs ID and class are not displayed in the tooltip.\n\n\nShow the code\nexam_data$tooltip <- c(paste0(     #<<\n  \"Name = \", exam_data$ID,         #<<\n  \"\\n Class = \", exam_data$CLASS)) #<<\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), #<<\n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\n\n\n\n\n\nCustomise Tooltip style\nWe can use opts_tooltip() of ggiraph to customize tooltip rendering by add css declarations.\nüëáCheck out the formatting style of the tooltip of the 2 sample charts below ,\n\nexam_datamtcars data\n\n\n\n\nShow the code\ntooltip_css <- \"background-color:white; #<<\nfont-style:bold; color:black;\" #<<\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #<<\n    opts_tooltip(    #<<\n      css = tooltip_css)) #<<\n)                             \n\n\n\n\n\n\n\n\n\n\nShow the code\ndataset <- mtcars\ndataset$carname = row.names(mtcars)\n\ngg <- ggplot(\n  data = dataset,\n  mapping = aes(x = wt, y = qsec, color = disp,\n                tooltip = carname, data_id = carname) ) +\n  geom_point_interactive() + theme_minimal()\n\nx <- girafe(ggobj = gg)\nx <- girafe_options(x,\n  opts_tooltip(opacity = .7,\n    offx = 20, offy = -10,\n    use_fill = TRUE, use_stroke = TRUE,\n    delay_mouseout = 1000) )\nx\n\n\n\n\n\n\n\n\n\nRefer to this page to learn more about how to customise girafe animations\nDisplay statistics on tooltip\nIn the following example, a function is used to compute the standard error of the mean math scores. The derived statistics are then displayed in the tooltip.\nüñ±Ô∏èMouse over the chart to check out the tooltip!\n\n\nShow the code\ntooltip <- function(y, ymax, accuracy = .01) {   #<<\n  mean <- scales::number(y, accuracy = accuracy) #<<\n  sem <- scales::number(ymax - y, accuracy = accuracy) #<<\n  paste(\"Mean maths scores (with standard error):\\n\", mean, \"+/-\", sem) #<<\n} #<<\n\n# Create graph\ngg_point <- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  #<<  after_stat() function specifies that the tooltip should be calculated after the summary statistics are calculated.\n                     tooltip(y, ymax))),  #<<\n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  #<<\n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\n# Generate SVG object\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n\n3.3.1.2 The onclick argument\nonclick argument of ggiraph provides hotlink interactivity on the web. Web document link with a data object will be displayed on the web browser upon mouse click.\n\n\nShow the code\nexam_data$onclick <- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              #<<\n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)                                                                                \n\n\n\n\n\n\n\n\n3.3.1.3 The data_id argument\nWe assign the data_id argument to CLASS and when we mouse over a particular student in the chart below, the fellow classmates will also be highlighted.\nüñ±Ô∏è Mouse over the chart to check out the hover effect!\n\n\nShow the code\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             #<<\n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)                                        \n\n\n\n\n\n\nThe default color of hover_css = ‚Äúfill:orange;‚Äù\nCustomise the hover effect\nCSS codes are used to change the highlighting effect.\nüñ±Ô∏è Mouse over the chart to check out the new hover effect!\n\n\nShow the code\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        #<<\n    opts_hover(css = \"fill: red;\"),  #<<\n    opts_hover_inv(css = \"opacity:0.2;\") #<<\n  )                                      #<<  \n)                                        \n\n\n\n\n\n\nCombine use of tooltip and data_id arguments\nThere are times when we want to combine tooltip and hover effect on an interactive statistical graph. In the following chart, elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over. At the same time, the tooltip will show the CLASS.\n\n\nShow the code\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, #<<\n        data_id = CLASS),#<<              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: red;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\n\n\n\n3.3.1.4 Coordinated multiple views with ggiraph\nWhen a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualisation will be highlighted too.\nIn order to build a coordinated multiple views, the following programming strategy will be used:\n\nAppropriate interactive functions of ggiraph will be used to create the different plots.\npatchwork function will be used inside girafe() function to create the interactive coordinated multiple views.\n\n\n\nShow the code\np1 <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS,\n        data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + #<<\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 <- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS,\n        data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + #<<\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 / p2), #<<\n       width_svg = 6,\n       height_svg = 6,\n       options = list(\n         opts_hover(css = \"fill: orange;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       ) \n\n\n\n\n\n\n\n\n\n3.3.2 Working with plotly package\n\nPlotly‚Äôs R graphing library creates interactive web graphics from ggplot2 graphs and/or a custom interface to the (MIT-licensed) JavaScript library plotly.js inspired by the grammar of graphics.\nDifferent from other plotly platform, plot.R is free and open source.\n\nThere are two ways to create interactive graph by using plotly, they are:\n\nby using plot_ly(), and\nby using ggplotly()\n\n\n3.3.2.1 Create an interactive scatter plot: plot_ly() function\nThe graph below is plotted using plot_ly().\n\n\nShow the code\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\n\n\n\n\n\nIn the next plot, the color argument is mapped to a qualitative visual variable (i.e.¬†RACE). We can hide/unhide the data points for each race by click on their label in the legend.\n\n\nShow the code\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE) #<<\n\n\n\n\n\n\n\ncolors argument is used to change the default colour palette to ColorBrewer colour palette.\n\n\n\nShow the code\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE, \n        colors = \"Set1\") #<<\n\n\n\n\n\n\n\nWe can also customise our own colour scheme and assign it to the colors argument.\n\n\n\nShow the code\npal <- c(\"red\", \"purple\", \"blue\", \"green\") #<<\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE, \n        colors = pal) #<<\n\n\n\n\n\n\n\ntext argument is used to change the default tooltip.\n\n\n\nShow the code\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS,\n        text = ~paste(\"Student ID:\", ID,     #<<\n                      \"<br>Class:\", CLASS),  #<<\n        color = ~RACE, \n        colors = \"Set1\")\n\n\n\n\n\n\n\nlayout argument is used to update the chart title and axes limit.\n\n\n\nShow the code\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS,\n        text = ~paste(\"Student ID:\", ID,     \n                      \"<br>Class:\", CLASS),  \n        color = ~RACE, \n        colors = \"Set1\") %>%\n  layout(title = 'English Score versus Maths Score ', #<<\n         xaxis = list(range = c(0, 100)),             #<<\n         yaxis = list(range = c(0, 100)))             #<<\n\n\n\n\n\n\nTo learn more about layout, visit this link.\n\n\n3.3.2.2 Create an interactive scatter plot: ggplotly() funciton\nWe just need to include the original ggplot2 graph in the ggplotly() function.\n\n\nShow the code\np <- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(dotsize = 1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p) #<<\n\n\n\n\n\n\n3.3.2.3 Coordinated Multiple Views with plotly\n\nWe use subplot() of plotly package to place two scatterplots side-by-side.\n\n\n\nShow the code\np1 <- ggplot(data=exam_data, \n              aes(x = MATHS,\n                  y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 <- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),            #<<\n        ggplotly(p2))            #<<\n\n\n\n\n\n\n\nWe use the highlight_key() of plotly package to coordinate the selection of data points across two scatterplots.\n\nüñ±Ô∏èClick on the data point of one of the charts and we will see the same data point being highlighted in the other chart\n\n\nShow the code\nd <- highlight_key(exam_data)  #<<\n\np1 <- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 <- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\nsubplot(ggplotly(p1),\n        ggplotly(p2))\n\n\n\n\n\n\n\n\n\n\n\n\nHow highlight_key() function works\n\n\n\n\nThe function creates an object of class crosstalk::SharedData.\nVisit this link to learn more about crosstalk\n\n\n\n\n\n\n3.3.3 Working with crosstalk package\nCrosstalk is an add-on to the htmlwidgets package. It extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\n\n3.3.3.1 Interactive Data Table: DT package\n\nA wrapper of the JavaScript Library DataTables\nData objects in R can be rendered as HTML tables using the JavaScript library ‚ÄòDataTables‚Äô (typically via R Markdown or Shiny).\n\n\n\nShow the code\nDT::datatable(exam_data, class= \"compact\")\n\n\n\n\n\n\n\n\n\n3.3.3.2 Linked brushing: crosstalk method\nThings to note when implementing coordinated brushing:\n\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document.\n\nüëáSelect a cluster of points from the scatterplot and the data table below will return the records of the selected data points\n\n\nShow the code\nd <- highlight_key(exam_data) \n\np <- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg <- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d),\n                  widths = 12)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex03/Hands-On_Ex3a.html#reference",
    "href": "Hands-On_Ex/Hands-On_Ex03/Hands-On_Ex3a.html#reference",
    "title": "3 Interactive Data Visualisation with R",
    "section": "3.4 Reference",
    "text": "3.4 Reference\n\n3.4.1 ggiraph\nThis link provides online version of the reference guide and several useful articles. Use this link to download the pdf version of the reference guide.\n\nHow to Plot With Ggiraph\nInteractive map of France with ggiraph\nCustom interactive sunbursts with ggplot in R\nThis link provides code example on how ggiraph is used to interactive graphs for Swiss Olympians - the solo specialists.\n\n\n\n3.4.2 plotly for R\n\nGetting Started with Plotly in R\nA collection of plotly R graphs are available via this link.\nCarson Sievert (2020) Interactive web-based data visualization with R, plotly, and shiny, Chapman and Hall/CRC is the best resource to learn plotly for R. The online version is available via this link\nPlotly R Figure Reference provides a comprehensive discussion of each visual representations.\nPlotly R Library Fundamentals is a good place to learn the fundamental features of Plotly‚Äôs R API.\nGetting Started\nVisit this link for a very interesting implementation of gganimate by your senior.\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels\n\n\n\\(**That's\\) \\(all\\) \\(folks!**\\)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex03/Hands-On_Ex3b.html",
    "href": "Hands-On_Ex/Hands-On_Ex03/Hands-On_Ex3b.html",
    "title": "4 Animated Statistical Graph with R",
    "section": "",
    "text": "(First Published: 26-Apr-2023)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex03/Hands-On_Ex3b.html#learning-outcome",
    "href": "Hands-On_Ex/Hands-On_Ex03/Hands-On_Ex3b.html#learning-outcome",
    "title": "4 Animated Statistical Graph with R",
    "section": "4.1 Learning Outcome",
    "text": "4.1 Learning Outcome\nWhen telling a visually-driven data story, animated graphics tends to attract the interest of the audience and make deeper impression than static graphics. We will learn to create animated data visualisation. At the same time, we will also learn to reshape, process, wrangle and transform the data-set.\n\n4.1.1 Basic concept of animation\nWhen creating animations, the plot does not actually move. Instead, many individual plots are built and then stitched together as movie frames, just like an old-school flip book or cartoon. Each frame is a different plot, which is built using a relevant subset of the data-set. Motion (and the animated effect) appears when the stitched plots are displayed sequentially.\n\n\n\n4.1.2 Terminologies\nBefore we dive into the steps for creating an animated statistical graph, it‚Äôs important to understand some of the key concepts and terminologies that will be used:\n\nFrame: In an animated line graph, each frame represents a different point in time or a different category. When the frame changes, the data points on the graph are updated to reflect the new data.\nAnimation Attributes: The animation attributes are the settings that control how the animation behaves. For example, we can specify the duration of each frame, the easing function used to transit between frames, and whether to start the animation from the current frame or from the beginning.\n\n\n\n\n\n\n\nWhen do we use animated graphs?\n\n\n\nBefore we start making animated graphs, we should first ask ourselves: Does it makes sense to go through the effort? If we are conducting an exploratory data analysis, a animated graphic may not be worth the time investment. However, if we are giving a presentation, a few well-placed animated graphics can help an audience connect with our topic remarkably better than static counterparts."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex03/Hands-On_Ex3b.html#getting-started",
    "href": "Hands-On_Ex/Hands-On_Ex03/Hands-On_Ex3b.html#getting-started",
    "title": "4 Animated Statistical Graph with R",
    "section": "4.2 Getting Started",
    "text": "4.2 Getting Started\n\n4.2.1 Install and load the required r libraries\nInstall and load the the required R packages. The name and function of the new package that will be used for this exercise are as follow:\n\ngganimate: creates animated graphics, such as line charts, bar charts, and maps, by specifying a series of frames with data and aesthetics that change over time\ngifski: converts images to GIF animations using pngquant‚Äôs efficient cross-frame palettes and temporal dithering with thousands of colors per frame.\ngapminder: An excerpt of the data available at Gapminder.org. We just want to use its country_colors scheme.\nreadxl: makes it easy to get data out of Excel and into R\n\n\n\nShow the code\npacman::p_load(tidyverse,plotly,gganimate, gifski, gapminder, gapminder,readxl)\n\n\n\n\n4.2.2 Import the data\nThe Data worksheet from GlobalPopulation Excel workbook will be used. We will use the following functions to process the data-set:\n\nread_xls() of readxl package is used to import the Excel worksheet.\nmutate_if() of dplyr package is used to a subset of columns in a data frame based on a logical condition\nmutate() of dplyr package is used to convert data values of Year field into integer.\n\n\n\nShow the code\ncol <- c(\"Country\", \"Continent\")\nglobalPop <- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %>%\n  mutate_if(is.character, as.factor) %>%  \n  mutate(Year = as.integer(Year))"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex03/Hands-On_Ex3b.html#animated-data-visualisation",
    "href": "Hands-On_Ex/Hands-On_Ex03/Hands-On_Ex3b.html#animated-data-visualisation",
    "title": "4 Animated Statistical Graph with R",
    "section": "4.3 Animated data visualisation",
    "text": "4.3 Animated data visualisation\nThe basic ggplot2 functions are used to create the static bubble plot.\n\n\nShow the code\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') \n\n\n\n\n\n\n\n\n\n\n\nThe graph looks awful‚Ä¶\n\n\n\nThis is because all data points for all featured countries across all years were plotted on a single chart. Read on to see the magic of animated charts üòú.\n\n\n\n4.3.1 Working with gganimate package\ngganimate extends the grammar of graphics as implemented by ggplot2 to include the description of animation. It does this by providing a range of new grammar classes that can be added to the plot object in order to customise how it should change with time.\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_*()/exit_*() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\nBuilding the animated bubble plot\nIn the following chart:\n\ntransition_time() is used to create transition through distinct states in time (i.e.¬†Year).\nease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\n\n\nShow the code\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes(y = 'linear')          \n\n\n\n\n\n\n\n4.3.2 Working with plotly package\nIn Plotly R package, both ggplotly() and plot_ly() support key frame animations through the frame argument/aesthetic. They also support an ids argument/aesthetic to ensure smooth transitions between objects with the same id (which helps facilitate object constancy).\nIn the following graph, ggplotly() is used to convert the ggplot2 graph into an animated object.\nggplotly(gg)\n\n\nShow the code\ngg <- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') +\n  theme(legend.position='none')\n\nggplotly(gg)\n\n\n\n\n\n\nIn this next graph, we will use plot_ly() function to create the animated bubble plot.\n\n\nShow the code\nbp <- globalPop %>%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent, \n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers') %>%\n  layout(xaxis=list(title='% Old',\n                showgrid=FALSE,\n                zeroline=FALSE,\n                showticklabels=FALSE),\n         yaxis=list(title='% Young'))\nbp"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex03/Hands-On_Ex3b.html#reference",
    "href": "Hands-On_Ex/Hands-On_Ex03/Hands-On_Ex3b.html#reference",
    "title": "4 Animated Statistical Graph with R",
    "section": "4.4 Reference",
    "text": "4.4 Reference\n\nGetting Started\nVisit this link for a very interesting implementation of gganimate by your senior.\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels\nHow to change the yaxis linecolor in the horizontal bar? - Plotly R, MATLAB, Julia, Net / Plotly R - Plotly Community Forum: This helps to resolve the black y-axis in the last bubble plot. The interesting finding is that we have to set the x-axis‚Äôs zeroline argument to FALSE to resolve the issue üòÖ.\n\n\n\\(**That's\\) \\(all\\) \\(folks!**\\)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4a.html",
    "href": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4a.html",
    "title": "5 Visual Statistical Analysis",
    "section": "",
    "text": "(First Published: May 4, 2023)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4a.html#learning-outcome",
    "href": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4a.html#learning-outcome",
    "title": "5 Visual Statistical Analysis",
    "section": "5.1 Learning Outcome",
    "text": "5.1 Learning Outcome\nWe will learn to create visual graphics with rich statistical information, visualise model diagnostics, and model parameters."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4a.html#getting-started",
    "href": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4a.html#getting-started",
    "title": "5 Visual Statistical Analysis",
    "section": "5.2 Getting Started",
    "text": "5.2 Getting Started\n\n5.2.1 Install and load the required r libraries\nInstall and load the the required R packages. The name and function of the new package that will be used for this exercise are as follow:\n\nggstatsplot: offers various types of statistical plots and functions for statistical tests such as t-tests, ANOVA, correlation tests, and regression analysis\nperformance: offers functions for computing model evaluation metrics for model evaluation and comparison\nparameters: provides for managing complex experiments with many parameters and for automating parameter tuning in machine learning workflows\nsee: provides a host of functions and tools to produce a range of publication-ready visualizations for model parameters, predictions, and performance diagnostics\n\n\n\nShow the code\npacman::p_load(tidyverse,readxl, ggstatsplot, performance, parameters, see)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4a.html#visual-statistical-analysis-with-ggstatsplot-package",
    "href": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4a.html#visual-statistical-analysis-with-ggstatsplot-package",
    "title": "5 Visual Statistical Analysis",
    "section": "5.3 Visual Statistical Analysis with ggstatsplot package",
    "text": "5.3 Visual Statistical Analysis with ggstatsplot package\nggstatsplot is an extension of ggplot2 package for creating graphics enriched with statistical test details. It:\n\nProvides alternative statistical inference methods by default.\nFollows the best practices for statistical reporting. For all statistical tests reported in the plots, the default template abides by the APA gold standard for statistical reporting. For example, here are the results from a t-test:\n\n\n\n5.3.1 Import the data\nWe will be using the same exam scores data-set that was featured in my Hands-On Exercise for Week 1.\n\n\nShow the code\nexam <- read_csv('data/Exam_data.csv', show_col_types = FALSE )\n\n\n\n\n5.3.2 One-sample Mean Test\nWe use gghistostats() to to build an visual of one-sample test on English scores.\nIn the following case:\n\ntype = \"bayes\": specifies the type of statistical test to perform on the data to generate the interval estimate. In this case, it is a Bayesian analysis, which provides a posterior distribution of plausible values based on prior knowledge and observed data.\ntest.value = 60: specifies the value for the null hypothesis that will be used to calculate the probability of the observed data. In this case, it is 60, which assumes that the average English score is 60.\n\n\n\nShow the code\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\nUnpacking the Bayes Factor\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThe Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as:\n\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\nHow to interpret Bayes Factor\nA Bayes Factor can be any positive number. One of the most common interpretations is this one‚Äîfirst proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013: \n\n\n5.3.3 Two-sample Mean Test\nNext, we use ggbetweenstats() to build a visual for two-sample mean test of Maths scores by gender. This generates a combination of box and violin plots along with jittered data points for between-subjects designs with statistical details included in the plot as a subtitle.\nIn the following case:\n\ntype = \"np\": specifies the type of test to be used to compare the groups, in this case, a non-parametric test (Wilcoxon-Mann-Whitney test).\nmessages = FALSE: specifies whether or not to display informative messages about the statistical test being performed.\n\n\n\nShow the code\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n5.3.4 One-way ANOVA Test\nWe can also use ggbetweenstats() to build a visual for One-way ANOVA test on English score by race.\nIn the following case:\n\ntype = \"p\": specifies the type of test to be used to compare the groups, in this case, a parametric test (one-way ANOVA).\nmean.ci = TRUE: specifies whether or not to display confidence intervals for the group means.\npairwise.comparisons = TRUE: specifies whether or not to display pairwise comparisons between the groups.\npairwise.display = \"s\": specifies the format of the pairwise comparison display, in this case, ‚Äús‚Äù for compact letter display.\np.adjust.method = \"fdr\": specifies the method used for p-value adjustment for multiple comparisons, in this case, false discovery rate (FDR) correction.\n\n\n\nShow the code\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\nSummary of tests with ggbetweenstats\n  \n\n\n5.3.5 Significant Test of Correlation\nWe can use ggscatterstats() to build a visual for Significant Test of Correlation between Maths scores and English scores. This creates a scatter plot with overlaid regression lines between the variables ‚ÄúMATHS‚Äù and ‚ÄúENGLISH‚Äù in the ‚Äúexam‚Äù dataset.\nIn the following case:\n\nmarginal = TRUE: specifies whether or not to display marginal histograms or density plots along the axes of the scatter plot. In this case, marginal plots are not displayed.\n\n\n\nShow the code\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = TRUE,\n  )\n\n\n\n\n\n\n\n5.3.5 Significant Test of Association (Dependence)\nFirst, we bin the Maths scores into a 4-class variable using cut() function.\n\n\nShow the code\nexam1 <- exam %>% \n  mutate(MATHS_bins = cut(MATHS, \n               breaks = c(0,60,75,85,100))\n  )\n\n\nWe use ggbarstats() to build a visual for Significant Test of Association between the categorised Maths scores and gender.\n\n\nShow the code\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4a.html#visualise-models",
    "href": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4a.html#visualise-models",
    "title": "5 Visual Statistical Analysis",
    "section": "5.4 Visualise Models",
    "text": "5.4 Visualise Models\nIn this section, we will learn how to visualise model diagnostic and model parameters.\n\n5.4.1 Import the data\nToyota Corolla case study will be used and the datat-set is imported. We will build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.\n\n\nShow the code\ncar_resale <- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n\n# A tibble: 1,436 √ó 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   <dbl> <chr>    <dbl>     <dbl>     <dbl>    <dbl>  <dbl>         <dbl>  <dbl>\n 1    81 TOYOTA ‚Ä¶ 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA ‚Ä¶ 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA ‚Ä¶ 13750        23        10     2002  72937           210   1165\n 4     3 ¬†TOYOTA‚Ä¶ 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA ‚Ä¶ 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA ‚Ä¶ 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA ‚Ä¶ 12950        32         1     2002  61000           210   1170\n 8     7 ¬†TOYOTA‚Ä¶ 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA ‚Ä¶ 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA ‚Ä¶ 16950        27         6     2002 110404           234   1255\n# ‚Ñπ 1,426 more rows\n# ‚Ñπ 29 more variables: Guarantee_Period <dbl>, HP_Bin <chr>, CC_bin <chr>,\n#   Doors <dbl>, Gears <dbl>, Cylinders <dbl>, Fuel_Type <chr>, Color <chr>,\n#   Met_Color <dbl>, Automatic <dbl>, Mfr_Guarantee <dbl>,\n#   BOVAG_Guarantee <dbl>, ABS <dbl>, Airbag_1 <dbl>, Airbag_2 <dbl>,\n#   Airco <dbl>, Automatic_airco <dbl>, Boardcomputer <dbl>, CD_Player <dbl>,\n#   Central_Lock <dbl>, Powered_Windows <dbl>, Power_Steering <dbl>, ‚Ä¶\n\n\n\n\n5.4.2 Create a Multi-variate Linear Regression Model\nWe use lm() function of R Base Stats to calibrate a multi-variate linear regression model.\n\n\nShow the code\nmodel <- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n5.4.3 Model Diagnostic: Check for Multicolinearity\nWe use check_collinearity() of performance package to conduct the test.\n\n\nShow the code\ncheck_collinearity(model)\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\nShow the code\nplot(check_collinearity(model))\n\n\n\n\n\n\n\n5.4.4 Model Diagnostic: Check Normality Assumption\nThere is the check_normality() function to conduct the test.\n\n\nShow the code\nmodel1 <- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\ncheck_n <- check_normality(model1)\n\nplot(check_n)\n\n\n\n\n\n\n\n5.4.5 Model Diagnostic: Check model for homogeneity of variances\nWe can use the check_heteroscedasticity() to do this check.\n\n\nShow the code\ncheck_h <- check_heteroscedasticity(model1)\n\nplot(check_h)\n\n\n\n\n\n\n\n5.4.6 Model Diagnostic: Complete Check\nThank goodness! There‚Äôs also a check_model()function to conduct the diagnostic tests discussed above.\n\n\nShow the code\ncheck_model(model1)\n\n\n\n\n\n\n\n5.4.7 Visualise Regression Parameters\nWe can use plot() function of see package and parameters() of parameters package to visualise the parameters of a regression model.\n\n\nShow the code\nplot(parameters(model1))\n\n\n\n\n\nAlternatively, we can use ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\n\nShow the code\nggcoefstats(model1, \n            output = \"plot\")\n\n\n\n\n\n\n\\(**That's\\) \\(all\\) \\(folks!**\\)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4b.html",
    "href": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4b.html",
    "title": "6 Visualise Uncertainty",
    "section": "",
    "text": "(First Published: 4-May-2023)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4b.html#learning-outcome",
    "href": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4b.html#learning-outcome",
    "title": "6 Visualise Uncertainty",
    "section": "6.1 Learning Outcome",
    "text": "6.1 Learning Outcome\nA point estimate, such as mean, is a single numerical value that is used to estimate an unknown population parameter. On the other hand, a range estimate is a range of values that is used to estimate an unknown population parameter. A range estimate is useful when a single point estimate is not precise enough, or when we want to communicate the level of uncertainty surrounding the point estimate.\nIn this exercise, we will explore approaches to visualise the uncertainty of point estimates."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4b.html#getting-started",
    "href": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4b.html#getting-started",
    "title": "6 Visualise Uncertainty",
    "section": "6.2 Getting Started",
    "text": "6.2 Getting Started\n\n6.2.1 Install and load the required r libraries\nInstall and load the the required R packages. The name and function of the new package that will be used for this exercise is as follow:\n\nggdist: provides a range of functions for creating visualizations of probability distributions\nknitr: create high-quality, fully reproducible documents that integrate code, text, and graphics\n\n\n\nShow the code\npacman::p_load(tidyverse, plotly, crosstalk, DT, gganimate, ggdist, knitr)\n\n\n\n\n6.2.2 Import the data\nWe will be using the same exam scores data-set that was featured in my Hands-On Exercise for Week 1.\n\n\nShow the code\nexam <- read_csv('data/Exam_data.csv', show_col_types = FALSE )"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4b.html#visualise-the-uncertainty-of-point-estimates-with-ggplot2-package",
    "href": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4b.html#visualise-the-uncertainty-of-point-estimates-with-ggplot2-package",
    "title": "6 Visualise Uncertainty",
    "section": "6.3 Visualise the uncertainty of point estimates with ggplot2 package",
    "text": "6.3 Visualise the uncertainty of point estimates with ggplot2 package\nFirst, we perform with following on the exam scores data-set:\n\ngroup the observations by RACE,\ncompute the count of observations, mean, standard deviation and standard error of Maths scores by RACE, and\nassign the output as a tibble data table called `my_sum\nprint the tibble data using kable() function from knitr\n\n\n\nShow the code\nmy_sum <- exam %>%\n  group_by(RACE) %>%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %>%\n  mutate(se=sd/sqrt(n-1))\n\nkable(head(my_sum), format = 'html') \n\n\n\n\n \n  \n    RACE \n    n \n    mean \n    sd \n    se \n  \n \n\n  \n    Chinese \n    193 \n    76.50777 \n    15.69040 \n    1.132357 \n  \n  \n    Indian \n    12 \n    60.66667 \n    23.35237 \n    7.041005 \n  \n  \n    Malay \n    108 \n    57.44444 \n    21.13478 \n    2.043177 \n  \n  \n    Others \n    9 \n    69.66667 \n    10.72381 \n    3.791438 \n  \n\n\n\n\n\nWe can use the geom_errorbar() function to reveal the standard error of mean maths score by race in the following chart.\n\n\nShow the code\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    linewidth=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard Error of the Mean Math Scores by Race\")\n\n\n\n\n\n\n\n\n\n\n\nWe plotted this before...\n\n\n\nCheck out this plot which we did in an earlier exercise.\n\n\nWe can also use the geom_errorbar() function to reveal the 95% Confidence Interval of the mean maths score by race in the following chart.\n\n# Calculate the lower and upper bound for confidence intervals\nmy_sum2 <- my_sum %>% \n  mutate(lower_ci = mean - qt(0.975, n-1)*se,\n         upper_ci = mean + qt(0.975, n-1)*se)\n\n# Create point estimate plot with error bars\nggplot(my_sum2) +\n  geom_errorbar(\n    aes(x=reorder(RACE,-mean), \n        ymin=lower_ci, \n        ymax=upper_ci), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    linewidth=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(title=\"95% Confidence Interval of the Mean Math Scores by Race\",\n       x = \"Race\",\n       y = \"Math Score\")\n\n\n\n\n\n6.3.1 Visualise the uncertainty of point estimates with interactive error bars\nThe following interactive error bar plot shows the 99% Confidence Interval of mean maths score by race.\n( üñ±Ô∏èClick on the average score (in red) for the cross-filtering to work)\n\n\nShow the code\n# Prepare the summary table with the relevant stats\nmy_sum3 <- exam %>%\n  group_by(RACE) %>%\n  summarise(\n    'No. of pupils'=n(),\n    'Avg Scores'=mean(MATHS),\n    'Std Dev'=sd(MATHS),\n    'Std Error' = sd(MATHS)/sqrt(n()-1)\n    ) \n\n# Use highlight_key() to add a unique key to the data frame my_sum3 so that it can be linked to interactive plots later\nd <- highlight_key(my_sum3)\n\n# Prepare the error bar plot\np <- ggplot(d) +\n  geom_errorbar(\n    aes(x=reorder(RACE,-`Avg Scores`), \n        ymin=`Avg Scores` - qt(0.995, `No. of pupils`-1)*`Std Error`, \n        ymax=`Avg Scores` + qt(0.995, `No. of pupils`-1)*`Std Error`), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    linewidth=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=`Avg Scores`,\n            text = paste(\"Race:\", `RACE`, \n                                  \"<br>N:\", `No. of pupils`,\n                                  \"<br>Avg. Scores:\", round(`Avg Scores`, digits = 2),\n                                  \"<br>95% CI:[\", \n                                  round((`Avg Scores` - qt(0.995, `No. of pupils`-1)*`Std Error`), digits = 2), \",\",\n                                  round((`Avg Scores` + qt(0.995, `No. of pupils`-1)*`Std Error`), digits = 2),\"]\")), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(title=\"99% Confidence Interval of the Mean Math Scores by Race\",\n       x = \"Race\",\n       y = \"Math Score\")\n\n# Convert ggplot to an interactive plotly plot using the ggplotly(), \"plotly click\" specifies that highlight should be based on click\ngg <- highlight(ggplotly(p),on =        \n                \"plotly_click\")\n\n# Create a Bootstrap grid of two columns to house the 2 plots in the ratio of 5:7\nbscols(gg,datatable(d,options = list(dom='t')),widths = c(5,7))"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4b.html#visualising-uncertainty-using-ggdist-package",
    "href": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4b.html#visualising-uncertainty-using-ggdist-package",
    "title": "6 Visualise Uncertainty",
    "section": "6.4 Visualising Uncertainty using ggdist package",
    "text": "6.4 Visualising Uncertainty using ggdist package\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty. It is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(‚Äúfreq-uncertainty-vis‚Äù));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\nWe use the stat_pointinterval() of ggdist to build a visual for displaying distribution of maths scores by race.\n\n\nShow the code\nexam %>%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +   #<<\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\") +\n  theme_minimal()\n\n\n\n\n\nThis stat_pointinterval() function comes with many arguments. The following plot creates a plot that displays the median math score for each race group, along with an interval estimate for each group. This can help visualize the differences in math scores across different racial groups and also the variability within each group.\n\n\nShow the code\nexam %>%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  point_interval = \"median_qi\"\n  ) +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\nWe can use the stat_gradientinterval() function of ggdist to build a visual for displaying distribution of maths scores by race.\n\n\nShow the code\nexam %>%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4b.html#visualise-uncertainty-with-hypothetical-outcome-plots-hops",
    "href": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4b.html#visualise-uncertainty-with-hypothetical-outcome-plots-hops",
    "title": "6 Visualise Uncertainty",
    "section": "6.5 Visualise Uncertainty with Hypothetical Outcome Plots (HOPs)",
    "text": "6.5 Visualise Uncertainty with Hypothetical Outcome Plots (HOPs)\nThe ungeviz package is meant to provide helpful add-on functionality for ggplot2 to visualize uncertainty. The package is particularly focused on hypothetical outcome plots (HOPs) and provides bootstrapping and sampling functionality that integrates well with the ggplot2 API.\nStep 1: Install the ungeviz package\n\n\nShow the code\ndevtools::install_github(\"wilkelab/ungeviz\")\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe installation should only be carried out once.\n\n\nStep 2: Launch the application in R\n\n\nShow the code\nlibrary(ungeviz)\n\n\n\n\nShow the code\nggplot(data = exam, \n       (aes(x = factor(RACE), \n            y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = RACE), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe sampler() function generates sampling and bootstrapping exam scores repeatedly and these scores are used as input data in ggplot2 layer. This helps to create the hypothetical outcome plot.\n\n\n\n\\(**That's\\) \\(all\\) \\(folks!**\\)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4c.html",
    "href": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4c.html",
    "title": "7 Funnel Plots for Fair Comparison",
    "section": "",
    "text": "(First Published: May 4, 2023)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4c.html#learning-outcome",
    "href": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4c.html#learning-outcome",
    "title": "7 Funnel Plots for Fair Comparison",
    "section": "7.1 Learning Outcome",
    "text": "7.1 Learning Outcome\nFunnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities.\nIn this hands-on exercise, we will learn to design and produce static and interactive funnel plots."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4c.html#getting-started",
    "href": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4c.html#getting-started",
    "title": "7 Funnel Plots for Fair Comparison",
    "section": "7.2 Getting Started",
    "text": "7.2 Getting Started\n\n7.2.1 Install and load the required r libraries\nInstall and load the the required R packages. The name and function of the new packages that will be used for this exercise are as follow:\n\nFunnelPlotR for creating funnel plot\nkableExtra for additional functionality to format kable() tables\n\n\n\nShow the code\npacman::p_load(tidyverse, plotly, knitr, FunnelPlotR, kableExtra )\n\n\n\n\n7.2.2 Import the data\nWe will be using the COVID-19_DKI_Jakarta data-set. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e.¬†kelurahan) as at 31st July 2021, DKI Jakarta.\nWe import the data into R and save it into a tibble data frame object called covid19.\n\n\nShow the code\ncovid19 <- read_csv(\"data/COVID-19_DKI_Jakarta.csv\", show_col_types = FALSE) %>%\n  mutate_if(is.character, as.factor)\n\nkable(head(covid19), format = 'html', caption = \"Table 1 First Records from the Covid-19 dataset\")%>%\n  kable_styling(\"striped\")\n\n\n\n\nTable 1 First Records from the Covid-19 dataset\n \n  \n    Sub-district ID \n    City \n    District \n    Sub-district \n    Positive \n    Recovered \n    Death \n  \n \n\n  \n    3172051003 \n    JAKARTA UTARA \n    PADEMANGAN \n    ANCOL \n    1776 \n    1691 \n    26 \n  \n  \n    3173041007 \n    JAKARTA BARAT \n    TAMBORA \n    ANGKE \n    1783 \n    1720 \n    29 \n  \n  \n    3175041005 \n    JAKARTA TIMUR \n    KRAMAT JATI \n    BALE KAMBANG \n    2049 \n    1964 \n    31 \n  \n  \n    3175031003 \n    JAKARTA TIMUR \n    JATINEGARA \n    BALI MESTER \n    827 \n    797 \n    13 \n  \n  \n    3175101006 \n    JAKARTA TIMUR \n    CIPAYUNG \n    BAMBU APUS \n    2866 \n    2792 \n    27 \n  \n  \n    3174031002 \n    JAKARTA SELATAN \n    MAMPANG PRAPATAN \n    BANGKA \n    1828 \n    1757 \n    26"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4c.html#funnelplotr-methods",
    "href": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4c.html#funnelplotr-methods",
    "title": "7 Funnel Plots for Fair Comparison",
    "section": "7.3 FunnelPlotR methods",
    "text": "7.3 FunnelPlotR methods\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: label outliers (true or false).\nPoisson_limits: add Poisson limits to the plot.\nOD_adjust: add overdispersed limits to the plot.\nxrange and yrange: specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc.\n\n\n\n\n\n\n\nHow do I interpret a funnel plot?\n\n\n\nCheck out this video which explains the elements of the funnel plot and how it is constructed.\n\n\n\n7.3.1 FunnelPlotR methods: The Basic Plot\nWe use the funnel_plot() function to create a basic plot. Things to note:\n\ngroup argument is different from that in the scatterplot. Here, it specifics the level of the points to be plotted i.e.¬†Sub-district, District or City. If City is chosen, there will only be six data points.\nBy default, data_typeargument is ‚ÄúSR‚Äù.\nlimit: the accepted plot limit values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\n\nShow the code\nfunnel_plot(\n  numerator = covid19$Positive,\n  denominator = covid19$Death,\n  group = covid19$`Sub-district`\n)\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n7.3.2 FunnelPlotR methods: Makeover Part 1\nWe updated the arguments used in the funnel_plot() function to create the following plot:\n\ndata_type: A character string specifying the type of data to be plotted. In this case, it is set to ‚ÄúPR‚Äù, which stands for proportion or percentage, indicating that the data in the numerator and denominator arguments are in the form of proportions or percentages, and not absolute counts.\nx_range: A numeric vector of length two specifying the range of the x-axis of the plot. Here, it is set to c(0, 6500), which means the x-axis ranges from 0 to 6500.\ny_range: A numeric vector of length two specifying the range of the y-axis of the plot. Here, it is set to c(0, 0.05), which means the y-axis ranges from 0 to 0.05.\n\n\n\nShow the code\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",     #<<\n  x_range = c(0, 6500),  #<<\n  y_range = c(0, 0.05)   #<<\n)\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n7.3.3 FunnelPlotR methods: Makeover Part 2\nFurther updates to the relevant arguments of the funnel_plot() function are:\n\nlabel: A logical value indicating whether or not to display the group labels on the plot. Here, it is set to NA, which means that no labels will be displayed.\ntitle: A character string specifying the title of the plot. Here, it is set to ‚ÄúCOVID-19 Fatality Rate by Total Number of COVID-19 Positive Cases‚Äù.\nx_label: A character string specifying the label for the x-axis of the plot. Here, it is set to ‚ÄúCOVID-19 Positive Cases‚Äù.\ny_label: A character string specifying the label for the y-axis of the plot. Here, it is set to ‚ÄúFatality Rate‚Äù.\n\n\n\nShow the code\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",   \n  x_range = c(0, 6500),  \n  y_range = c(0, 0.05),\n  label = NA,\n  title = \"COVID-19 Fatality Rate by Total Number of COVID-19 Positive Cases\", #<<           \n  x_label = \"COVID-19 Positive Cases\", #<<\n  y_label = \"Fatality Rate\"  #<<\n)\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4c.html#funnel-plot-for-fair-visual-comparison-using-ggplot2-package",
    "href": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4c.html#funnel-plot-for-fair-visual-comparison-using-ggplot2-package",
    "title": "7 Funnel Plots for Fair Comparison",
    "section": "7.4 Funnel Plot for Fair Visual Comparison using ggplot2 package",
    "text": "7.4 Funnel Plot for Fair Visual Comparison using ggplot2 package\nIn this section, we will learn to build funnel plots step-by-step by using ggplot2. This will enhance our working experience of ggplot2 to customise a speciallised data visualisation like funnel plot.\n\n7.4.1 Derive the basic statistics\nTo plot the funnel plot from scratch, we need to derive cumulative death rate (or fatality rate) and standard error of cumulative death rate.\n\n\n\nStandard Error for Sample Proportion\n\n\n\n\nShow the code\ndf <- covid19 %>%\n  mutate(rate = Death / Positive) %>%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %>%\n  filter(rate > 0)\n\n\nNext, we derive the weighted mean of the values. In this case, we use the weighted.mean() function to find the weighted mean of the rate values.\n\n\nShow the code\nfit.mean <- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n\n7.4.2 Calculate lower and upper limits for 95% and 99.9% Confidence Interval\nWe will then compute the lower and upper limits for 95% and 99.9% confidence interval.\n\n\nShow the code\nnumber.seq <- seq(1, max(df$Positive), 1)\nnumber.ll95 <- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 <- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 <- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 <- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \n\ndfCI <- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\nkable(head(dfCI), format = 'html', caption = \"Table 2 First Records of CI Intervals and Fit.Mean Value\")%>%\n  kable_styling(\"striped\")\n\n\n\n\nTable 2 First Records of CI Intervals and Fit.Mean Value\n \n  \n    number.ll95 \n    number.ul95 \n    number.ll999 \n    number.ul999 \n    number.seq \n    fit.mean \n  \n \n\n  \n    -0.2230353 \n    0.2529745 \n    -0.3845386 \n    0.4144778 \n    1 \n    0.0149696 \n  \n  \n    -0.1533253 \n    0.1832645 \n    -0.2675254 \n    0.2974645 \n    2 \n    0.0149696 \n  \n  \n    -0.1224426 \n    0.1523818 \n    -0.2156866 \n    0.2456257 \n    3 \n    0.0149696 \n  \n  \n    -0.1040328 \n    0.1339720 \n    -0.1847845 \n    0.2147237 \n    4 \n    0.0149696 \n  \n  \n    -0.0914694 \n    0.1214086 \n    -0.1636959 \n    0.1936351 \n    5 \n    0.0149696 \n  \n  \n    -0.0821955 \n    0.1121347 \n    -0.1481289 \n    0.1780681 \n    6 \n    0.0149696 \n  \n\n\n\n\n\n\n\n7.4.3 Create a static funnel plot\nWe can use ggplot2 functions to plot a static funnel plot\n\n\nShow the code\np <- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            linewidth = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            linewidth = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            linewidth = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            linewidth = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             linewidth = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Fatality Rate by Number of COVID-19 Cases\") +\n  xlab(\"Number of COVID-19 Cases\") + \n  ylab(\"Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n\n7.4.4 Create an Interactive Funnel Plot\nThe funnel plot created using ggplot2 functions above can be made interactive with ggplotly() of plotly package.\n\n\nShow the code\nfp_ggplotly <- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4c.html#references",
    "href": "Hands-On_Ex/Hands-On_Ex04/Hands-On_Ex4c.html#references",
    "title": "7 Funnel Plots for Fair Comparison",
    "section": "7.5 References",
    "text": "7.5 References\n\nfunnelPlotR package.\nFunnel Plots for Indirectly-standardised ratios.\nChanging funnel plot options\nggplot2 package.\n\n\n\\(**That's\\) \\(all\\) \\(folks!**\\)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5a.html",
    "href": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5a.html",
    "title": "8 Visualise Distribution",
    "section": "",
    "text": "(First Published: May 12, 2023)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5a.html#learning-outcome",
    "href": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5a.html#learning-outcome",
    "title": "8 Visualise Distribution",
    "section": "8.1 Learning Outcome",
    "text": "8.1 Learning Outcome\nWe will learn 2 fairly new techniques to visual distribution, namely:\n\nridgeline plot\nraincloud plot"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5a.html#getting-started",
    "href": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5a.html#getting-started",
    "title": "8 Visualise Distribution",
    "section": "8.2 Getting Started",
    "text": "8.2 Getting Started\n\n8.2.1 Install and load the required r libraries\nInstall and load the the required R packages. The name and function of the new packages that will be used for this exercise are as follow:\n\nggridges : a ggplot2 extension specially designed for plotting ridgeline plots\ncolorspace : for working with and manipulating colors\n\n\n\nShow the code\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse,plotly)\n\n\n\n\n8.2.2 Import the data\nWe will be using the same exam scores data-set that was featured in my Hands-On Exercise for Week 1.\n\n\nShow the code\nexam <- read_csv(\"data/Exam_data.csv\", show_col_types = FALSE)\n\n\n8.3 Visualise Distribution with Ridegeline plots\nRidgeline plot (sometimes called Joyplot) is a data visualisation technique for revealing the distribution of a numeric value for several groups. Distribution can be represented using histograms or density plots, all aligned to the same horizontal scale and presented with a slight overlap.\nThe figure below is a ridgelines plot showing the distribution of English score by class.\n\n\n\n\n\n\n\nNote\n\n\n\n\nRidgeline plots can be used when the number of groups to be represented is medium to high, and where a classic window separation would take to much space. Indeed, the fact that groups overlap each other allows to use space more efficiently. If we have fewer than 5 groups, using other distribution plots is probably better.\nIt works well especially when there is a clear pattern in the result, like if there is an obvious ranking in groups. In other cases, the groups will tend to overlap each other, leading to a messy plot not providing any insight.\n\n\n\n8.3.1 Plotting ridgeline graph: ggridges package\nThere are several ways to plot ridgeline plot with R. In this section, we will learn how to plot ridgeline plot by using ggridges package.\nggridges package provides two main geoms to plot gridgeline plots, they are: geom_ridgeline() and geom_density_ridges(). The former takes height values directly to draw the ridgelines, and the latter first estimates data densities and then draws those using density curves.\nThe ridgeline plot below is plotted by using geom_density_ridges()\n\n\nShow the code\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(\n    name = NULL, \n    expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n8.3.2 Vary fill colors along the x axis\nSometimes we would like to have the area under a ridgeline not filled with a single solid color but rather with colors that vary with the x axis. This effect can be achieved by using either geom_ridgeline_gradient() or geom_density_ridges_gradient(). Both geoms work just like geom_ridgeline() and geom_density_ridges(), except that they allow for varying fill colors. However, they do not allow for alpha transparency in the fill. For technical reasons, we can have changing fill colors or transparency but not both.\n\n\nShow the code\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = after_stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(\n    name = NULL, \n    expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n8.3.3 Map the probabilities directly onto colour\nBeside providing additional geom objects to support the need to plot ridgeline plot, ggridges package also provides a stat function called stat_density_ridges() that replaces stat_density() of ggplot2.\nThe figure below is plotted by mapping the probabilities calculated by using stat(ecdf) which represent the empirical cumulative density function for the distribution of English score.\n\n\nShow the code\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\nImportant!\n\n\n\nThe argument calc_ecdf = TRUE must be included in the stat_density_ridges() function.\n\n\n\n\n8.3.4 Ridgeline plots with quantile lines\nBy using geom_density_ridges_gradient(), we can colour the ridgeline plot by quantile, via the calculated stat(quantile) aesthetic as shown in the figure below.\n\n\nShow the code\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in the figure below.\n\n\nShow the code\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5a.html#visualise-distribution-with-raincloud-plot",
    "href": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5a.html#visualise-distribution-with-raincloud-plot",
    "title": "8 Visualise Distribution",
    "section": "8.4 Visualise Distribution with Raincloud Plot",
    "text": "8.4 Visualise Distribution with Raincloud Plot\nRaincloud Plot is a data visualisation technique that produces a density and a dot plot symmetrically cupped together along a common pane. It gets the name because the density plot is in the shape of a ‚Äúraincloud‚Äù while the dots represent raindrops üåßÔ∏è. The raincloud (half-density) plot enhances the traditional box-plot by highlighting multiple modalities (an indicator that groups may exist). The boxplot does not show where densities are clustered, but the raincloud plot does!\nIn this section, we will learn how to create a raincloud plot to visualise the distribution of English scores by race. It will be created by using functions provided by ggdist and ggplot2 packages.\n\n8.4.1 Plotting a Half Eye graph\nFirst, we will plot a Half-Eye graph by using stat_halfeye() of ggdist package.\nThis produces a Half Eye visualization, which is contains a half-density and a slab-interval. We remove the slab interval by setting .width = 0 and point_colour = NA.\n\n\nShow the code\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\n8.4.2 Adding the boxplot with geom_boxplot()\nNext, we will add the second geometry layer using geom_boxplot() of ggplot2. This produces a narrow boxplot. We reduce the width and adjust the opacity.\n\n\nShow the code\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  # Add'l codes from the previous plot\n  geom_boxplot(width = .20,\n               outlier.shape = NA)\n\n\n\n\n\n\n\n8.4.3 Adding the Dot Plots with stat_dots()\nNext, we will add the third geometry layer using stat_dots() of ggdist package. This produces a half-dotplot, which is similar to a histogram that indicates the number of samples (number of dots) in each bin. We select side = ‚Äúleft‚Äù to indicate we want it on the left-hand side.\n\n\nShow the code\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  # Add'l codes from the previous plot\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\n\n8.4.4 Finishing touch\nLastly, coord_flip() of ggplot2 package will be used to flip the raincloud chart horizontally to give it the raincloud appearance. At the same time, theme_economist() of ggthemes package is used to give the raincloud chart a professional publishing standard look.\n\n\nShow the code\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  # Add'l codes from the previous plot\n  coord_flip() +\n  theme_economist() +\n  # Add geom_text layer for displaying median values\n  stat_summary(fun = median, geom = \"text\", aes(label = round(..y.., 1)),\n               position = position_nudge(x = 0.15), vjust = -0.5) +\n  # Add geom_text layer for displaying mean dot in red\n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 3, color = \"red\",\n               position = position_nudge(x = 0.0)) +\n    stat_summary(fun = mean, geom = \"text\", aes(label = round(..y.., 0)),\n               position = position_nudge(x = 0.15), vjust = 2.5, color = \"red\") \n\n\n\n\n\n\n\\(**That's\\) \\(all\\) \\(folks!**\\)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5b.html",
    "href": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5b.html",
    "title": "9 Visualise Network Data",
    "section": "",
    "text": "(First Published: May 12, 2023)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5b.html#learning-outcome",
    "href": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5b.html#learning-outcome",
    "title": "9 Visualise Network Data",
    "section": "9.1 Learning Outcome",
    "text": "9.1 Learning Outcome\nBy the end of this hands-on exercise, we will be able to:\n\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph,\nbuild network graph visualisation using appropriate functions of ggraph,\ncompute network geometrics using tidygraph,\nbuild advanced graph visualisation by incorporating the network geometrics, and\nbuild interactive network visualisation using visNetwork package."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5b.html#getting-started",
    "href": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5b.html#getting-started",
    "title": "9 Visualise Network Data",
    "section": "9.2 Getting Started",
    "text": "9.2 Getting Started\n\n9.2.1 Install and load the required r libraries\nInstall and load the the required R packages. The name and function of the new packages that will be used for this exercise are as follow:\n\ngraph: The ‚Äògraph‚Äô package is used for creating, manipulating, and analyzing graphs and networks\ntidygraph: Builds on top of the graph package and extends it with the principles of the ‚Äòtidyverse‚Äô. It allows for a tidy data approach to working with graph data by providing a grammar of graph manipulation.\nggraph: For visualizing graphs and networks. It provides a flexible and intuitive grammar of graphics interface for creating customized and aesthetically pleasing network visualizations.\nvisNetwork: For creating interactive network plots with features like zooming, panning, tooltips, and filtering.\nlubridate: Provides convenient functions to parse, manipulate, and work with dates and times.\nclock: A modern alternative to ‚Äòlubridate‚Äô for handling date and time data.\ngraphlayouts: Provides various layout algorithms for visualizing graphs and networks.\n\n\n\nShow the code\npacman::p_load(igraph, tidygraph, ggraph, visNetwork, lubridate, clock,               tidyverse, graphlayouts)\n\n\n\n\n9.2.2 Import the data\nThe data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets:\n\nGAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees\n\n\n\nShow the code\nGAStech_nodes <- read_csv(\"data/GAStech_email_node.csv\", show_col_types = FALSE)\nGAStech_edges <- read_csv(\"data/GAStech_email_edge-v2.csv\", show_col_types = FALSE)\n\n\nReview the imported data\nWe will examine the structure of the data frame using glimpse() of dplyr.\n\n\nShow the code\nglimpse(GAStech_edges)\n\n\nRows: 9,063\nColumns: 8\n$ source      <dbl> 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26‚Ä¶\n$ target      <dbl> 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29‚Ä¶\n$ SentDate    <chr> \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"‚Ä¶\n$ SentTime    <time> 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0‚Ä¶\n$ Subject     <chr> \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP‚Ä¶\n$ MainSubject <chr> \"Work related\", \"Work related\", \"Work related\", \"Work rela‚Ä¶\n$ sourceLabel <chr> \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr‚Ä¶\n$ targetLabel <chr> \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc‚Ä¶\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as ‚ÄúCharacter‚Äù data type instead of date data type. Before we continue, it is important for us to change the data type of SentDate field back to ‚ÄúDate‚Äù‚Äù data type.\nCreate a new column for week day\nWe will use the dmy() and wday() functions of lubridate package to convert SentDate to ‚ÄòDate‚Äô data type. To note the following:\n\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The abbr argument is set to FALSE to spell the day of the week in full, i.e.¬†Monday. The function will create a new column in the data.frame i.e.¬†Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale.\n\n\n\nShow the code\nGAStech_edges <- GAStech_edges %>%\n  mutate(SendDate = dmy(SentDate)) %>%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\nRemove unneeded records\nA close examination of GAStech_edges data.frame reveals that it consists of:\n\nnon-worked related emails\nweekends\nself-directed e-mails where the individuals send an email to themselves\nemails that were only sent once\n\nWe will wrangle the data to address these issues.\n\n\nShow the code\nGAStech_edges_aggregated <- GAStech_edges %>%\n  filter(MainSubject == \"Work related\") %>%\n  group_by(source, target, Weekday) %>%\n    summarise(Weight = n()) %>%\n  filter(source!=target) %>%\n  filter(Weight > 1) %>%\n  ungroup()"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5b.html#create-network-objects-using-tidygraph",
    "href": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5b.html#create-network-objects-using-tidygraph",
    "title": "9 Visualise Network Data",
    "section": "9.3 Create network objects using tidygraph",
    "text": "9.3 Create network objects using tidygraph\nWhile network data itself is not tidy, it can be envisioned as two tidy tables - one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating the data. Furthermore the package provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\nThese two articles provide useful information on tidygraph:\n\nIntroducing tidygraph\ntidygraph 1.1 - A tidy hope\n\n\n9.3.1 The tbl_graph object\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\n9.3.2 The dplyr verbs in tidygraph\nThe activate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n In the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\n9.3.3 Using tbl_graph() to build tidygraph data model.\nWe will use tbl_graph() of tinygraph package to build an tidygraph‚Äôs network graph data.frame.\n\n\n\n\n\n\nTip\n\n\n\nBefore typing the codes, you are recommended to review to reference guide of tbl_graph()\n\n\n\n\nShow the code\nGAStech_graph <- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\nLet‚Äôs take a look at the output tidygraph‚Äôs graph object.\n\n\nShow the code\nGAStech_graph\n\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# A tibble: 54 √ó 4\n     id label               Department     Title                                \n  <dbl> <chr>               <chr>          <chr>                                \n1     1 Mat.Bramar          Administration Assistant to CEO                     \n2     2 Anda.Ribera         Administration Assistant to CFO                     \n3     3 Rachel.Pantanal     Administration Assistant to CIO                     \n4     4 Linda.Lagos         Administration Assistant to COO                     \n5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Manag‚Ä¶\n6     6 Carla.Forluniau     Administration Assistant to IT Group Manager        \n# ‚Ñπ 48 more rows\n#\n# A tibble: 1,372 √ó 4\n   from    to Weekday Weight\n  <int> <int> <ord>    <int>\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ‚Ñπ 1,369 more rows\n\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 1372 edges. The command also prints the first six rows of ‚ÄúNode Data‚Äù and the first three of ‚ÄúEdge Data‚Äù.\nChanging the active object\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest ‚Äúweight‚Äù first, we could use activate() and then arrange().\nFor example,\n\n\nShow the code\nGAStech_graph %>%\n  activate(edges) %>%\n  arrange(desc(Weight))\n\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# A tibble: 1,372 √ó 4\n   from    to Weekday  Weight\n  <int> <int> <ord>     <int>\n1    40    41 Saturday     13\n2    41    43 Monday       11\n3    35    31 Tuesday      10\n4    40    41 Monday       10\n5    40    43 Monday       10\n6    36    32 Sunday        9\n# ‚Ñπ 1,366 more rows\n#\n# A tibble: 54 √ó 4\n     id label           Department     Title           \n  <dbl> <chr>           <chr>          <chr>           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ‚Ñπ 51 more rows\n\n\n\n\n\n\n\n\nTip\n\n\n\nVisit the reference guide of activate() to find out more about the function."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5b.html#plot-static-network-graphs-with-ggraph-package",
    "href": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5b.html#plot-static-network-graphs-with-ggraph-package",
    "title": "9 Visualise Network Data",
    "section": "9.4 Plot Static Network Graphs with ggraph package",
    "text": "9.4 Plot Static Network Graphs with ggraph package\nggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph‚Äôs network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\nFor a comprehensive discussion of each of this aspect of graph, please refer to their respective vignettes provided.\n\n9.4.1 Plot a basic network graph\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before we get started, it is advisable to read their respective reference guide at least once.\n\n\nShow the code\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.\n\n\n\n\n9.4.2 Change the default network graph theme\nWe can use theme_graph() to remove the x and y axes.\n\n\nShow the code\ng <- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nggraph introduces a special ggplot theme that provides better defaults for network graphs than the normal ggplot defaults. theme_graph(), besides removing axes, grids, and border, changes the font to Arial Narrow (this can be overridden).\nThe ggraph theme can be set for a series of plots with the set_graph_style() command run before the graphs are plotted or by using theme_graph() in the individual plots.\n\n\n\n\n\n9.4.3 Change the coloring of the plot\nThe theme_graph() funtion makes it easy to change the coloring of the plot\n\n\nShow the code\ng <- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\n9.4.4 Working with different graph layouts\nggraph supports many layouts for standard use. They are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl.\nThe figures below are supported by ggraph().  9.4.5 Fruchterman and Reingold layout\nWe can use the layout argument to the ggraph() function to specify the preferred layout.\n\n\nShow the code\ng <- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n9.4.5 Modify the network nodes\nWe can use the aes() function within the geom_node_point() function to assign the nodes with colors based on the department of the employees.\n\n\nShow the code\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\ngeom_node_point() is equivalent in functionality to geom_point() of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the code chunck above, colour and size are used.\n\n\n\n\n9.4.6 Modify the edges\nSimilarly, we can use the aes() function within the geom_edge_link() function to map the thickness of the edges with the Weight variable.\n\n\nShow the code\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\ngeom_edge_link() draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5b.html#creating-facet-graphs",
    "href": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5b.html#creating-facet-graphs",
    "title": "9 Visualise Network Data",
    "section": "9.5 Creating facet graphs",
    "text": "9.5 Creating facet graphs\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, we will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\n9.5.1 Working with facet_edges()\nWe will use facet_edges() to generate graphs based on the day of the week.\n\n\nShow the code\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\nWe can change the position of the legend using the theme() layer.\n\n\nShow the code\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\nFurther tweaks to add frame and a subtitle border to each graph can be made via the th_background() function.\n\n\nShow the code\nset_graph_style() \n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n9.5.2 Working with facet_node()\nWe use the facet_node() function to generate departmental graphs in small multiples.\n\n\nShow the code\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5b.html#network-metric-analysis",
    "href": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5b.html#network-metric-analysis",
    "title": "9 Visualise Network Data",
    "section": "9.6 Network Metric Analysis",
    "text": "9.6 Network Metric Analysis\n\n9.6.1Compute Centrality Indices\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Go online to find out more!\nIn the graph below, we compute the Betweenness Centrality (a measure of how often the node serves as a bridge in the shortest path of other node pairs) of the nodes first and map it to the size of the nodes. Hence, the bigger the node, the higher its centrality score.\n\n\nShow the code\ng <- GAStech_graph %>%\n  mutate(betweenness_centrality = centrality_betweenness()) %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\nFrom ggraph v2.0 onwards, tidygraph algorithms such as centrality measures can be accessed directly in ggraph() calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\n\nShow the code\ng <- GAStech_graph %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\n9.6.2 Visualise Community\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph.\nIn the following graph, group_edge_betweenness() function is used to detect the communities.\n\n\nShow the code\ng <- GAStech_graph %>%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5b.html#build-interactive-network-graphs-with-visnetwork-package",
    "href": "Hands-On_Ex/Hands-On_Ex05/Hands-On_Ex5b.html#build-interactive-network-graphs-with-visnetwork-package",
    "title": "9 Visualise Network Data",
    "section": "9.7 Build interactive network graphs with visNetwork package",
    "text": "9.7 Build interactive network graphs with visNetwork package\nvisNetwork is a R package for network visualization, using vis.js javascript library.\nThe visNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an ‚Äúid‚Äù column, and the edge list must have ‚Äúfrom‚Äù and ‚Äúto‚Äù columns.\nThe function also plots the labels for the nodes, using the names of the actors from the ‚Äúlabel‚Äù column in the node list.\n\nThe resulting graph is fun to play around with.\n\nWe can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nWe can also zoom in and out on the plot and move it around to re-center it.\n\n\n9.7.1 Data Preparation\nBefore we can plot the interactive network graph, we need to prepare the data by combining the 2 data sets and then group + filter the results.\n\n\nShow the code\nGAStech_edges_aggregated <- GAStech_edges %>%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %>%\n  rename(from = id) %>%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %>%\n  rename(to = id) %>%\n  filter(MainSubject == \"Work related\") %>%\n  group_by(from, to) %>%\n    summarise(weight = n()) %>%\n  filter(from!=to) %>%\n  filter(weight > 1) %>%\n  ungroup()\n\n\n\n\n9.7.2 Plot the basic interactive network graph\nIn the graph below, the Fruchterman and Reingold layout is used.\n\n\nShow the code\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nVisit Igraph to find out more about visIgraphLayout‚Äôs argument.\n\n\n\n\n9.7.3 Working with visual attributes - Nodes\nvisNetwork() looks for a field called ‚Äúgroup‚Äù in the nodes object and colour the nodes according to the values of the group field. Hence, we will have to rename Department field to group.\nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\n\n\nShow the code\n# Rename Department attribute to group\nGAStech_nodes <- GAStech_nodes %>%\n  rename(group = Department)\n\n# Re-gen the network group\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n9.7.4 Working with visual attributes - Edges\nWe updated the following arguments in the visEdges() function to generate the graph below:\n\nThe arrows argument is used to define where to place the arrow.\nThe smooth argument is used to plot the edges using a smooth curve.\n\n\n\nShow the code\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nVisit Option to find out more about visEdges‚Äôs argument.\n\n\n\n\n9.7.5 Additional Interactivity\nNext, we updated the following arguments in the visOptions() function to generate the graph below:\n\nThe highlightNearest argument to highlight nearest when clicking a node.\nThe nodesIdSelection argument adds an id node selection creating an HTML select element.\n\n\n\nShow the code\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nVisit Option to find out more about visOption‚Äôs argument.\n\n\n\n\\(**That's\\) \\(all\\) \\(folks!**\\)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6a.html",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6a.html",
    "title": "10 Ternary Plot to display composition of 3 variables",
    "section": "",
    "text": "(First Published: May 17, 2023)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6a.html#learning-outcome",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6a.html#learning-outcome",
    "title": "10 Ternary Plot to display composition of 3 variables",
    "section": "10.1 Learning Outcome",
    "text": "10.1 Learning Outcome\nWe will learn how to build ternary plot programmatically using R to visualise and analyse population structure of Singapore.\nTernary plots display the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) It‚Äôs display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6a.html#getting-started",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6a.html#getting-started",
    "title": "10 Ternary Plot to display composition of 3 variables",
    "section": "10.2 Getting Started",
    "text": "10.2 Getting Started\n\n10.2.1 Install and load the required r libraries\nInstall and load the the required R packages. The name and function of the new package that will be used for this exercise is as follow:\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\n\n\n\n\n\n\n\nNote on ggtern\n\n\n\nDue to some technical issue, ggtern is currently not available for downloading via cran. We need to download ggtern from the archive by using the code chunk below. The latest archive version is 3.4.1.\n\n\n\n\nShow the code\n# Codes below are used to install ggtern from archive\nrequire(devtools)\ninstall_version(\"ggtern\", version = \"3.4.1\", repos = \"http://cran.us.r-project.org\")\n\n\n(The codes above to install ggtern should only run once)\n\n\nShow the code\nlibrary(plotly)\nlibrary(tidyverse)\nlibrary(ggtern)\n\n\n\n\n10.2.2 Import the data\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set is called respopagsex2000to2018_tidy.csv and is in csv file format.\n\n\nShow the code\n#Reading the data into R environment\npop_data <- read_csv(\"data/respopagsex2000to2018_tidy.csv\", show_col_types = FALSE) \n\n\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n\nShow the code\n#Deriving the young, economy active and old measures\nagpop_mutated <- pop_data %>%\n  mutate(`Year` = as.character(Year))%>%\n  pivot_wider(names_from = AG, values_from = Population) %>%\n  mutate(YOUNG = rowSums(.[4:8]))%>%\n  mutate(ACTIVE = rowSums(.[9:16]))  %>%\n  mutate(OLD = rowSums(.[17:21])) %>%\n  mutate(TOTAL = rowSums(.[22:24])) %>%\n  filter(Year == 2018)%>%\n  filter(TOTAL > 0)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6a.html#plotting-static-ternary-diagram-with-ggtern",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6a.html#plotting-static-ternary-diagram-with-ggtern",
    "title": "10 Ternary Plot to display composition of 3 variables",
    "section": "10.3 Plotting Static Ternary Diagram with ggtern",
    "text": "10.3 Plotting Static Ternary Diagram with ggtern\nWe can use ggtern() function of ggtern package to create a simple ternary plot.\n\n\nShow the code\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\nLet‚Äôs beautify the plot.\n\n\nShow the code\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"2018 Population Structure based on Subzones\") +\n  theme_rgbw()"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6a.html#plotting-interactive-ternary-diagram-with-plotly",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6a.html#plotting-interactive-ternary-diagram-with-plotly",
    "title": "10 Ternary Plot to display composition of 3 variables",
    "section": "10.4 Plotting Interactive Ternary Diagram with plotly",
    "text": "10.4 Plotting Interactive Ternary Diagram with plotly\nWe can create an interactive ternary plot using plot_ly() function of Plotly R.\nüñ±Ô∏èMouse over the dots below to check out the proportion of each population group for the subzones!\n\n\nShow the code\n# reusable function for creating annotation object\nlabel <- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 10,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 20, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis <- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes <- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD,\n  color = I(\"black\"), \n  type = \"scatterternary\",\n  text = ~SZ   # Added this argument to show the name of the subzone\n) %>%\n  layout(\n    annotations = label(\"2018 Population Structure\\nbased on Subzones\"), \n    ternary = ternaryAxes\n  ) \n\n\n\n\n\n\n\n\\(**That's\\) \\(all\\) \\(folks!**\\)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6b.html",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6b.html",
    "title": "11 Visual Correlation Analysis of Numerical and Categorical Data",
    "section": "",
    "text": "(First published: May 17. 2023)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6b.html#learning-outcome",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6b.html#learning-outcome",
    "title": "11 Visual Correlation Analysis of Numerical and Categorical Data",
    "section": "11.1 Learning Outcome",
    "text": "11.1 Learning Outcome\nWe will learn how to plot data visualisation for visualising correlation matrix with R using 3 methods:\n\nCreate correlation matrix using pairs() of R Graphics\nPlot corrgram using corrplot package of R\nCreate an interactive correlation matrix using plotly\n\nWhen multivariate data is used, the correlation coefficients of each pair of variables are displayed in a table form known as correlation matrix or scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix:\n\nTo reveal the relationship between high-dimensional variables pair-wisely.\nAs input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression‚Äôs estimates will be unreliable.\n\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that ‚Äúsimilar‚Äù variables are positioned adjacently, facilitating perception."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6b.html#getting-started",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6b.html#getting-started",
    "title": "11 Visual Correlation Analysis of Numerical and Categorical Data",
    "section": "11.2 Getting Started",
    "text": "11.2 Getting Started\n\n11.2.1 Install and load the required r libraries\nInstall and load the the required R packages. The name and function of the new package that will be used for this exercise is as follow:\n\ncorrplot : for creating correlation matrices and correlation plots\n\n\n\nShow the code\npacman::p_load(corrplot, ggstatsplot, tidyverse)\n\n\n11.2.2 Import the data\nWe will use the Wine Quality Data Set of UCI Machine Learning Repository. The data set consists of 13 variables and 6497 observations. For this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\n\n\nShow the code\nwine <- read_csv(\"data/wine_quality.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6b.html#building-correlation-matrix-with-pairsfunction",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6b.html#building-correlation-matrix-with-pairsfunction",
    "title": "11 Visual Correlation Analysis of Numerical and Categorical Data",
    "section": "11.3 Building Correlation Matrix with pairs()function",
    "text": "11.3 Building Correlation Matrix with pairs()function\nWe can create a scatterplot matrix by using the pairs function of R Graphics.\n\n\nShow the code\npairs(wine[,1:11])\n\n\n\n\n\nThe required input of pairs() can be a matrix or data frame.\nColumns 2 to 12 of wine dataframe is used to build the next scatterplot matrix. The variables are: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates and alcohol.\n\n\nShow the code\npairs(wine[,2:12])\n\n\n\n\n\nDrawing the lower corner\npairs() function of R Graphics provided many customisation arguments. For example, it is a common practice to show either the upper half or lower half of the correlation matrix instead of both. This is because a correlation matrix is symmetric.\nTo show the lower half of the correlation matrix, the upper.panel argument will be used.\n\n\nShow the code\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\nSimilarly, we can display the upper half of the correlation matrix.\n\n\nShow the code\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\nIncluding with correlation coefficients\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor() function will be used. This will also show higher correlations in a larger font.\n\n\nShow the code\npanel.cor <- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr <- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr <- abs(cor(x, y, use=\"complete.obs\"))\ntxt <- format(c(r, 0.123456789), digits=digits)[1]\ntxt <- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6b.html#visualise-correlation-matrix-using-ggcormat-function",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6b.html#visualise-correlation-matrix-using-ggcormat-function",
    "title": "11 Visual Correlation Analysis of Numerical and Categorical Data",
    "section": "11.4 Visualise Correlation Matrix using ggcormat() function",
    "text": "11.4 Visualise Correlation Matrix using ggcormat() function\nOne of the major limitation of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e.¬†more than 500 observations). To overcome this problem, Corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used.\nThere are at least three R packages provide function to plot corrgram, they are:\n\ncorrgram\nellipse\ncorrplot\n\nOn top that, some R packages like ggstatsplot package also provides functions for building corrgram.\nIn this section, you will learn how to visualise correlation matrix by using ggcorrmat() of ggstatsplot package.\nThe basic plot\nggcorrmat() can provide a comprehensive and yet professional statistical report as shown in the figure below.\n\n\nShow the code\nset.seed(123)\nlibrary(ggcorrplot)\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11\n)\n\n\n\n\n\n\n\n\n\n\n\nOn ggcorrmat()\n\n\n\nThe ggcorrmat() function from the ggstatsplot package can conflict with the titleGrob() function from the ggpubr package. Both packages have functions with the same name, which is why we have to prefix the function with ‚Äúggstatsplot::‚Äù.\n\n\nLet‚Äôs touch up the plot.\n\n\nShow the code\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs of variables are not significant at p < 0.05\"\n)\n\n\n\n\n\nThings to note:\n\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram.\nggcorrplot.args argument provide additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot() function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\nThe following codes can be used to control specific components of the plot such as the font size of the x-axis, y-axis and the statistical report.\n\n\nShow the code\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))\n\n\nBuilding multiple plots\nSince ggstasplot is an extension of ggplot2, it also supports faceting. However the feature is not available in ggcorrmat() but in the grouped_ggcorrmat() of ggstatsplot.\n\n\nShow the code\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 1),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 3),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  ),\n  ggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))\n) \n\n\n\n\n\nThings to note:\n\nto build a facet plot, the only argument needed is grouping.var.\nBehind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which was separately specified earlier.\nLikewise, annotation.args argument is calling plot annotation arguments of patchwork package."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6b.html#visualise-correlation-matrix-using-corrplot-package",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6b.html#visualise-correlation-matrix-using-corrplot-package",
    "title": "11 Visual Correlation Analysis of Numerical and Categorical Data",
    "section": "11.5 Visualise Correlation Matrix Using corrplot package",
    "text": "11.5 Visualise Correlation Matrix Using corrplot package\nGetting started with corrplot Before we can plot a corrgram using corrplot(), we need to compute the correlation matrix of wine data frame. Thereafter, corrplot() is used to plot the corrgram by using all the default settings.\n\n\nShow the code\n# Compute the correlation matrix\nwine.cor <- cor(wine[, 1:11])\n\n# Create the basic corrplot\ncorrplot(wine.cor)\n\n\n\n\n\nNotice that the default visual object used to plot the corrgram is circle. The default layout of the corrgram is a symmetric matrix. The default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients. The intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship.\nWorking with visual geometrics\nIn corrplot package, there are seven visual geometrics (parameter method) can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle. As shown in the previous section, the default visual geometric of corrplot matrix is circle. However, this default setting can be changed by using the method argument.\n\n\nShow the code\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\nWorking with layout\ncorrplor() supports three layout types, namely: ‚Äúfull‚Äù, ‚Äúupper‚Äù or ‚Äúlower‚Äù. The default is ‚Äúfull‚Äù which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\n\nShow the code\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, arguments diag and tl.col are used to turn off the diagonal cells and to change the axis text label colour to black colour respectively.\n\n\nShow the code\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\nWe can also experiment with other layout design argument such as tl.pos, tl.cex, tl.offset, cl.pos, cl.cex and cl.offset,\nWorking with mixed layout\nWith corrplot package, it is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used.\n\n\nShow the code\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\nNotice that argument lower and upper are used to define the visualisation method used. In this case ellipse is used to map the lower half of the corrgram and numerical matrix (i.e.¬†number) is used to map the upper half of the corrgram. The argument tl.pos, on the other, is used to specify the placement of the axis label. Lastly, the diag argument is used to specify the glyph on the principal diagonal of the corrgram.\n\n11.5.1 Combining corrgram with the significant test\nIn statistical analysis, we are also interested to know which pair of variables their correlation coefficients are statistically significant.\nThee figure below shows a corrgram combined with the significant test. The corrgram reveals that not all correlation pairs are statistically significant. For example the correlation between total sulfur dioxide and free sulfur dioxide is statistically significant at significant level of 0.1 but not the pair between total sulfur dioxide and citric acid.\nWe can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\n\nShow the code\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\n\nWe can then include the results in the p.mat argument of corrplot() function.\n\n\nShow the code\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\nReorder a corrgram\nMatrix reorder is very important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e.¬†‚Äúoriginal‚Äù). The default setting can be over-write by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are:\n\n‚ÄúAOE‚Äù is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n‚ÄúFPC‚Äù for the first principal component order.\n‚Äúhclust‚Äù for hierarchical clustering order, and ‚Äúhclust.method‚Äù for the agglomeration method to be used.\n\n‚Äúhclust.method‚Äù should be one of ‚Äúward‚Äù, ‚Äúsingle‚Äù, ‚Äúcomplete‚Äù, ‚Äúaverage‚Äù, ‚Äúmcquitty‚Äù, ‚Äúmedian‚Äù or ‚Äúcentroid‚Äù.\n\n‚Äúalphabet‚Äù for alphabetical order.\n\n‚ÄúAOE‚Äù, ‚ÄúFPC‚Äù, ‚Äúhclust‚Äù, ‚Äúalphabet‚Äù. More algorithms can be found in seriation package.\n\n\nShow the code\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\nReordering a correlation matrix using hclust\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\n\nShow the code\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6b.html#references",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6b.html#references",
    "title": "11 Visual Correlation Analysis of Numerical and Categorical Data",
    "section": "11.6 References",
    "text": "11.6 References\n\nMichael Friendly (2002). ‚ÄúCorrgrams: Exploratory displays for correlation matrices‚Äù. The American Statistician, 56, 316‚Äì324.\nD.J. Murdoch, E.D. Chow (1996). ‚ÄúA graphical display of large correlation matrices‚Äù. The American Statistician, 50, 178‚Äì180."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6c.html",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6c.html",
    "title": "12 Heatmap to visualise Multivariate Data",
    "section": "",
    "text": "(First Published: May 18, 2023)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6c.html#learning-outcome",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6c.html#learning-outcome",
    "title": "12 Heatmap to visualise Multivariate Data",
    "section": "12.1 Learning Outcome",
    "text": "12.1 Learning Outcome\nWe will learn to plot static and interactive heatmap for visualising and analysing multivariate data.\nHeatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6c.html#getting-started",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6c.html#getting-started",
    "title": "12 Heatmap to visualise Multivariate Data",
    "section": "12.2 Getting Started",
    "text": "12.2 Getting Started\n\n12.2.1 Install and load the required r libraries\nInstall and load the the required R packages. The name and function of the new packages that will be used for this exercise are as follow:\n\nseriation : includes various algorithms and methods for ordering objects and variables, such as hierarchical clustering-based seriation, matrix reordering, and optimal leaf ordering\nheatmaply : used for creating interactive and customizable heatmaps\ndendextend : for manipulating and enhancing dendrograms\n\n\n\nShow the code\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)\n\n\n\n\n12.2.2 Import the data\nWe import the data set for World Happines 2018 report and assign it to wh.\n\n\nShow the code\n# Import data into R\nwh <- read_csv(\"data/WHData-2018.csv\", show_col_types = FALSE)\n\n\nTransform the data frame into a matrix\nWe need to transform the data frame to a data matrix to generate the heatmap. Before generating the data matrix, we will need to:\n\nSelect country and the relevant attributes which we are keen to visualise.\nChange the default row name (which is in a numeric series) to the country name.\n\n\n\nShow the code\n# Select country and attributes to visualise \nwh1 <- dplyr::select(wh, c(1,3, 7:12))\n\n# Set country column as rownames\nrow.names(wh1) <- wh1$Country\n\n# Generate the data matrix\nwh_matrix <- data.matrix(wh1) \n\n\nWe will exclude the country column from the data matrix since the information is now captured as row names.\n\n\nShow the code\nwh_matrix <- wh_matrix[,-1]"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6c.html#plot-static-heatmap",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6c.html#plot-static-heatmap",
    "title": "12 Heatmap to visualise Multivariate Data",
    "section": "12.3 Plot static Heatmap",
    "text": "12.3 Plot static Heatmap\nThe R packages and functions for drawing static heatmaps are:\n\nheatmap() of R stats package. It draws a simple heatmap.\nheatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function.\npheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here.\n\nNext, we will learn how to plot static heatmaps by using heatmap() of R Stats package.\n\n\n\n\n\n\nNote\n\n\n\n\nBy default, heatmap() plots a cluster dendrogram heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\nIf we were to generate the default dendrogram heatmap (without specifying the Rowv and Colv arguments), the order of both rows and columns maybe be different from the plot below. This is because the dendrogram feature will do a reordering using clustering: it calculates the distance between each pair of rows and columns, and try to group and order them by similarity.\n\n\n\n\n\nShow the code\nwh_heatmap <- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA,\n                      cexRow = 0.6, \n                      cexCol = 0.6)\n\n\n\n\n\nThis heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns.\nThe plot below has its column-wise values normalised.\nNotice that the values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively.\n\n\nShow the code\nwh_heatmap <- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.6,\n                      margins = c(10, 4))"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6c.html#create-interactive-heatmap",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6c.html#create-interactive-heatmap",
    "title": "12 Heatmap to visualise Multivariate Data",
    "section": "12.4 Create Interactive Heatmap",
    "text": "12.4 Create Interactive Heatmap\nheatmaply is an R package for building interactive cluster heatmap that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili.\n\n12.4.1 The basic plot with heatmaply\nWe will use heatmaply to design an interactive cluster heatmap. We will still use the wh_matrix as the input data.\n\n\n\n\n\n\nNote\n\n\n\nWe will extract a subset of 30 records from the wh_matrix to make it easier for us to see the effects of each tweak we are making to the plot.\n\n\n\n\nShow the code\nheatmaply(wh_matrix[1:30,],\n          cexRow = 0.6,\n          cexCol = 0.6)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the right hand side of the heatmap.\nThe text label of each raw, on the other hand, is placed on the left hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\n\n\n12.4.2 Data Transformation\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables‚Äô values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentilse.\n\n12.4.2.1 Scaling\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution. In such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling. In the plot behind, we scale variable values columewise.\n\n\nShow the code\nheatmaply(wh_matrix[1:30,],\n          scale = \"column\",\n          cexRow = 0.6,\n          cexCol = 0.6)\n\n\n\n\n\n\n\n\n12.4.2.2 Normalising\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variable‚Äôs distribution while making them easily comparable on the same ‚Äúscale‚Äù.\nDifferent from Scaling, the normalise method is performed on the input data set i.e.¬†wh_matrix as shown in the code chunk below.\n\n\nShow the code\nheatmaply(normalize(wh_matrix[1:30,]),\n          cexRow = 0.6,\n          cexCol = 0.6)\n\n\n\n\n\n\n\n\n12.4.2.3 Percentising\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\n\nSimilar to Normalize method, the Percentize method is also performed on the input data set instead of using an argument.\n\n\nShow the code\nheatmaply(percentize(wh_matrix[1:30,]),\n          cexRow = 0.6,\n          cexCol = 0.6)\n\n\n\n\n\n\n\n\n\n12.4.3 Clustering algorithm\nheatmaply supports a variety of hierarchical clustering algorithms. The main arguments to provide are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. To use correlation-based clustering, we can use options ‚Äúpearson‚Äù, ‚Äúspearman‚Äù or ‚Äúkendall‚Äù, which use as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method: default is NULL, which results in ‚Äúeuclidean‚Äù to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is ‚Äúdist‚Äù‚Äù hence this can be one of ‚Äúeuclidean‚Äù, ‚Äúmaximum‚Äù, ‚Äúmanhattan‚Äù, ‚Äúcanberra‚Äù, ‚Äúbinary‚Äù or ‚Äúminkowski‚Äù.\nhclust_method default is NULL, which results in ‚Äúcomplete‚Äù method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of ‚Äúward.D‚Äù, ‚Äúward.D2‚Äù, ‚Äúsingle‚Äù, ‚Äúcomplete‚Äù, ‚Äúaverage‚Äù (= UPGMA), ‚Äúmcquitty‚Äù (= WPGMA), ‚Äúmedian‚Äù (= WPGMC) or ‚Äúcentroid‚Äù (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n12.4.3.1 Manual approach\nThe following heatmap is plotted by using hierachical clustering algorithm with ‚ÄúEuclidean distance‚Äù and ‚Äúward.D‚Äù method.\n\n\nShow the code\nheatmaply(normalize(wh_matrix[1:30,]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\",\n          cexRow = 0.6,\n          cexCol = 0.6)\n\n\n\n\n\n\n\n\n12.4.3.2 Statistical approach\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\n\nShow the code\nwh_d <- dist(normalize(wh_matrix[1:30,]), method = \"euclidean\")\n\nresults <- dend_expend(wh_d)[[3]]\n\nresults[order(-results$optim), ]\n\n\n  dist_methods hclust_methods     optim\n5      unknown        average 0.7814164\n6      unknown       mcquitty 0.7793163\n8      unknown       centroid 0.7680779\n3      unknown         single 0.7432104\n7      unknown         median 0.6990172\n4      unknown       complete 0.6959322\n2      unknown        ward.D2 0.5690666\n1      unknown         ward.D 0.4642559\n\n\nThe output table shows that ‚Äúaverage‚Äù method should be used because it gives the highest optimum value. Next, find_k() is used to determine the optimal number of cluster.\n\n\nShow the code\nwh_clust <- hclust(wh_d, method = \"average\")\nnum_k <- find_k(wh_clust)\nplot(num_k)\n\n\n\n\n\nThe figure above shows that k=3 would be good.\n\n\n\n\n\n\nOther methods to determine k\n\n\n\nCheck out my Geospatial Analytics page here where I shared 2 other methods - Elbow and Gap Statistic - to find the best k.\n\n\nUsing the statistical analysis results, we can prepare the hierarchical plot.\n\n\nShow the code\nheatmaply(normalize(wh_matrix[1:30,]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3,\n          cexRow = 0.6,\n          cexCol = 0.6)\n\n\n\n\n\n\n\n\n\n12.4.4 Seriation\nOne problem with hierarchical clustering is that it doesn‚Äôt actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can‚Äôt end up between A and B, but it doesn‚Äôt tell you which way to flip the A+B cluster. It doesn‚Äôt tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nHere we meet our first seriation algorithm: Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying OLO to the same clustering result as the heatmap above.\n\n\nShow the code\nheatmaply(normalize(wh_matrix[1:30,]),\n          seriate = \"OLO\",\n          cexRow = 0.6,\n          cexCol = 0.6)\n\n\n\n\n\n\nThe default options is ‚ÄúOLO‚Äù (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)). Another option is ‚ÄúGW‚Äù (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\n\nShow the code\nheatmaply(normalize(wh_matrix[1:30,]),\n          seriate = \"GW\",\n          cexRow = 0.6,\n          cexCol = 0.6)\n\n\n\n\n\n\nThe option ‚Äúmean‚Äù gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\n\nShow the code\nheatmaply(normalize(wh_matrix[1:30,]),\n          seriate = \"mean\",\n          cexRow = 0.6,\n          cexCol = 0.6)\n\n\n\n\n\n\nThe option ‚Äúnone‚Äù gives us a dendrogram without any rotation that is based on the data matrix.\n\n\nShow the code\nheatmaply(normalize(wh_matrix[1:30,]),\n          seriate = \"none\",\n          cexRow = 0.6,\n          cexCol = 0.6)\n\n\n\n\n\n\n\n\n12.4.5 Working with color palettes\nThe default colour palette uses by heatmaply is viridis. We can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nWe use the Blues colour palette of rColorBrewer for the following plot.\n\n\nShow the code\nheatmaply(normalize(wh_matrix[1:30,]),\n          seriate = \"none\",\n          colors = Blues,\n          cexRow = 0.6,\n          cexCol = 0.6)\n\n\n\n\n\n\n\n\n12.4.6 The finishing touch\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 5.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\nWhich countries are clustered together with üá∏üá¨?\n\n\nShow the code\nheatmaply(normalize(wh_matrix),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 5,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )\n\n\n\n\n\n\n\n\\(**That's\\) \\(all\\) \\(folks!**\\)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6d.html",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6d.html",
    "title": "13 Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "",
    "text": "(First Published: May 18, 2023)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6d.html#learning-outcome",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6d.html#learning-outcome",
    "title": "13 Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "13.1 Learning Outcome",
    "text": "13.1 Learning Outcome\nWe will learn how to create static and interactive Parallel Coordinates plots using various R packages.\nParallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), ‚ÄúThis certainly isn‚Äôt a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn‚Äôt in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.‚Äù For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6d.html#getting-started",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6d.html#getting-started",
    "title": "13 Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "13.2 Getting Started",
    "text": "13.2 Getting Started\n\n13.2.1 Install and load the required R libraries\nInstall and load the the required R packages. The name and function of the new packages that will be used for this exercise are as follow\n\nGGally : provides additional functions and tools for creating more complex and customized plots\nparallelPlot : for creating parallel coordinate plots\n\n\n\nShow the code\npacman::p_load(GGally, parallelPlot, tidyverse)\n\n\n\n\n13.2.2 Import the data\nWe import the data set for World Happines 2018 report and assign it to wh.\n\n\nShow the code\nwh <- read_csv(\"data/WHData-2018.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6d.html#create-static-parallel-coordinates-plot",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6d.html#create-static-parallel-coordinates-plot",
    "title": "13 Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "13.3 Create static parallel coordinates plot",
    "text": "13.3 Create static parallel coordinates plot\n\n13.3.1 A simple parallel coordinates plot\nWe will learn how to plot static parallel coordinates plot by using ggparcoord() of GGally package. Before getting started, it is a good practice to read the function description in detail.\n\n\nShow the code\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\n\nNotice that only two arguments, namely data and columns is used. data argument is used to map the data object (i.e.¬†wh) and columns is used to select the columns for preparing the parallel coordinates plot.\n\n\n13.3.2 Including a boxplot\nThe basic parallel coordinates failed to reveal any meaning understanding of the World Happiness measures. We will learn how to makeover the plot by using a collection of arguments provided by ggparcoord().\n\n\nShow the code\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.4,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ngroupColumn argument is used to group the observations (i.e.¬†parallel lines) by using a single variable (i.e.¬†Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.\n\n\n\n\n\n13.3.3 Parallel Coordinates plots with facet\nSince ggparcoord() is developed by extending ggplot2 package, we can combination use some of the ggplot2 function when plotting a parallel coordinates plot.\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\n\nShow the code\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region,ncol=3) +\n  theme(legend.position = \"top\")\n\n\n\n\n\nOne of the aesthetic defect of the current design is that some of the variable names overlap on x-axis.\n\n\n13.3.4 Rotating x-axis text label\nTo make the x-axis text label easy to read, let us rotate the labels by 30 degrees. We can rotate axis text labels using theme() function in ggplot2.\n\n\nShow the code\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region, ncol=3) +\n  theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 30, hjust=1))\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nTo rotate x-axis text labels, we use axis.text.x as argument to theme() function. And we specify element_text(angle = 30) to rotate the x-axis text by an angle 30 degree.\nRotating x-axis text labels to 30 degrees makes the label overlap with the plot and we can avoid this by adjusting the text location using hjust argument to theme‚Äôs text element with element_text(). We use axis.text.x as we want to change the look of x-axis text."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6d.html#create-interactive-parallel-coordinates-plot-with-parallelplot",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6d.html#create-interactive-parallel-coordinates-plot-with-parallelplot",
    "title": "13 Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "13.4 Create Interactive Parallel Coordinates Plot with parallelPlot",
    "text": "13.4 Create Interactive Parallel Coordinates Plot with parallelPlot\nparallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‚Äòhtmlwidgets‚Äô package and d3.js.\n\n13.4.1 The basic plot\nWe create an interactive parallel coordinates plot by using parallelPlot().\n\n\nShow the code\nwh1 <- wh %>%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\n\nNotice that some of the axis labels are too long. We will address this problem in the next step.\n\n\n13.4.2 Rotate axis label\nIn the code chunk below, rotateTitle argument is used to avoid overlapping axis labels.\n\n\nShow the code\nparallelPlot(wh1,\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\n13.4.3 Parallel coordinates plot with histogram\nhistoVisibility argument is used to plot histogram along the axis of each variables.\n\n\nShow the code\nhistoVisibility <- rep(TRUE, ncol(wh1))\n\nparallelPlot(wh1,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6d.html#references",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6d.html#references",
    "title": "13 Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "13.5 References",
    "text": "13.5 References\n\nggparcoord() of GGally package\nparcoords user guide\nparallelPlot\n\n\n\\(**That's\\) \\(all\\) \\(folks!**\\)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6e.html",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6e.html",
    "title": "14 Treemap Visualisation",
    "section": "",
    "text": "(First Published: May 20, 2023)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6e.html#learning-outcome",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6e.html#learning-outcome",
    "title": "14 Treemap Visualisation",
    "section": "14.1 Learning Outcome",
    "text": "14.1 Learning Outcome\nWe will learn how to create static and interactive Treemaps using various R packages."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6e.html#getting-started",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6e.html#getting-started",
    "title": "14 Treemap Visualisation",
    "section": "14.2 Getting Started",
    "text": "14.2 Getting Started\n\n14.2.1 Install and load the required R libraries\nInstall and load the the required R packages. The name and function of the new package that will be used for this exercise is as follow\n\ntreemap : provides the necessary functions, classes, and methods to generate treemaps\n\n\n\nShow the code\npacman::p_load(treemap, treemapify, tidyverse) \n\n\n\n\n14.2.2 Import the data\nWe import the data set for property sales in 2018 and assign it to realis2018.\n\n\nShow the code\nrealis2018 <- read_csv(\"data/realis2018.csv\", show_col_types = FALSE)\n\n\nData Wrangling and Manipulation\nThe data.frame realis2018 contains trasactional records which are highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No.¬†of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\nWe can use a two lines code approach to perform grouping without using the pipe (|)\n\n\nShow the code\nrealis2018_grouped <- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised <- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\n\n\n\n\nNote\n\n\n\nAggregation functions such as sum() and median() obey the usual rule of missing values: if there‚Äôs any missing value in the input, the output will be a missing value. The argument na.rm = TRUE removes the missing values prior to computation.\n\n\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name.\nA more efficient way to tackle the same processes by using the pipe, %>%:\n\n\nShow the code\nrealis2018_summarised <- realis2018 %>% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %>%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo learn more about pipe, visit this excellent article: Pipes in R Tutorial For Beginners."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6e.html#design-treemap-with-treemap-package",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6e.html#design-treemap-with-treemap-package",
    "title": "14 Treemap Visualisation",
    "section": "14.3 Design Treemap with treemap Package",
    "text": "14.3 Design Treemap with treemap Package\ntreemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments. In this section, we will only explore the major arguments for designing elegent and yet truthful treemaps.\nDesign a static Treemap\ntreemap() of treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, we will select records belonging to resale condominium property type from realis2018_selected data frame.\n\n\nShow the code\nrealis2018_selected <- realis2018_summarised %>%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\nNext, we create a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\n\nShow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2018\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThings to learn from the three arguments used:\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because it‚Äôs vaues will be used to map the sizes of the rectangles of the treemaps.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\nWorking with vColor and type arguments\nIn the following plot, we assign value to the type argument.\n\n\nShow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2018\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThings to note:\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e.¬†0-5000, 5000-10000, etc. using equal interval of 5000.\n\nColours in treemap package\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between 2 options is the default value for mapping. The ‚Äúvalue‚Äù treemap considers palette to be a diverging color palette (say ColorBrewer‚Äôs ‚ÄúRdYlBu‚Äù), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The ‚Äúmanual‚Äù treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n(A) The following plot shows a treemap with type = ‚Äòvalue‚Äô\n\n\nShow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2018\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThings to note:\n\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n(B) The following plot shows a treemap with type = ‚Äòmanual‚Äô\nThe range of the Median Unit Price is mapped linearly to the colour palette.\n\n\nShow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2018\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThings to note:\n\nThe colour scheme used is very confusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative.\n\nTo overcome this problem, a single colour palette such as Blues should be used.\n\n\nShow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2018\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nTreemap Layout\ntreemap() supports two popular treemap layouts, namely: ‚Äúsquarified‚Äù and ‚ÄúpivotSize‚Äù. The default is ‚ÄúpivotSize‚Äù.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\nThe following plot shows a treemap with algorithm = ‚Äòsquarified‚Äô\n\n\nShow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2018\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nUsing sortID When ‚ÄúpivotSize‚Äù algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\n\nShow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2018\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6e.html#design-treemap-using-treemapify-package",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6e.html#design-treemap-using-treemapify-package",
    "title": "14 Treemap Visualisation",
    "section": "14.4 Design Treemap using treemapify Package",
    "text": "14.4 Design Treemap using treemapify Package\ntreemapify is a R package specially developed to draw treemaps in ggplot2. In this section, we will learn how to designing treemps closely resemble treemaps designing in previous section by using treemapify. Before we getting started, you should read Introduction to ‚Äútreemapify‚Äù its user guide.\nDesign a basic treemap\n\n\nShow the code\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\nDefine sub-grouping\n(A) The following plot is grouped by Planning Region\n\n\nShow the code\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\n\n(B) Now, we group by ‚ÄúPlanning Region‚Äù as the 1st level and ‚ÄúPlanning Area‚Äù as the 2nd level\n\n\nShow the code\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\n\nWe add boundary lines to outline the 2 levels of grouping\n\n\nShow the code\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"#fee8c8\",\n                                size = 3) +\n  geom_treemap_subgroup_border(colour = \"#bdbdbd\")"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6e.html#design-interactive-treemap-using-d3treer",
    "href": "Hands-On_Ex/Hands-On_Ex06/Hands-On_Ex6e.html#design-interactive-treemap-using-d3treer",
    "title": "14 Treemap Visualisation",
    "section": "14.5 Design Interactive Treemap using d3treeR",
    "text": "14.5 Design Interactive Treemap using d3treeR\nInstall the d3treeR package using devtools.\n\n\nShow the code\nlibrary(devtools)\ninstall_github(\"timelyportfolio/d3treeR\")\n\n\n(The above codes should only be run once)\nLoad the d3treeR package\n\n\nShow the code\nlibrary(d3treeR)\n\n\n\nDesigning An Interactive Treemap\nThe plot requires two steps.\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\n\nShow the code\ntm <- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2018\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\nThen d3tree() is used to build an interactive treemap.\n\n\n\nShow the code\nd3tree(tm, rootname = \"Singapore\")\n\n\n\n\n\n\n\n\\(**That's\\) \\(all\\) \\(folks!**\\)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex07/Hands-On_Ex7a.html",
    "href": "Hands-On_Ex/Hands-On_Ex07/Hands-On_Ex7a.html",
    "title": "19 Visualise And Analyse Time-Oriented Data",
    "section": "",
    "text": "(First Published: Jul 5, 2023)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex07/Hands-On_Ex7a.html#learning-outcome",
    "href": "Hands-On_Ex/Hands-On_Ex07/Hands-On_Ex7a.html#learning-outcome",
    "title": "19 Visualise And Analyse Time-Oriented Data",
    "section": "19.1 Learning Outcome",
    "text": "19.1 Learning Outcome\nWe will learn how to create the following visualisations:\n\nplotting a calender heatmap,\nplotting a cycle plot,\nplotting a slopegraph, and\nplotting a horizon chart"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex07/Hands-On_Ex7a.html#getting-started",
    "href": "Hands-On_Ex/Hands-On_Ex07/Hands-On_Ex7a.html#getting-started",
    "title": "19 Visualise And Analyse Time-Oriented Data",
    "section": "19.2 Getting Started",
    "text": "19.2 Getting Started\n\n19.2.1 Install and load the required R libraries\nInstall and load the the required R packages. The name and function of the new packages that will be used for this exercise are as follow\n\nscales : provides various functions for scaling and formatting data\nviridis : provides color palettes that are perceptually uniform and work well for representing data in visualisations\ngridExtra : provides functions for arranging multiple plots on a page or within a plot\nreadxl :enables reading data from Microsoft Excel files (.xls and .xlsx) into R\nknitr :used for dynamic report generation in R\ndata.table :offers fast data manipulation and aggregation operations, making it useful for working with large datasets\nCGPfuntions : contains a function that is designed to automate the process of producing a Tufte style slopegraph using ggplot2.\n\n\n\nShow the code\npacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse, CGPfunctions)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex07/Hands-On_Ex7a.html#plotting-calendar-heatmap",
    "href": "Hands-On_Ex/Hands-On_Ex07/Hands-On_Ex7a.html#plotting-calendar-heatmap",
    "title": "19 Visualise And Analyse Time-Oriented Data",
    "section": "19.3 Plotting Calendar Heatmap",
    "text": "19.3 Plotting Calendar Heatmap\nIn this section, we will learn how to plot a calender heatmap programmetically by using ggplot2 package.\n\n19.3.1 Import the data\neventlog.csv file consists of 199,999 rows of time-series cyber attack records by a country. It is imported in R and assigned to the attacks data frame.\n\n\nShow the code\nattacks <- read_csv(\"data/eventlog.csv\", show_col_types = F)\n\n\n\n\n19.3.2 Examine the data structure\nkable() can be used to review the structure of the imported data frame.\n\n\nShow the code\nkable(head(attacks))\n\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\n\n\n19.3.3 Data Preparation\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, we will write a function to perform the task.\n\n\nShow the code\nmake_hr_wkday <- function(ts, sc, tz) {\n  real_times <- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt <- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\n\n\n\n\n\n\n\nThings to note:\n\n\n\n\nymd_hms() and hour() are from lubridate package, and\nweekdays() is a base R function.\n\n\n\nStep 2: Deriving the attacks tibble data frame\n\n\nShow the code\nwkday_levels <- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks <- attacks %>%\n  group_by(tz) %>%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %>% \n  ungroup() %>% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\n\n\n\n\n\n\nThings to note:\n\n\n\nBeside extracting the necessary data into attacks data frame, mutate() of dplyr package is used to convert wkday and hour fields into factor so they‚Äôll be ordered when plotting.\n\n\nThe table below shows the tidy tibble table after processing.\n\n\nShow the code\nkable(head(attacks))\n\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n19.3.4 Building a Calendar Heatmap\n\n\nShow the code\ngrouped <- attacks %>% \n  count(wkday, hour) %>% \n  ungroup() %>%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\nThings to note:\n\n\n\n\na tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\na new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1.\nscale_fill_gradient() function is used to creates a two colour gradient (low-high).\n\n\n\n\n\n19.3.5 Building Multiple Calendar Heatmaps\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, we can do the followings:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\n\nShow the code\nattacks_by_country <- count(\n  attacks, source_country) %>%\n  # percent() is a function from scales packege\n  mutate(percent = percent(n/sum(n))) %>%\n  arrange(desc(n))\n\n\nStep 2: Preparing the tidy data frame\nIn this step, we extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e.¬†top4_attacks).\n\n\nShow the code\ntop4 <- attacks_by_country$source_country[1:4]\ntop4_attacks <- attacks %>%\n  filter(source_country %in% top4) %>%\n  count(source_country, wkday, hour) %>%\n  ungroup() %>%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %>%\n  na.omit()\n\n\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\n\nShow the code\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex07/Hands-On_Ex7a.html#plotting-cycle-plot",
    "href": "Hands-On_Ex/Hands-On_Ex07/Hands-On_Ex7a.html#plotting-cycle-plot",
    "title": "19 Visualise And Analyse Time-Oriented Data",
    "section": "19.4 Plotting Cycle Plot",
    "text": "19.4 Plotting Cycle Plot\n\n19.4.1 Import the data\nThe codes below import arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\n\nShow the code\nair <- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\n\n19.4.2 Data Preparation\nNext, two new fields called month and year are derived from Month-Year field.\n\n\nShow the code\nair$month <- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year <- year(ymd(air$`Month-Year`))\n\n\nThen, we extract the data for the target country (i.e.¬†Vietnam)\n\n\nShow the code\nVietnam <- air %>% \n  select(`Vietnam`, \n         month, \n         year) %>%\n  filter(year >= 2010)\n\n\nThereafter, we compute year average arrivals by month.\n\n\nShow the code\nhline.data <- Vietnam %>% \n  group_by(month) %>%\n  summarise(avg_value = mean(`Vietnam`))\n\n\n\n\n19.4.3 Generating the Cycle Plot\n\n\nShow the code\n# Plot the graph\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=factor(year), # Set year to factor to ensure that x-axis label is formatted as 4-digit year\n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avg_value), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_text(angle = 90, hjust = 1),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"Year\") +\n  ylab(\"No. of Visitors\")+\n  # Added to rotate the x-axis labels 90 degrees clockwise so that they don't overlap\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  # Added this line to display year label every 3 years on the x-axis to reduce clutter and overlapping of the year\n  scale_x_discrete(breaks = function(x) x[seq(1, length(x), 3)])\n\n\n\n\n\n\n\n19.5 Plotting a Slopegraph\n\n\n\n\n\n\nNote:\n\n\n\nBefore getting start, make sure that CGPfunctions has been installed and loaded onto R environment. Then, refer to Using newggslopegraph to learn more about the function. Lastly, read more about newggslopegraph() and its arguments by referring to this link.\n\n\n\n\n19.5.1 Import the data\n\n\nShow the code\nrice <- read_csv(\"data/rice.csv\", show_col_types = F)\n\n\n\n\n19.5.2 Generate the Slopegraph\n\n\nShow the code\nrice %>% \n  mutate(Year = factor(Year)) %>%\n  filter(Year %in% c(1961, 1980)) %>%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")\n\n\n\n\n\n\n\n\n\n\n\nThings to note:\n\n\n\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor.\n\n\n\n\\(**That's\\) \\(all\\) \\(folks!**\\)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex07/Hands-On_Ex7b.html",
    "href": "Hands-On_Ex/Hands-On_Ex07/Hands-On_Ex7b.html",
    "title": "20 Time on the Horizon: ggHoriPlot methods",
    "section": "",
    "text": "(First Published: Jul 5, 2023)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex07/Hands-On_Ex7b.html#learning-outcome",
    "href": "Hands-On_Ex/Hands-On_Ex07/Hands-On_Ex7b.html#learning-outcome",
    "title": "20 Time on the Horizon: ggHoriPlot methods",
    "section": "20.1 Learning Outcome",
    "text": "20.1 Learning Outcome\nWe will learn how to create a Horizon Graph.\nA horizon graph is an analytical graphical method specially designed for visualising large numbers of time-series. It aims to overcome the issue of visualising highly overlapping time-series as shown in the figure below.\n\nA horizon graph essentially an area chart that has been split into slices and the slices then layered on top of one another with the areas representing the highest (absolute) values on top. Each slice has a greater intensity of colour based on the absolute value it represents.\n\n\n\n\n\n\n\nTip\n\n\n\nBefore getting started, do visit Getting Started to learn more about the functions of ggHoriPlot package. Next, read geom_horizon() to learn more about the usage of its arguments."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex07/Hands-On_Ex7b.html#getting-started",
    "href": "Hands-On_Ex/Hands-On_Ex07/Hands-On_Ex7b.html#getting-started",
    "title": "20 Time on the Horizon: ggHoriPlot methods",
    "section": "20.2 Getting Started",
    "text": "20.2 Getting Started\n\n20.2.1 Install and load the required R libraries\nInstall and load the the required R packages. The name and function of the new package that will be used for this exercise is as follow:\n\nggHoriPlot : allows building horizon plots in ggplot2\n\n\n\nShow the code\npacman::p_load(ggHoriPlot, ggthemes, tidyverse)\n\n\n\n\n20.2.2 Import the Data\nThe Average Retail Prices Of Selected Consumer Items data will be used.\n\n\nShow the code\naverp <- read_csv(\"data/AVERP.csv\", show_col_types = F) %>%\n  mutate(`Date` = dmy(`Date`))\n\n\n\n\n\n\n\n\nThings to note:\n\n\n\nBy default, read_csv will import data in Date field as Character data type. dmy() of lubridate package, which is a part of tidyverse, to parse the Date field into the Date data type in R.\n\n\n\n\n20.2.3 Plotting the Horizon Graph\nThe codes below are used to plot the horizon graph.\n\n\nShow the code\naverp %>% \n  filter(Date >= \"2018-01-01\") %>%\n  ggplot() +\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Consumer Items`~.) +\n    theme_few() +\n  scale_fill_hcl(palette = 'RdBu') +\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") +\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe red portion of the plot indicates a decline in value while the blue portion denotes an increase in value. The intensity of the red and blue tone signifies the magnitude of decline or increase.\n\n\n\n\\(**That's\\) \\(all\\) \\(folks!**\\)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8a.html",
    "href": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8a.html",
    "title": "15 Choropleth Mapping",
    "section": "",
    "text": "(First Published: Jun 10, 2023)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8a.html#learning-outcome",
    "href": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8a.html#learning-outcome",
    "title": "15 Choropleth Mapping",
    "section": "15.1 Learning Outcome",
    "text": "15.1 Learning Outcome\nWe will learn how to plot functional and truthful choropleth maps by using an R package called tmap package."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8a.html#getting-started",
    "href": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8a.html#getting-started",
    "title": "15 Choropleth Mapping",
    "section": "15.2 Getting Started",
    "text": "15.2 Getting Started\n\n15.2.1 Install and load the required R libraries\nInstall and load the the required R packages. The name and function of the new package that will be used for this exercise is as follow:\n\nsf for handling geospatial data.\n\n\n\nShow the code\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe will be using readr, tidyr and dplyr, which are part of tidyverse package\n\n\n\n\n15.2.2 Import the data\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e.¬†MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e.¬†respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it‚Äôs PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\nImport the simple feature (sf) data frame\nWe use the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\n\nShow the code\nmpsz <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\Cabbie-UK\\ISSS608\\Hands-On_Ex\\Hands-On_Ex08\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nLet‚Äôs examine the content of mpsz\n\n\nShow the code\nmpsz\n\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\nImport Attribute Data\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popdata.\n\n\nShow the code\npopdata <- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\", show_col_types = F)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8a.html#data-preparation",
    "href": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8a.html#data-preparation",
    "title": "15 Choropleth Mapping",
    "section": "15.3 Data Preparation",
    "text": "15.3 Data Preparation\nBefore a thematic map can be prepared, we will prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n15.3.1 Data Wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\n\nShow the code\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup() %>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n\n15.3.2 Join the attribute data and geospatial data\nBefore we can perform the geo-relational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\n\nShow the code\npopdata2020 <- popdata2020 %>%\n  # convert the PA and SZ columns to uppercase\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g.¬†SUBZONE_N and SZ as the common identifier.\n\n\nShow the code\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\n\n\n\n\n\n\nThings to learn from the codes\n\n\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame and the resulting table contains the geometry information inherited from mpsz.\n\n\nWe save a copy of the mpsz_pop2020 data frame before we start with the plots.\n\n\nShow the code\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8a.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8a.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "15 Choropleth Mapping",
    "section": "15.4 Choropleth Mapping Geospatial Data Using tmap",
    "text": "15.4 Choropleth Mapping Geospatial Data Using tmap\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n15.4.1 Plot a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\n\n\nShow the code\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the codes\n\n\n\n\ntmap_mode() with ‚Äúplot‚Äù option is used to produce a static map. For interactive mode, ‚Äúview‚Äù option should be used.\nfill argument is used to map the attribute (i.e.¬†DEPENDENCY)\n\n\n\n\n\n15.4.2 Create a choropleth map by using tmap‚Äôs elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes the aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap‚Äôs drawing elements should be used.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\nIn the following sub-sections, we will share the tmap functions that are used to plot these elements.\n\n15.4.2.1 Draw a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the codes below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons.\n\n\nShow the code\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n15.4.2.2 Draw a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the codes\n\n\n\n\nThe default interval binning used to draw the choropleth map is called ‚Äúpretty‚Äù. A detailed discussion of the data classification methods supported by tmap will be provided below\nThe default colour scheme used is YlOrRd of ColorBrewer.\nBy default, Missing value will be shaded in grey.\n\n\n\n\n\n15.4.2.3 Draw a choropleth map using tm_fill() and tm_border()\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nWe draws a choropleth map by using tm_fill() alone.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders() will be used as shown in the code chunk below.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is ‚Äúsolid‚Äù.\n\n\n\n\n15.4.3 Data classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n15.4.3.1 Plot choropleth maps with built-in classification methods\nThe plot below shows a quantile data classification that used 5 classes.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nThe plot below uses the equal data classification method.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nFeel free to Explore!\n\n\n\nTry to following:\n\nprepare choropleth maps by using different classification methods supported by tmap and compare their differences.\nprepare choropleth maps by using similar classification method but with different numbers of classes (i.e.¬†2, 6, 10, 20).\n\n\n\n\n\n15.4.3.2 Plot choropleth map with custome break\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\n\nShow the code\nsummary(mpsz_pop2020$DEPENDENCY)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00).\nNow, we will plot the choropleth map by using the codes.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n15.4.4 Colour Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n15.4.4.1 Using ColourBrewer palette\nTo change the colour, we assign the preferred colour to palette argument of tm_fill().\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a ‚Äú-‚Äù prefix.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n15.4.5 Map Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n15.4.5.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by Planning Subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n15.4.5.2 Map style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n15.4.5.3 Cartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the codes below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency Ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby Planning Subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  # for the borders of the polygon, with alpha = 0.5\n  # there are arguments for colours, line width etc\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  # control the features of the grid line\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\nUse the codes to reset the default style.\n\n\nShow the code\ntmap_style(\"white\")\n\n\n\n\n\n15.4.6 Draw Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n15.4.6.1 Assign multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\nIn the following example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n15.4.6.2 Define a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\n\nShow the code\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\",\n            # THe allow each facet square to be scaled its frame\n            # Setting it to FALSE will retain the original size as based on the SG map\n            free.coords=TRUE,\n            # Not to include neighbouring region\n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n15.4.6.3 Create multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\n\nShow the code\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n15.4.7 Map Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, we can also use selection function to map spatial objects meeting the selection criterion.\n\n\nShow the code\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8a.html#reference",
    "href": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8a.html#reference",
    "title": "15 Choropleth Mapping",
    "section": "15.5 Reference",
    "text": "15.5 Reference\n\n15.5.1 All about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n15.5.2 Geospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n15.5.3 Data wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‚Äòspread()‚Äô and ‚Äògather()‚Äô Functions\n\n\n\\(**That's\\) \\(all\\) \\(folks!**\\)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8b.html",
    "href": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8b.html",
    "title": "16 Visualise Geospatial Point Data",
    "section": "",
    "text": "(First Published: Jun 10, 2023)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8b.html#learning-outcome",
    "href": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8b.html#learning-outcome",
    "title": "16 Visualise Geospatial Point Data",
    "section": "16.1 Learning Outcome",
    "text": "16.1 Learning Outcome\nWe will learn how to create a proportional symbol map showing the number of wins by Singapore Pools‚Äô outlets using an R package called tmap."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8b.html#getting-started",
    "href": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8b.html#getting-started",
    "title": "16 Visualise Geospatial Point Data",
    "section": "16.2 Getting Started",
    "text": "16.2 Getting Started\n\n16.2.1 Install and load the required R libraries\nInstall and load the the required R packages.\n\n\nShow the code\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n\n16.2.2 Import the data\nThe data set use for this hands-on exercise is called SGPools_svy21. The data is in csv file format.\nFigure below shows the first 15 records of SGPools_svy21.csv. It consists of seven columns. The XCOORD and YCOORD columns are the x-coordinates and y-coordinates of SingPools outlets and branches.\nWe wll use read_csv() function of readr package to import SGPools_svy21.csv into R as a tibble data frame called sgpools.\n\n\n\nShow the code\nsgpools <- read_csv(\"data/aspatial/SGPools_svy21.csv\", show_col_types = F)\n\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\n\n\nShow the code\nlist(sgpools) \n\n\n[[1]]\n# A tibble: 306 √ó 7\n   NAME           ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n   <chr>          <chr>      <dbl>  <dbl>  <dbl> <chr>                     <dbl>\n 1 Livewire (Mar‚Ä¶ 2 Bayf‚Ä¶    18972 30842. 29599. Branch                        5\n 2 Livewire (Res‚Ä¶ 26 Sen‚Ä¶    98138 26704. 26526. Branch                       11\n 3 SportsBuzz (K‚Ä¶ Lotus ‚Ä¶   738078 20118. 44888. Branch                        0\n 4 SportsBuzz (P‚Ä¶ 1 Sele‚Ä¶   188306 29777. 31382. Branch                       44\n 5 Prime Serango‚Ä¶ Blk 54‚Ä¶   552542 32239. 39519. Branch                        0\n 6 Singapore Poo‚Ä¶ 1A Woo‚Ä¶   731001 21012. 46987. Branch                        3\n 7 Singapore Poo‚Ä¶ Blk 64‚Ä¶   370064 33990. 34356. Branch                       17\n 8 Singapore Poo‚Ä¶ Blk 88‚Ä¶   370088 33847. 33976. Branch                       16\n 9 Singapore Poo‚Ä¶ Blk 30‚Ä¶   540308 33910. 41275. Branch                       21\n10 Singapore Poo‚Ä¶ Blk 20‚Ä¶   560202 29246. 38943. Branch                       25\n# ‚Ñπ 296 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe sgpools data in tibble data frame and not the common R data frame.\n\n\n\n\n16.2.3 Create a sf data frame from an aspatial data frame\nWe convert sgpools data frame into a simple feature data frame by using st_as_sf() of sf packages.\n\n\nShow the code\nsgpools_sf <- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       #crs stands for coordinate reference system\n                       crs= 3414)\n\n\n\n\n\n\n\n\nThings to learn from the codes\n\n\n\n\nThe coords argument requires us to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\nThe crs argument required us to provide the coordinates system in epsg format. EPSG: 3414 is Singapore SVY21 Projected Coordinate System. We can search for other country‚Äôs epsg code by refering to epsg.io.\n\n\n\nNotice that a new column called geometry has been added into the sgpools_sf data frame.\n We can display the basic information of the newly created sgpools_sf.\n\n\nShow the code\nlist(sgpools_sf)\n\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 √ó 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * <chr>                        <chr>      <dbl> <chr>                     <dbl>\n 1 Livewire (Marina Bay Sands)  2 Bayf‚Ä¶    18972 Branch                        5\n 2 Livewire (Resorts World Sen‚Ä¶ 26 Sen‚Ä¶    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus ‚Ä¶   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele‚Ä¶   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54‚Ä¶   552542 Branch                        0\n 6 Singapore Pools Woodlands C‚Ä¶ 1A Woo‚Ä¶   731001 Branch                        3\n 7 Singapore Pools 64 Circuit ‚Ä¶ Blk 64‚Ä¶   370064 Branch                       17\n 8 Singapore Pools 88 Circuit ‚Ä¶ Blk 88‚Ä¶   370088 Branch                       16\n 9 Singapore Pools Anchorvale ‚Ä¶ Blk 30‚Ä¶   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio ‚Ä¶ Blk 20‚Ä¶   560202 Branch                       25\n# ‚Ñπ 296 more rows\n# ‚Ñπ 1 more variable: geometry <POINT [m]>\n\n\nThe output shows that sgppols_sf is in point feature class. It‚Äôs epsg ID is 3414. The bbox provides information of the extend of the geospatial data."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8b.html#draw-proportional-symbol-map",
    "href": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8b.html#draw-proportional-symbol-map",
    "title": "16 Visualise Geospatial Point Data",
    "section": "16.3 Draw Proportional Symbol Map",
    "text": "16.3 Draw Proportional Symbol Map\nTo create an interactive proportional symbol map in R, the view mode of tmap will be used.\nThe code churn below will turn on the interactive mode of tmap.\n\n\nShow the code\ntmap_mode(\"view\")\n\n\n\n16.3.1 Plot an interactive point symbol map\nLet‚Äôs plot an interactive point symbol map.\n\n\nShow the code\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n\n16.3.2 Let‚Äôs make the points proportional\nTo draw a proportional symbol map, we need to assign a numerical variable to the size visual attribute. The codes below show that the variable Gp1Gp2Winnings is assigned to size visual attribute.\n\n\nShow the code\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n\n16.3.3 Let‚Äôs give the points a different colour\nThe proportional symbol map can be further improved by using the colour visual attribute. In the codes below, OUTLET TYPE variable is used as the colour attribute variable.\n\n\nShow the code\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8b.html#i-have-a-twin-brothers",
    "href": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8b.html#i-have-a-twin-brothers",
    "title": "16 Visualise Geospatial Point Data",
    "section": "16.3.4 I have a twin brothers :)",
    "text": "16.3.4 I have a twin brothers :)\nAn impressive and little-know feature of tmap‚Äôs view mode is that it also works with faceted plots. The argument sync in tm_facets() can be used in this case to produce multiple maps with synchronised zoom and pan settings.\n\n\nShow the code\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            # this line set both maps to be in sync\n            sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore we end the session, it is wiser to switch tmap‚Äôs Viewer back to plot mode by using the code chunk below.\n\n\nShow the code\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8b.html#reference",
    "href": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8b.html#reference",
    "title": "16 Visualise Geospatial Point Data",
    "section": "16.4 Reference",
    "text": "16.4 Reference\n\n16.4.1 All about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n16.4.2 Geospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n16.4.3 Data wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‚Äòspread()‚Äô and ‚Äògather()‚Äô Functions\n\n\n\\(**That's\\) \\(all\\) \\(folks!**\\)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8c.html",
    "href": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8c.html",
    "title": "17 Analytical Mapping on Choropleth",
    "section": "",
    "text": "(First Published: Jun 10, 2023)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8c.html#learning-outcome",
    "href": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8c.html#learning-outcome",
    "title": "17 Analytical Mapping on Choropleth",
    "section": "17.1 Learning Outcome",
    "text": "17.1 Learning Outcome\nWe will gain hands-on experience on using appropriate R methods to plot analytical map. In this class exercise, we will be able to use appropriate functions of tmap and tidyverse to perform the following tasks:\n\nImporting geospatial data in rds format into R environment.\nCreating cartographic quality choropleth maps by using appropriate tmap functions.\nCreating rate map\nCreating percentile map\nCreating boxmap"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8c.html#getting-started",
    "href": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8c.html#getting-started",
    "title": "17 Analytical Mapping on Choropleth",
    "section": "17.2 Getting Started",
    "text": "17.2 Getting Started\n\n17.2.1 Install and load the required R libraries\nInstall and load the the required R packages.\n\n\nShow the code\npacman::p_load(tmap, tidyverse, sf)\n\n\n\n\n17.2.2 Import the data\nA prepared data set called NGA_wp.rds will be used. The data set is a polygon feature data frame providing information on water point of Nigeria at the LGA level.\n\n\nShow the code\nNGA_wp <- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8c.html#basic-choropleth-mapping",
    "href": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8c.html#basic-choropleth-mapping",
    "title": "17 Analytical Mapping on Choropleth",
    "section": "17.3 Basic Choropleth Mapping",
    "text": "17.3 Basic Choropleth Mapping\n\n17.3.1 Visualising distribution of non-functional water point\nFirst, we plot a choropleth map showing the distribution of functional and non-function water point by LGA\n\n\nShow the code\np1 <- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water point by LGAs\",\n            legend.outside = FALSE)\n\np2 <- tm_shape(NGA_wp) +\n  tm_fill(\"wp_nonfunctional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Reds\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of non-functional water point by LGAs\",\n            legend.outside = FALSE)\n\ntmap_arrange(p2, p1, ncol = 1)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8c.html#choropleth-map-for-rates",
    "href": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8c.html#choropleth-map-for-rates",
    "title": "17 Analytical Mapping on Choropleth",
    "section": "17.4 Choropleth Map for Rates",
    "text": "17.4 Choropleth Map for Rates\nIn much of our readings we have now seen the importance to map rates rather than counts of things, and that is for the simple reason that water points are not equally distributed in space. That means that if we do not account for how many water points are somewhere, we end up mapping total water point size rather than our topic of interest.\n\n17.4.1 Derive Proportion of Functional Water Points and Non-Functional Water Points\nWe will tabulate the proportion of functional water points and the proportion of non-functional water points in each LGA. In the following code chunk, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional.\n\n\nShow the code\nNGA_wp <- NGA_wp %>%\n  mutate(pct_functional = wp_functional/total_wp) %>%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\n\n17.4.2 Plott map of rate\nWe will plot a choropleth map showing the distribution of percentage functional water point by LGA.\n\n\nShow the code\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\",\n          legend.hist = TRUE) +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water point by LGAs\",\n            legend.outside = TRUE)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8c.html#extreme-value-maps",
    "href": "Hands-On_Ex/Hands-On_Ex08/Hands-On_Ex8c.html#extreme-value-maps",
    "title": "17 Analytical Mapping on Choropleth",
    "section": "17.5 Extreme Value Maps",
    "text": "17.5 Extreme Value Maps\nExtreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin 1994).\n\n17.5.1 Percentile Map\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\n\n17.5.1.1 Data Preparation\nStep 1: Exclude records with NA by using the codes below.\n\n\nShow the code\nNGA_wp <- NGA_wp %>%\n  drop_na()\n\n\nStep 2: Create customised classification and extracting values\n\n\nShow the code\npercent <- c(0,.01,.1,.5,.9,.99,1)\nvar <- NGA_wp[\"pct_functional\"] %>%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen variables are extracted from an sf data.frame, the geometry is extracted as well. For mapping and spatial manipulation, this is the expected behavior, but many base R functions cannot deal with the geometry. Specifically, the quantile() gives an error. As a result st_set_geomtry(NULL) is used to drop geomtry field.\n\n\n\n\nCreate the get.var function\nFirstly, we will write an R function as shown below to extract a variable (i.e.¬†wp_nonfunctional) as a vector out of an sf data.frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\n\nShow the code\nget.var <- function(vname,df) {\n  v <- df[vname] %>% \n    st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\n\n\nA percentile mapping function\nNext, we will write a percentile mapping function by using the code chunk below.\n\n\nShow the code\npercentmap <- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent <- c(0,.01,.1,.5,.9,.99,1)\n  var <- get.var(vnam, df)\n  bperc <- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"< 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"> 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n\nTest drive the percentile mapping function\nTo run the function, type the codes as shown below.\n\n\nShow the code\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is just a bare bones implementation. Additional arguments such as the title, legend positioning just to name a few of them, could be passed to customise various features of the map.\n\n\n\n\n\n17.5.2 Box map\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\n\nShow the code\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\n\nDisplaying summary statistics on a choropleth map by using the basic principles of boxplot.\nTo create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n\n17.5.2.1 Creating the boxbreaks function\nThe code chunk below is an R function that creating break points for a box map.\n\narguments:\n\nv: vector with observations\nmult: multiplier for IQR (default 1.5)\n\nreturns:\n\nbb: vector with 7 break points compute quartile and fences\n\n\n\n\nShow the code\nboxbreaks <- function(v,mult=1.5) {\n  qv <- unname(quantile(v))\n  iqr <- qv[4] - qv[2]\n  upfence <- qv[4] + mult * iqr\n  lofence <- qv[2] - mult * iqr\n  # initialize break points vector\n  bb <- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence < qv[1]) {  # no lower outliers\n    bb[1] <- lofence\n    bb[2] <- floor(qv[1])\n  } else {\n    bb[2] <- lofence\n    bb[1] <- qv[1]\n  }\n  if (upfence > qv[5]) { # no upper outliers\n    bb[7] <- upfence\n    bb[6] <- ceiling(qv[5])\n  } else {\n    bb[6] <- upfence\n    bb[7] <- qv[5]\n  }\n  bb[3:5] <- qv[2:4]\n  return(bb)\n}\n\n\n\n\n17.5.2.2 Creating the get.var function\nThe code chunk below is an R function to extract a variable as a vector out of an sf data frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\n\nShow the code\nget.var <- function(vname,df) {\n  v <- df[vname] %>% st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\n\n\n17.5.2.3 Test drive the newly created function\nLet‚Äôs test the newly created function.\n\n\nShow the code\nvar <- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\n17.5.2.4 Boxmap function\nThe code chunk below is an R function to create a box map.\n\narguments:\n\nvnam: variable name (as character, in quotes)\ndf: simple features polygon layer\nlegtitle: legend title\nmtitle: map title\nmult: multiplier for IQR\n\nreturns:\n\na tmap-element (plots a map)\n\n\n\n\nShow the code\nboxmap <- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var <- get.var(vnam,df)\n  bb <- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"< 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"> 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)\n\n\n\n\n\n\n\\(**That's\\) \\(all\\) \\(folks!**\\)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex09/Hands-On_Ex9.html",
    "href": "Hands-On_Ex/Hands-On_Ex09/Hands-On_Ex9.html",
    "title": "18 Information Dashboard Design",
    "section": "",
    "text": "(First Published: Jun 15, 2023)"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex09/Hands-On_Ex9.html#learning-outcome",
    "href": "Hands-On_Ex/Hands-On_Ex09/Hands-On_Ex9.html#learning-outcome",
    "title": "18 Information Dashboard Design",
    "section": "18.1 Learning Outcome",
    "text": "18.1 Learning Outcome\nWe will learn how to\n\ncreate bullet chart by using ggplot2,\ncreate sparklines by using ggplot2 ,\nbuild industry standard dashboard by using R Shiny."
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex09/Hands-On_Ex9.html#getting-started",
    "href": "Hands-On_Ex/Hands-On_Ex09/Hands-On_Ex9.html#getting-started",
    "title": "18 Information Dashboard Design",
    "section": "18.2 Getting Started",
    "text": "18.2 Getting Started\n\n18.2.1 Install and load the required R libraries\nInstall and load the the required R packages. The name and function of the new packages that will be used for this exercise are as follow:\n\ngtExtras provides some additional helper functions to assist in creating beautiful tables with gt, an R package specially designed for anyone to make wonderful-looking tables using the R\nreactable provides functions to create interactive data tables for R, based on the React Table library and made with reactR.\nreactablefmtr provides various features to streamline and enhance the styling of interactive reactable tables with easy-to-use and highly-customizable functions and themes.\nRODBC provides functions to establish connections with ODBC-compliant databases, execute SQL queries, and retrieve data into R for further analysis\n\n\n\nShow the code\npacman::p_load(ggthemes, reactable,reactablefmtr, gt, gtExtras, tidyverse, RODBC)\n\n\n\n\n18.2.2 Import the data\nA personal database in Microsoft Access mdb format called Coffee Chain will be used.\nodbcConnectAccess2007() of RODBC package is used to import a database query table into R. The import file is then saved in rds format.\n\n\nShow the code\n# connect to odbc driver to read the mdb file\n# the suggested odbcConnectAccess() function does not work for 64-bit machine\n# used odbcConnectAccess2007() function instead\ncon <- odbcConnectAccess2007('data/Coffee Chain.mdb')\n\n# ingest the coffee chain data and assign it to coffeechain variable\ncoffeechain <- sqlFetch(con, 'CoffeeChain Query')\n\n# write the coffeechain df to rds format\nwrite_rds(coffeechain, \"data/CoffeeChain.rds\")\n\n# close odbc connection\nodbcClose(con)\n\n\n(Note the above steps should only be run once)\nWe import CoffeeChain.rds into our working environment for the processing and analysis.\n\n\nShow the code\ncoffeechain <- read_rds(\"data/CoffeeChain.rds\")\n\n\nWe aggregate Sales and Budgeted Sales at the Product level to generate a product data frame.\n\n\nShow the code\nproduct <- coffeechain %>%\n  group_by(`Product`) %>%\n  summarise(`target` = sum(`Budget Sales`),\n            `current` = sum(`Sales`)) %>%\n  ungroup()"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex09/Hands-On_Ex9.html#bullet-chart",
    "href": "Hands-On_Ex/Hands-On_Ex09/Hands-On_Ex9.html#bullet-chart",
    "title": "18 Information Dashboard Design",
    "section": "18.3 Bullet Chart",
    "text": "18.3 Bullet Chart\nThe codes below is used to plot the bullet charts using ggplot2 functions.\n\n\nShow the code\n# Plotting the 'product' data with 'Product' on the x-axis and 'current' on the y-axis\nggplot(product, aes(Product, current)) +\n\n  # Adding a column geom for the upper bound of the target values\n  # This provides the slightly darker=toned gray frame for the plot\n  geom_col(aes(Product, max(target) * 1.01),\n           fill = \"grey85\", width = 0.85) +\n\n  # Adding a column geom for the mid-range of the target values\n  geom_col(aes(Product, target * 0.75),\n           fill = \"grey60\", width = 0.85) +\n\n  # Adding a column geom for the lower bound of the target values\n  geom_col(aes(Product, target * 0.5),\n           fill = \"grey50\", width = 0.85) +\n\n  # Adding a column geom for the current values\n  geom_col(aes(Product, current),\n           width = 0.35,\n           fill = \"black\") +\n\n  # Adding error bars based on the target values\n  geom_errorbar(aes(y = target,\n                    x = Product,\n                    ymin = target,\n                    ymax = target),\n                width = 0.4,\n                colour = \"red\",\n                size = 1) +\n\n  # Flipping the coordinates to have horizontal bars\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the chart:\n\n\n\n\nThe red strip is the target\nThe black bar is the current achievement\nThe 2 tone gray bars are the target values at 50% and 75%"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex09/Hands-On_Ex9.html#sparklines",
    "href": "Hands-On_Ex/Hands-On_Ex09/Hands-On_Ex9.html#sparklines",
    "title": "18 Information Dashboard Design",
    "section": "18.4 Sparklines",
    "text": "18.4 Sparklines\nPrepare the data\nWe aggregate the data from the coffee chain data frame to derive the monthly sales by product\n\n\nShow the code\nsales_report <- coffeechain %>%\n  filter(Date >= \"2013-01-01\") %>%\n  mutate(Month = month(Date)) %>%\n  group_by(Month, Product) %>%\n  summarise(Sales = sum(Sales)) %>%\n  ungroup() %>%\n  select(Month, Product, Sales)\n\n\nWe then compute the minimum, maximum and end othe the month sales.\n\n\nShow the code\n# Grouping the 'sales_report' data by 'Product' and selecting the row with the minimum 'Sales' value for each group\nmins <- group_by(sales_report, Product) %>%\n  slice(which.min(Sales))\n\n# Grouping the 'sales_report' data by 'Product' and selecting the row with the maximum 'Sales' value for each group\nmaxs <- group_by(sales_report, Product) %>%\n  slice(which.max(Sales))\n\n# Grouping the 'sales_report' data by 'Product' and selecting the rows where the 'Month' is equal to the maximum 'Month' value\nends <- group_by(sales_report, Product) %>%\n  filter(Month == max(Month))\n\n\nNext, we compute the 25th and 75 quantiles\n\n\nShow the code\nquarts <- sales_report %>%\n  group_by(Product) %>%\n  summarise(quart1 = quantile(Sales, \n                              0.25),\n            quart2 = quantile(Sales, \n                              0.75)) %>%\n  right_join(sales_report)\n\n\nPlot the chart\n\n\nShow the code\nggplot(sales_report, aes(x=Month, y=Sales)) + \n  facet_grid(Product ~ ., scales = \"free_y\") + \n  geom_ribbon(data = quarts, aes(ymin = quart1, max = quart2), \n              fill = 'grey90') +\n  geom_line(size=0.3) +\n  geom_point(data = mins, col = 'red') +\n  geom_point(data = maxs, col = 'blue') +\n  geom_text(data = mins, aes(label = Sales), vjust = -1) +\n  geom_text(data = maxs, aes(label = Sales), vjust = 2.5) +\n  geom_text(data = ends, aes(label = Sales), hjust = 0, nudge_x = 0.5) +\n  geom_text(data = ends, aes(label = Product), hjust = 0, nudge_x = 1.0) +\n  expand_limits(x = max(sales_report$Month) + \n                  (0.25 * (max(sales_report$Month) - min(sales_report$Month)))) +\n  scale_x_continuous(breaks = seq(1, 12, 1)) +\n  scale_y_continuous(expand = c(0.1, 0)) +\n  theme_tufte(base_size = 3, base_family = \"Helvetica\") +\n  theme(axis.title=element_blank(), axis.text.y = element_blank(), \n        axis.ticks = element_blank(), strip.text = element_blank())"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex09/Hands-On_Ex9.html#static-information-dashboard-design",
    "href": "Hands-On_Ex/Hands-On_Ex09/Hands-On_Ex9.html#static-information-dashboard-design",
    "title": "18 Information Dashboard Design",
    "section": "18.5 Static Information Dashboard Design",
    "text": "18.5 Static Information Dashboard Design\nNext, we will learn how to create static information dashboard by using gt and gtExtras packages. Visit the webpage of these two packages and review all the materials provided on the webpages at least once.\n\n18.5.1 Plot a simple bullet chart\nWe prepare a bullet chart report by using functions of gt and gtExtras packages.\n\n\nShow the code\nproduct %>%\n  gt::gt() %>%\n  gt_plt_bullet(column = current, \n              target = target, \n              width = 60,\n              palette = c(\"lightblue\", \n                          \"black\")) %>%\n  gt_theme_538()\n\n\n\n\n\n\n  \n  \n    \n      Product\n      current\n    \n  \n  \n    Amaretto\n          \n    Caffe Latte\n          \n    Caffe Mocha\n          \n    Chamomile\n          \n    Colombian\n          \n    Darjeeling\n          \n    Decaf Espresso\n          \n    Decaf Irish Cream\n          \n    Earl Grey\n          \n    Green Tea\n          \n    Lemon\n          \n    Mint\n          \n    Regular Espresso\n          \n  \n  \n  \n\n\n\n\n\n\n18.5.2 Create Sparklines using gtExtrad method\nPrepare the data\nWe extract the sales for 2013 and group the results by product and month.\n\n\nShow the code\nreport <- coffeechain %>%\n  mutate(Year = year(Date)) %>%\n  filter(Year == \"2013\") %>%\n  mutate (Month = month(Date, \n                        label = TRUE, \n                        abbr = TRUE)) %>%\n  group_by(Product, Month) %>%\n  summarise(Sales = sum(Sales)) %>%\n  ungroup()\n\n\ngtExtras functions require us to pass a data frame with list columns. In view of this, codes below will be used to convert the report data.frame into list columns.\n\n\nShow the code\nreport %>%\n  group_by(Product) %>%\n  summarise('Monthly Sales' = list(Sales),\n            .groups = \"drop\") \n\n\n# A tibble: 13 √ó 2\n   Product           `Monthly Sales`\n   <chr>             <list>         \n 1 Amaretto          <dbl [12]>     \n 2 Caffe Latte       <dbl [12]>     \n 3 Caffe Mocha       <dbl [12]>     \n 4 Chamomile         <dbl [12]>     \n 5 Colombian         <dbl [12]>     \n 6 Darjeeling        <dbl [12]>     \n 7 Decaf Espresso    <dbl [12]>     \n 8 Decaf Irish Cream <dbl [12]>     \n 9 Earl Grey         <dbl [12]>     \n10 Green Tea         <dbl [12]>     \n11 Lemon             <dbl [12]>     \n12 Mint              <dbl [12]>     \n13 Regular Espresso  <dbl [12]>     \n\n\nPlot the sparklines\nWe pass the data frame with list columns throught the gt() function followed by the the gt_plt_sparkline() function to get the table of plots.\n\n\nShow the code\nreport %>%\n  group_by(Product) %>%\n  summarise('Monthly Sales' = list(Sales), \n            .groups = \"drop\") %>%\n   gt() %>%\n   gt_plt_sparkline('Monthly Sales',\n                    same_limit = FALSE)\n\n\n\n\n\n\n  \n  \n    \n      Product\n      Monthly Sales\n    \n  \n  \n    Amaretto\n          1.2K\n    Caffe Latte\n          1.5K\n    Caffe Mocha\n          3.7K\n    Chamomile\n          3.3K\n    Colombian\n          5.5K\n    Darjeeling\n          3.0K\n    Decaf Espresso\n          3.2K\n    Decaf Irish Cream\n          2.7K\n    Earl Grey\n          3.0K\n    Green Tea\n          1.5K\n    Lemon\n          4.4K\n    Mint\n          1.5K\n    Regular Espresso\n          1.1K\n  \n  \n  \n\n\n\n\nAdd statistics to the table\nWe can calculate the min, max and average for each product and display them in a gt table\n\n\nShow the code\nreport %>% \n  group_by(Product) %>% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            ) %>%\n  gt() %>%\n  fmt_number(columns = 4,\n    decimals = 2)\n\n\n\n\n\n\n  \n  \n    \n      Product\n      Min\n      Max\n      Average\n    \n  \n  \n    Amaretto\n1016\n1210\n1,119.00\n    Caffe Latte\n1398\n1653\n1,528.33\n    Caffe Mocha\n3322\n3828\n3,613.92\n    Chamomile\n2967\n3395\n3,217.42\n    Colombian\n5132\n5961\n5,457.25\n    Darjeeling\n2926\n3281\n3,112.67\n    Decaf Espresso\n3181\n3493\n3,326.83\n    Decaf Irish Cream\n2463\n2901\n2,648.25\n    Earl Grey\n2730\n3005\n2,841.83\n    Green Tea\n1339\n1476\n1,398.75\n    Lemon\n3851\n4418\n4,080.83\n    Mint\n1388\n1669\n1,519.17\n    Regular Espresso\n890\n1218\n1,023.42\n  \n  \n  \n\n\n\n\nWe can also combine the sparklines table and the statistics table into one by joining them together.\n\n\nShow the code\n# prepare the sparkline table\nspark <- report %>%\n  group_by(Product) %>%\n  summarise('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\n# prepare the summary statistics table\nstats <- report %>% \n  group_by(Product) %>% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            )\n\n# combine the 2 tables together\nsales_data = left_join(stats,spark)\n\n# Generate the GT table\nsales_data %>%\n  gt() %>%\n  gt_plt_sparkline('Monthly Sales',\n                   same_limit = FALSE)\n\n\n\n\n\n\n  \n  \n    \n      Product\n      Min\n      Max\n      Average\n      Monthly Sales\n    \n  \n  \n    Amaretto\n1016\n1210\n1119.000\n          1.2K\n    Caffe Latte\n1398\n1653\n1528.333\n          1.5K\n    Caffe Mocha\n3322\n3828\n3613.917\n          3.7K\n    Chamomile\n2967\n3395\n3217.417\n          3.3K\n    Colombian\n5132\n5961\n5457.250\n          5.5K\n    Darjeeling\n2926\n3281\n3112.667\n          3.0K\n    Decaf Espresso\n3181\n3493\n3326.833\n          3.2K\n    Decaf Irish Cream\n2463\n2901\n2648.250\n          2.7K\n    Earl Grey\n2730\n3005\n2841.833\n          3.0K\n    Green Tea\n1339\n1476\n1398.750\n          1.5K\n    Lemon\n3851\n4418\n4080.833\n          4.4K\n    Mint\n1388\n1669\n1519.167\n          1.5K\n    Regular Espresso\n890\n1218\n1023.417\n          1.1K\n  \n  \n  \n\n\n\n\n\n\n18.5.3 Combine bullet chart and sparklines\nUsually a similar approach, we can further combine the bullet chart with the combined statistics and sparklines table above.\n\n\nShow the code\n# Prepare the data for bullet chart\nbullet <- coffeechain %>%\n  filter(Date >= \"2013-01-01\") %>%\n  group_by(`Product`) %>%\n  summarise(`Target` = sum(`Budget Sales`),\n            `Actual` = sum(`Sales`)) %>%\n  ungroup() \n\n# left join sales_data  with the bullet chart data frame\nsales_data = sales_data %>%\n  left_join(bullet)\n\n\n# generate the \"everything into one\" table\nsales_data %>%\n  gt() %>%\n  gt_plt_sparkline('Monthly Sales',\n                   # the following argument ensures that each sparkline graph has its own y-axis\n                   same_limit = FALSE) %>%\n  gt_plt_bullet(column = Actual, \n                target = Target, \n                width = 40,\n                palette = c(\"lightblue\", \n                          \"black\")) %>%\n  gt_theme_538() \n\n\n\n\n\n\n  \n  \n    \n      Product\n      Min\n      Max\n      Average\n      Monthly Sales\n      Actual\n    \n  \n  \n    Amaretto\n1016\n1210\n1119.000\n          1.2K\n          \n    Caffe Latte\n1398\n1653\n1528.333\n          1.5K\n          \n    Caffe Mocha\n3322\n3828\n3613.917\n          3.7K\n          \n    Chamomile\n2967\n3395\n3217.417\n          3.3K\n          \n    Colombian\n5132\n5961\n5457.250\n          5.5K\n          \n    Darjeeling\n2926\n3281\n3112.667\n          3.0K\n          \n    Decaf Espresso\n3181\n3493\n3326.833\n          3.2K\n          \n    Decaf Irish Cream\n2463\n2901\n2648.250\n          2.7K\n          \n    Earl Grey\n2730\n3005\n2841.833\n          3.0K\n          \n    Green Tea\n1339\n1476\n1398.750\n          1.5K\n          \n    Lemon\n3851\n4418\n4080.833\n          4.4K\n          \n    Mint\n1388\n1669\n1519.167\n          1.5K\n          \n    Regular Espresso\n890\n1218\n1023.417\n          1.1K"
  },
  {
    "objectID": "Hands-On_Ex/Hands-On_Ex09/Hands-On_Ex9.html#interactive-information-dashboard-design",
    "href": "Hands-On_Ex/Hands-On_Ex09/Hands-On_Ex9.html#interactive-information-dashboard-design",
    "title": "18 Information Dashboard Design",
    "section": "18.6 Interactive Information Dashboard Design",
    "text": "18.6 Interactive Information Dashboard Design\nIn this section, we will learn how to create interactive information dashboard by using reactable and reactablefmtr packages.\nIn order to build an interactive sparklines, we need to install dataui R package by using the codes below.\n\n\nShow the code\nremotes::install_github(\"timelyportfolio/dataui\")\n\n\n(the above code should only be run once)\nWe then load the package for use.\n\n\nShow the code\nlibrary(dataui)\n\n\n\n18.6.1 Plotting interactive sparklines\nSimilar to gtExtras, to plot an interactive sparklines by using reactablefmtr package we need to prepare the list field by using the codes below.\n\n\nShow the code\nreport <- report %>%\n  group_by(Product) %>%\n  summarise(`Monthly Sales` = list(Sales))\n\n\nNext, react_sparkline() function is used to plot the sparklines as shown below.\n\n\nShow the code\nreactable(\n  report,\n  # we include the following code line to increase the no. of rows displayed from 10 (the default) to 13 so that we have all items displayed\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(report)\n    )\n  )\n)\n\n\n\n\n\n\n\nAdd points and labels\nhighlight_points argument is used to show the minimum and maximum values points and label argument is used to label first and last values.\n\n\nShow the code\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        # use highligt_points to display min and max value\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        labels = c(\"first\", \"last\")\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\nAdd Reference Line\nstatline argument is used to show the mean line.\n\n\nShow the code\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        # include the mean reference line here\n        statline = \"mean\"\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\nAdd bandline\nBandline can be added by using the bandline argument.\n\n\nShow the code\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        line_width = 1,\n        bandline = \"innerquartiles\",\n        bandline_color = \"green\"\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\nChange from sparkline to sparkbar\nWe use the react_sparkbar() function to replace lines with bars.\n\n\nShow the code\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      # Use react_sparkbar here\n      cell = react_sparkbar(\n        report,\n        highlight_bars = highlight_bars(\n          min = \"red\", max = \"blue\"),\n        bandline = \"innerquartiles\",\n        statline = \"mean\")\n    )\n  )\n)\n\n\n\n\n\n\n\n\n\\(**That's\\) \\(all\\) \\(folks!**\\)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-Class_Ex1.html",
    "href": "In-Class_Ex/In-Class_Ex01/In-Class_Ex1.html",
    "title": "1 A Light Makeover of ggplots",
    "section": "",
    "text": "First published: 17-Apr-2023"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-Class_Ex1.html#learning-outcome",
    "href": "In-Class_Ex/In-Class_Ex01/In-Class_Ex1.html#learning-outcome",
    "title": "1 A Light Makeover of ggplots",
    "section": "1.1 Learning Outcome",
    "text": "1.1 Learning Outcome\nWe will:\n\nuse the graphic layers of ggplot2 to touch up various plots to make them more intuitive and appealing."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-Class_Ex1.html#getting-started",
    "href": "In-Class_Ex/In-Class_Ex01/In-Class_Ex1.html#getting-started",
    "title": "1 A Light Makeover of ggplots",
    "section": "1.2 Getting Started",
    "text": "1.2 Getting Started\n\n1.2.1 Install and load the required r libraries\nLoad the tidyverse library.\n\n\nShow the code\npacman::p_load(tidyverse)\n\n\n\n\n1.2.2 Import the data\nWe will be using the same exam scores data-set that was featured in my Hands-On Ex 1.\n\n\nShow the code\nexam_data <- read_csv('data/Exam_data.csv', show_col_types = FALSE )"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex01/In-Class_Ex1.html#candidates-for-makeover",
    "href": "In-Class_Ex/In-Class_Ex01/In-Class_Ex1.html#candidates-for-makeover",
    "title": "1 A Light Makeover of ggplots",
    "section": "1.3 Candidates for Makeover",
    "text": "1.3 Candidates for Makeover\n\n1.3.1 Working with theme\nWe can use the theme() function to change the colors of the plot panel background of the following plot to light blue and the color of grid lines to white. We will also include the title and subtitle to make the plot more meaningful.\n\nBeforeAfter\n\n\n\n\nShow the code\nggplot(data=exam_data, aes(x=RACE)) +\n  geom_bar() +\n  ylab('Count') +\n  theme_minimal() + \n  coord_flip()\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(data=exam_data, aes(x=RACE)) +\n  geom_bar() +\n  ylab('Count') +\n  theme_minimal() + \n  coord_flip() +\n  \n  theme(\n    panel.background = element_rect(fill = \"lightblue\", colour = \"lightblue\"),\n    panel.grid.major = element_line(size = 0.75, linetype = 'solid', colour = \"white\"), \n    panel.grid.minor = element_line(size = 0.25, linetype = 'solid', colour = \"white\")\n      \n  ) +\n  \n  labs(title = \"Chinese Students Formed The Bulk Of The Cohort\",\n       subtitle = \"This was followed by Malay and Indian students\")\n\n\n\n\n\n\n\n\n\n\n1.3.2 Sort and label the columns in a chart\nFor the column chart displayed below, we have to address the following criticisms:\n\ny-axis title is not clear (i.e.¬†count)\nTo support effective comparison, the bars should be sorted by their respective frequencies.\nFor static graph, frequency values should be added to provide addition information.\n\nTo address the first comment, we use ylab() and theme() function to relabel and calibrate the position of the y-axis title. For the 2nd and 3rd comment, we first summarise the data-set using group_by() and summarise() function to get the count of pupils by race. Thereafter, we use the reorder() function to sort the columns and geom_text() function to display the count.\n\nBeforeAfter\n\n\n\n\nShow the code\nggplot(data=exam_data, aes(x=RACE)) +\n  geom_bar() +\n  ylim(0,220) \n\n\n\n\n\n\n\n\n\nShow the code\n# Group the data by Race and aggregate by pupil count\nsorted_data <- exam_data %>% group_by(RACE) %>% summarise(count=n()) \n\n# Plot the chart\nggplot(data = sorted_data, aes(x=reorder(RACE, -count),y=count)) +\n  ylim(0,220) +\n  geom_col() +\n  ylab('No. of\\nPupils') +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5, hjust=1))+\n  xlab('Race') +\n  geom_text(aes(label = count), vjust = -0.5, size = 3.5) +\n  labs(title = \"Chinese Pupils Formed The Bulk Of The Cohort\",\n       subtitle = \"This was followed by Malay and Indian pupils\")\n\n\n\n\n\n\n\n\n\n\n1.3.3 Add mean and median lines on the histogram\nWe can use the geom_vline() function to add a computed mean and median line onto the chart.\n\nBeforeAfter\n\n\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins = 20)\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept=mean(MATHS, na.rm=T)),\n             color=\"red\", \n             linetype=\"dashed\", \n             size=1) +\n  geom_vline(aes(xintercept=median(MATHS, na.rm=T)),\n             color=\"grey30\",\n             linetype=\"dashed\", \n             size=1) +\n            xlab('Maths Score') +\n            ylab('Count') +\n  annotate(\"label\", color = \"red\", x=mean(exam_data$MATHS, na.rm=T)+2, y=50,\n          label=paste(\"Mean: \", round(mean(exam_data$MATHS, na.rm=T), 1)),fill = \"white\") +\n  annotate(\"label\",color=\"grey30\", x=median(exam_data$MATHS, na.rm=T)+2, y=40,\n          label=paste(\"Median: \", round(median(exam_data$MATHS, na.rm=T), 1)),fill = \"white\") + \n  labs(title = \"The Median score for Maths exam was 74\",\n         subtitle = \"We have a small porpotion of pupils who scored less than 25 marks\")\n\n\n\n\n\n\n\n\n\n\n1.3.4 Make the basic histogram more informative\nThe histograms below are elegantly designed but not informative. This is because they only reveal the distribution of English scores by gender but without context such as the score distribution for all pupils.\nTo show the distribution of English scores for all pupils as the background, we can add another geom_histogram() layer for all pupils.\n\nBeforeAfter\n\n\n\n\nShow the code\nggplot(data=exam_data, aes(x = ENGLISH )) +\n  geom_histogram() +\n  facet_wrap(~ GENDER) +\n  guides(fill = FALSE) \n\n\n\n\n\n\n\n\n\nShow the code\nd <- exam_data\n\n#create a d_bg dataframe without the gender attribute\nd_bg <- d[, -3]  \n\nggplot(d, aes(x = ENGLISH, fill = GENDER)) +\n  geom_histogram(data = d_bg, fill = \"grey\", alpha = .5) +\n  geom_histogram(colour = \"black\") +\n  facet_wrap(~ GENDER) +\n  # guides = false removes the legend from the chart\n  scale_fill_manual(values = c(\"Female\" = \"pink\", \"Male\" = \"light blue\")) +\n  guides(fill = FALSE) +  \n  theme_bw() +\n  labs(title = \"Female pupils scored higher marks than male pupils for English\",\n         subtitle = \"Their score distn covered the overall score distn for all pupuls on the right side fo the chart\")\n\n\n\n\n\n\n\n\n\n\n1.3.5 Dividing a chart into 4 quadrants\nWe can use geom_hline() and geom_vline() to divide the chart into equal quadrants (see After(1)). At the same time, we can use the coord_cartesian() function to adjust the limits of the coordinate system for a plot to only display the data that falls within those limits.\nIn After (2), I included a regression line and swtich the background to white (using theme_minimal()) to make the relationship between English and Math scores more prominent.\n\nBeforeAfter (1)After (2)\n\n\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x=MATHS, y=ENGLISH)) +\n  geom_point() +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(20,99))\n\n\n\n\n\n\n\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x=MATHS, y=ENGLISH)) +\n  geom_point() +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  geom_hline(yintercept=50,\n             linetype=\"dashed\",\n             color=\"grey60\",\n             size=1) + \n  geom_vline(xintercept=50, \n             linetype=\"dashed\",\n             color=\"grey60\",\n             size=1) +\n  labs(title = \"Pupils who did well in English tend to excel in Math as well\",\n         subtitle = \"There is a positive correlation between English and Math scores\") \n\n\n\n\n\n\n\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x=MATHS, y=ENGLISH)) +\n  geom_point() +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  geom_smooth(method=lm,linewidth=1.0)  +\n  labs(title = \"Pupils who did well in English tend to excel in Math as well\",\n         subtitle = \"There is a positive correlation between English and Math scores\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\\(**That's\\) \\(all\\) \\(folks!**\\)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex03/In-Class_Ex3.html",
    "href": "In-Class_Ex/In-Class_Ex03/In-Class_Ex3.html",
    "title": "2 Publishing a Quarto Document",
    "section": "",
    "text": "This test webpage was created as part of the follow-along exercise during Lesson 3."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex04/In-Class_Ex4.html",
    "href": "In-Class_Ex/In-Class_Ex04/In-Class_Ex4.html",
    "title": "3 Quantile‚Äìquantile plots",
    "section": "",
    "text": "(First published: May 6, 2023)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex04/In-Class_Ex4.html#load-the-required-packages",
    "href": "In-Class_Ex/In-Class_Ex04/In-Class_Ex4.html#load-the-required-packages",
    "title": "3 Quantile‚Äìquantile plots",
    "section": "1.Load the required packages",
    "text": "1.Load the required packages\n\n\nShow the code\npacman::p_load(rstatix,gt,patchwork,tidyverse,webshot2)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex04/In-Class_Ex4.html#load-the-data-set-into-r",
    "href": "In-Class_Ex/In-Class_Ex04/In-Class_Ex4.html#load-the-data-set-into-r",
    "title": "3 Quantile‚Äìquantile plots",
    "section": "2.Load the data-set into R",
    "text": "2.Load the data-set into R\n\n\nShow the code\nexam_data <- read_csv('data/Exam_data.csv', show_col_types = FALSE)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex04/In-Class_Ex4.html#visualise-normal-distribution",
    "href": "In-Class_Ex/In-Class_Ex04/In-Class_Ex4.html#visualise-normal-distribution",
    "title": "3 Quantile‚Äìquantile plots",
    "section": "3.Visualise Normal Distribution",
    "text": "3.Visualise Normal Distribution\nQuantile‚Äìquantile (Q-Q) plots are a useful visualization when we want to determine to what extent the observed data points do or do not follow a given distribution. If the data is normally distributed, the points in a Q-Q plot will be on a straight diagonally line. Conversely, if the points deviate significantly from the straight diagonally line, then it‚Äôs less likely that the data is normally distributed.\n\nThe PlotThe Codes\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, \n       aes(sample=ENGLISH))+\n  stat_qq()+\n  stat_qq_line()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can see that the points deviate significantly from the straight diagonal line. This is a clear indication that the set of data is not normally distributed."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex04/In-Class_Ex4.html#combining-statistical-graph-and-analysis-table",
    "href": "In-Class_Ex/In-Class_Ex04/In-Class_Ex4.html#combining-statistical-graph-and-analysis-table",
    "title": "3 Quantile‚Äìquantile plots",
    "section": "4.Combining statistical graph and analysis table",
    "text": "4.Combining statistical graph and analysis table\nWe will need to install webshot2\n\n\nShow the code\nqq <-ggplot(exam_data, \n       aes(sample=ENGLISH))+\n  stat_qq()+\n  stat_qq_line()\n\nsw_t <- exam_data %>%\n  shapiro_test(ENGLISH) %>%\n  gt()\n\ntmp <- tempfile(fileext = '.png')\ngtsave(sw_t,tmp)\ntable_png <- png::readPNG(tmp,\n                          native = TRUE)\n\nqq + table_png"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex05/In-Class_Ex5.html",
    "href": "In-Class_Ex/In-Class_Ex05/In-Class_Ex5.html",
    "title": "4 Working with json files for Network Analysis",
    "section": "",
    "text": "(First Published: May 13, 2023)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex05/In-Class_Ex5.html#getting-started",
    "href": "In-Class_Ex/In-Class_Ex05/In-Class_Ex5.html#getting-started",
    "title": "4 Working with json files for Network Analysis",
    "section": "4.1 Getting Started",
    "text": "4.1 Getting Started\n\n4.1.1 Install and load the required r libraries\n\njsonlite : allows the reading and importing of json files.\n\n\n\nShow the code\npacman::p_load(jsonlite,tidygraph,ggraph,visNetwork,tidyverse)\n\n\n\n\n4.1.2 Import the data\nImport the given MC1.json file into R and assign the data to MC1.\n\n\nShow the code\nMC1 = fromJSON(\"data/MC1.json\")\n\n\nExtract the nodes info from MC1 data frame\n\n\nShow the code\nMC1_nodes <- as_tibble(MC1$nodes) %>%\n  select(id, type, country)\n\n\nExtract the edges info from MC1 data frame\n\n\nShow the code\nMC1_edges <- as_tibble(MC1$links) %>%\n  select(source, target, type, weight, key)\n\n\nAggregate the weight information between each pair of notes and by the relationship type\n\n\nShow the code\nMC1_edges_aggregated <- MC1_edges  %>%\n  group_by(source, target, type) %>%\n  summarise(weight_sum = sum()) %>%\n  filter(source !=target) %>%\n  ungroup()\n\n\n\n\n4.1.3 Use tbl_graph() to build tidygraph data model\nWe use tbl_graph() of tinygraph package to build an tidygraph's network graph data.frame.\n\n\nShow the code\nMC1_graph <- tbl_graph(nodes = MC1_nodes,\n                       edges = MC1_edges_aggregated,\n                       directed = TRUE)\n\n\nLet's take a look at the output tidygraph's graph object.\n\n\nShow the code\nMC1_graph\n\n\n# A tbl_graph: 3428 nodes and 10747 edges\n#\n# A bipartite multigraph with 93 components\n#\n# A tibble: 3,428 √ó 3\n  id                       type         country \n  <chr>                    <chr>        <chr>   \n1 Spanish Shrimp  Carriers company      Nalakond\n2 12744                    organization <NA>    \n3 143129355                organization <NA>    \n4 7775                     organization <NA>    \n5 1017141                  organization <NA>    \n6 2591586                  organization <NA>    \n# ‚Ñπ 3,422 more rows\n#\n# A tibble: 10,747 √ó 4\n   from    to type                weight_sum\n  <int> <int> <chr>                    <int>\n1    49    51 family_relationship          0\n2    49    52 family_relationship          0\n3    49     4 family_relationship          0\n# ‚Ñπ 10,744 more rows\n\n\nFurther data cleaning is required before we can proceed to plot the graph."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Visual Analytics and Applications",
    "section": "",
    "text": "I am a student taking a post-grad analytics programme. Welcome to my ISSS608 Visual Analytics and Applications site! üòä\nThis site is a portfolio of my learning and accomplishments in the course. It showcases the exercises and projects I have completed and the skills I have acquired from the course."
  },
  {
    "objectID": "Tableau/1_Superstore.html",
    "href": "Tableau/1_Superstore.html",
    "title": "Superstore Annual Sales and Profit",
    "section": "",
    "text": "This was the first dashboard we developed in class. It marks the start of our fascinating journey into the world of data visualisation using Tableau."
  },
  {
    "objectID": "Tableau/2_SuperStore_Story.html",
    "href": "Tableau/2_SuperStore_Story.html",
    "title": "The Superstore Story",
    "section": "",
    "text": "I created this storyboard after going through the online tutorial on Tableau‚Äôs websiteüëª. It‚Äôs refreshing to learn some of the capabilities of the visualisation tool and the workflow of creating a data story"
  },
  {
    "objectID": "Tableau/3_Animation_CrossFiltering.html",
    "href": "Tableau/3_Animation_CrossFiltering.html",
    "title": "Animinated Bubble Plot and Cross-Filtering of Charts",
    "section": "",
    "text": "In line with the techniques covered under Hand-On Exercise 3 using R, , we were taught to create the following charts using Tableau this afternoon:\nA) An animated bubble plot using the Superstore data-set\nüëáSelect a different year using the ‚ÄúYear of Order Date‚Äô radio buttons on the right to check out the animated transition.\n\n\nB) Cross-filtered Charts using Exam Scores data-set\nüëáSelect a group of data points on one of the three charts and notice how the other two charts respond dynamically.\n\n\nNeat right? üòâ"
  },
  {
    "objectID": "Tableau/4_multivariate_plots.html",
    "href": "Tableau/4_multivariate_plots.html",
    "title": "Plots for multi-variate analysis",
    "section": "",
    "text": "In line with the techniques covered under Hand-On Exercises 12,13 and 14 using R, , we were taught to create the following charts using Tableau this afternoon:\nA) A Heatmap to understand the World Happiness Data in 2018\nüëáSelect a different year using the ‚ÄúRegion‚Äô radio buttons on the right to filter the countries by Region. Singapore is a part of South East Asia.\n\n\nB) Parallel Coordinates Plot on the World Happiness Data in 2018\nüëáSelect a data point or brush on a group of data points to observe how the ratings change for each of the attributes to the happiness measure.\n\n\nC) Treemap on Property Sale 2018\nüëáMouse over the tiles to see more details of property transactions that occurred for each property object."
  },
  {
    "objectID": "Tableau/5_time_plots.html",
    "href": "Tableau/5_time_plots.html",
    "title": "Time Plots",
    "section": "",
    "text": "Today, we covered a series of time/date related plots during lessons. Let‚Äôs check out what we did!.\nA) Line and Cyclical Pattern Plot\nüëáUse the slider to select a year-range, and select a country to display the overall tourist arrivals for the period on this Tableau dashboard.\n\n\nB) Slope Graph\nThis is essentially a variant of the line graph and was invented by Edward Tufte. Like the linegraph, it allows a change between points to be presented by a line. It shows 2 things:\n\nThe data points are arranged in descending, allowing us to identify which country has the highest rice yield for the year\nThe slope of the lines shows the rate the change during the period.\n\nüëáUse the year selector on the right to pick 2 years to see the graph in action\n\n\nC) Calender Heatmap\nüëáHover over the calender tiles to view the count of intrusions logged for the day."
  },
  {
    "objectID": "Tableau/6_geospatial_plots.html",
    "href": "Tableau/6_geospatial_plots.html",
    "title": "Visualise Geographic Data",
    "section": "",
    "text": "Today, we covered a few types of maps during using 4 different data sets. Enjoy the visuals!\nA) Proportional Symbol Map for the TOTO wins at Singapore Pools Branches and Outlets\nThis map was developed using aspatial data set with WGS84 projected coordinates\nüëáCheck out the popular branch/outlet in Yishun! üôÉ\n\n\nB) Map of Elderly Care Centres in Singapore\nThis map was developed using spatial data with a Geometry info\n\n\nC) Dashboard for Real Estate Transactions in 2019\nInformation is broken by project name and planning area.\nüëáChoose which property type to review using the radio buttons on the filtering pane\n\n\nD) Choropleth Map for Singapore Population 2022\nWe have to use URA‚Äôs Master Plan File 2019 to plot the choropleth map nicely"
  },
  {
    "objectID": "Tableau/7_Information_Dashboard.html",
    "href": "Tableau/7_Information_Dashboard.html",
    "title": "Information Dashboard",
    "section": "",
    "text": "Today is the final lesson of our Visual Analytics And Applications Courseüòï. We covered the 2 plots that are commonly used in Information Dashboards. One is a Bullet Chart created by Stephen Few and the one is Sparkline introduced by Edward Tufte."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html",
    "title": "The City of Engagement: Undestanding the Demographics and Financial Characteristics of its Residents",
    "section": "",
    "text": "Photo: Hadi Zaher\n(First Published: May 14, 2023)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#setting-the-scene",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#setting-the-scene",
    "title": "The City of Engagement: Undestanding the Demographics and Financial Characteristics of its Residents",
    "section": "1.1 Setting the Scene",
    "text": "1.1 Setting the Scene\nCity of Engagement, with a total population of 50,000, is a small city located at Country of Nowhere. The city serves as a service centre of an agriculture region surrounding the city. The main agriculture of the region is fruit farms and vineyards.\nThe city council is in the process of preparing the Local Plan 2023. A sample survey of representative residents had been conducted to collect data related to their household demographic and spending patterns, among other things. The city aims to use the data to assist with their major community revitalization efforts, including how to allocate a very large city renewal grant they have recently received."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#our-task",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#our-task",
    "title": "The City of Engagement: Undestanding the Demographics and Financial Characteristics of its Residents",
    "section": "1.2 Our Task",
    "text": "1.2 Our Task\nWe are required to create a user-friendly and interactive solution to help city managers and planners to explore the complex data in an engaging way and reveal hidden patterns.\nTo this end, we will apply the concepts and methods learned from Lesson 1-4 of the course to reveal the demographic and financial characteristics of the city, using appropriate static and interactive statistical graphics methods."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#load-the-relevant-packages-into-the-r-environment",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#load-the-relevant-packages-into-the-r-environment",
    "title": "The City of Engagement: Undestanding the Demographics and Financial Characteristics of its Residents",
    "section": "2.1 Load the relevant packages into the R environment",
    "text": "2.1 Load the relevant packages into the R environment\nWe use the pacman::p_load() function to load the required R packages into our working environment. The loaded packages are:\n\nplotly: For creating interactive web-based graphs.\nggstatsplot: For creating graphics with details from statistical tests.\nggdist: For visualising distribution and uncertainty\nggthemes: Provides additional themes for ggplot2\ngridExtra: For combining multiple plots into a single plot\ntidyverse: A collection of core packages designed for data science, used extensively for data preparation and wrangling.\nDT: For creating interactive tables using the DataTables JavaScript library\nreshape2: For transforming dataframes from one shape to another\n\n\n\nShow the code\n#Load packages\npacman::p_load(plotly, ggstatsplot, ggdist, gridExtra,ggthemes, tidyverse, DT, reshape2)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#import-the-data-sets",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#import-the-data-sets",
    "title": "The City of Engagement: Undestanding the Demographics and Financial Characteristics of its Residents",
    "section": "2.2 Import the data-sets",
    "text": "2.2 Import the data-sets\nTwo data-sets are provided for this exercise:\n(i) Partcipants.csv: contains information about the 1011 city residents who responded to the survey. Other than an identifier variable, we also have the household size, a boolean indicator for households having children, age, education level, interest group and joviality (or happiness) level of the respondents.\nWe import the records in the Participants.csv file as survey, convert the participantId, householdSize and age from numeric to integer data type, and convert educationLevel from character data type to an ordinal factor.\n\n\nShow the code\n# Import Participant.csv and assign it to survey variable\nsurvey <- read_csv('data/Participants.csv', show_col_types = FALSE ) %>%\n  mutate(participantId = as.integer(participantId),\n         householdSize = as.integer(householdSize),\n         age = as.integer(age),\n         educationLevel = as.factor(educationLevel)\n         ) %>%\n  mutate(educationLevel = ordered(educationLevel, levels = c(\"Low\",\"HighSchoolOrCollege\", \"Bachelors\",\"Graduate\")))\n\n\n(ii) FinancialJournal.csv contains the financial transactions of the respondents from 1 Mar 2022 to 28 Feb 2023. Other than an identifier variable (which permits us to cross-reference to the participants‚Äô demographic information), the time, nature and amount involved for the transactions were also provided.\nWe import the records in the FinancialJournal.csv file as financials, and convert the participantID from numeric to integer data type and round the amount values to 2 decimal point.\n\n\nShow the code\n# Import FinancialJournal.csv and assign it to financials variable\nfinancials <- read_csv('data/FinancialJournal.csv', show_col_types = FALSE) %>%\n  mutate(participantId = as.integer(participantId),\n         amount = round(amount,2)\n         )"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#create-a-month_year-column-for-temporal-analysis",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#create-a-month_year-column-for-temporal-analysis",
    "title": "The City of Engagement: Undestanding the Demographics and Financial Characteristics of its Residents",
    "section": "3.1 Create a month_year column for temporal analysis",
    "text": "3.1 Create a month_year column for temporal analysis\nThere are 1.5 million financial transactions over the 12-month period and the daily information is too granular for us to analyse. We will create a month_year column, from the timestamp column, which will then allow us to aggregate the transactions by month for temporal analysis.\n\n\nShow the code\n# Insert a month_year column\nfinancials <- financials %>%\n  mutate(month_year = format(timestamp, \"%b-%Y\")) %>%\n  mutate(month_year = as.factor(month_year)) %>%\n  mutate(month_year = ordered(month_year, levels = c(\"Mar-2022\", \"Apr-2022\",\"May-2022\",\"Jun-2022\",\"Jul-2022\",\"Aug-2022\",\"Sep-2022\",\"Oct-2022\",\"Nov-2022\", \"Dec-2022\",\"Jan-2023\",\"Feb-2023\")))"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#recode-rental-adjustments-as-part-of-shelter-expense",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#recode-rental-adjustments-as-part-of-shelter-expense",
    "title": "The City of Engagement: Undestanding the Demographics and Financial Characteristics of its Residents",
    "section": "3.2 Recode Rental Adjustments as part of Shelter Expense",
    "text": "3.2 Recode Rental Adjustments as part of Shelter Expense\nRent Adjustments do not occur frequently (only 131 transactions out of the 1.5 million financial transactions relate to RentAdjustment ) and they are essentially price adjustments associated with the cost of accommodation or rental refunds to the respondents.\n\n\nShow the code\n# Group and sumarise the transactions by category\nfinancials_grouped <- financials %>%\n  group_by(category) %>%\n  summarise(Count = n(), Sum_of_Amt = round(sum(amount),2)) \n\n# Display the results in tabular format and highliht RentalAdjustment transactions\ndatatable(financials_grouped, options = list(dom='t'), \n              caption = \"Table 1: Breakdown of Financial Journal Transactions By Category\",\n              rownames = FALSE) %>% \n    formatStyle(1,\n                target = 'row',\n                backgroundColor = styleEqual(c('RentAdjustment'), c('#c7e9c0')))\n\n\n\n\n\n\n\nAs such, Rent Adjustment transactions are recoded as Shelter in the financials table.\nAfter the recoding, the new breakdown of transactions by category is as follows:\n\n\nShow the code\n# Make a copy of the financials table \nfinancials_recoded <- financials\n\n# Recode all 'RentAdjustment' transactions to 'Shelter'\nfinancials_recoded$category <- ifelse(financials_recoded$category =='RentAdjustment','Shelter', financials_recoded$category)\n\n# Group and sumarise the transactions by category after recoding\nfinancials_grouped <- financials_recoded %>%\n  group_by(category) %>%\n  summarise(Count = n(), Sum_of_Amt = round(sum(amount),2)) \n\n# Display the results in tabular format and highliht Shelter transactions\ndatatable(financials_grouped, options = list(dom='t'), \n              caption = \"Table 2: New breakdown of Financial Journal Transactions By Category (after recoding)\",\n              rownames = FALSE) %>% \n    formatStyle(1,\n                target = 'row',\n                backgroundColor = styleEqual(c('Shelter'), c('#c7e9c0')))"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#remove-duplicate-records-found-in-the-financial-data",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#remove-duplicate-records-found-in-the-financial-data",
    "title": "The City of Engagement: Undestanding the Demographics and Financial Characteristics of its Residents",
    "section": "3.3 Remove duplicate records found in the financial data",
    "text": "3.3 Remove duplicate records found in the financial data\nWe notice duplicate records in the financial data. For instance, a survey participant (with Id=0) has duplicate records for Education and Shelter for the same amount and timestamp.\n\n\nShow the code\nparticipant_0 <- financials_recoded[,-5] %>% \n  filter(participantId=='0') %>%\n  group_by_all() %>%\n  filter(n()>1) %>%\n  arrange_all()\n  \n\ndatatable(participant_0, options = list(dom='t'), \n              caption = \"Table 3: Original and Duplicate Financial Records of a Survey Participant (Id=0)\",\n              rownames = FALSE) %>% \n  formatStyle(3,\n                target = 'row',\n                backgroundColor = styleEqual(c('Education'), c('#efedf5'))) %>%\n  formatStyle(3,\n                target = 'row',\n                backgroundColor = styleEqual(c('Shelter'), c('#9e9ac8')))\n\n\n\n\n\n\n\nApplying the duplicated() function on the financial data, we have:\n\n\nShow the code\npaste(sum(duplicated(financials_recoded)),\"duplicate records.\")\n\n\n[1] \"1113 duplicate records.\"\n\n\nWe are unsure of the reasons behind the duplicate records üòï. Nonetheless, we will use the distinct() function on the financial data to only retain unique records for our analysis.\n\n\nShow the code\nfinancials_unique <- financials_recoded %>%\n  distinct()"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#summarise-the-financial-information-by-participants",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#summarise-the-financial-information-by-participants",
    "title": "The City of Engagement: Undestanding the Demographics and Financial Characteristics of its Residents",
    "section": "3.4 Summarise the financial information by participants",
    "text": "3.4 Summarise the financial information by participants\nWe aggregate the financial information by participant, taking into consideration the timestamp and category using the dcast() function from reshape2 package.\nAfter performing the following steps, we notice 131 survey participants only have transaction records for Mar-2022 , but not in the other months :\n\nGroup the transactions by participantId, pivot the information by month_year and count the number of transactions per month\nAdd a new column, ‚ÄúMonths_with_trans‚Äù, which sum up the no. of months over the 12-month period where there were transactions\nPerform a value count of the ‚ÄúMonths_with_trans‚Äù column and this reveals the number of participants who only had 1 month of financial transactions.\n\n\n\nShow the code\n# Group the transactions by participantId, pivot the information by month_year and count the number of transactions per month.\nfinancials_count <- dcast(financials_unique,\n                           participantId ~ month_year, \n                           value.var = \"amount\", fun.aggregate = length)\n\n# Add a new column, \"Months_with_trans\", which sum up the no. of months over the 12-month period where there were transactions. \nfinancials_count$Months_with_trans <- apply(financials_count[,2:13]!=0,1,sum)\n\n# Perform a value count of the \"Months_with_trans\" column\nsummary_count <- financials_count %>%\n  group_by(Months_with_trans)%>%\n  summarise(Count=n())\n\ndatatable(summary_count, options = list(dom='t'), \n              caption = \"Table 4: No. of Survey Participants grouped by the count of months with transactions\",\n              rownames = FALSE) \n\n\n\n\n\n\n\nThe reason for the economic activity of the 131 participants appearing only in Mar-2022 is unclear and requires further investigation. To avoid biasing our results which covers the entire year, we will remove the records of these 131 participants from our subsequent analysis.\n\n\nShow the code\n# List those participants who only had transaction records for 1 month and assign it id_to_exclude\nids_to_exclude <- c(financials_count[financials_count$Months_with_trans==1,]$participantId)\n\n# Exclude from the financial_unique data-set participants whose id are in id_to_exclude list\nfinancials_880 <- subset(financials_unique, !(participantId %in% ids_to_exclude))\n\n\nNext, we derive the the monthly and total wages of the 880 participants. The first 5 records of the wage information are as follow:\n\n\nShow the code\nwages_880 <- \n  dcast(subset(financials_880,(category == 'Wage')), participantId ~ month_year, value.var = \"amount\", sum) %>%\n  mutate(total_wages = rowSums(.[2:13],na.rm=TRUE))\n\n\n# Inspect the first 5 records \ndatatable(head(wages_880, n=5), options = list(dom='t'), \n              caption = \"Table 5: First 5 records of the Monthly and Total Wages\",\n              rownames = FALSE) \n\n\n\n\n\n\n\nBased on Table 2 above, Education, Shelter, Food and Entertainment are all living expenses and we will also work out the monthly living expenses and the year‚Äôs total for the 880 participants. The first 5 records of the expenses information are as follow:\n\n\nShow the code\nexpenses_880 <- dcast(subset(financials_880,!(category == 'Wage')), participantId ~ month_year, value.var = \"amount\", sum) %>%\n  mutate(total_expenses = rowSums(.[2:13],na.rm=TRUE))\n\n# Inspect the first 5 records \ndatatable(head(expenses_880, n=5), options = list(dom='t'), \n              caption = \"Table 6: First 5 records of the Monthly and Total Expenses\",\n              rownames = FALSE)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#combine-the-participants-demographic-and-financial-data",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#combine-the-participants-demographic-and-financial-data",
    "title": "The City of Engagement: Undestanding the Demographics and Financial Characteristics of its Residents",
    "section": "3.5 Combine the participants‚Äô demographic and financial data",
    "text": "3.5 Combine the participants‚Äô demographic and financial data\nWe combine both sets of information into a single table (named as merged) and include 3 new columns:\n\ntotal_surplus : refers to the amount of wage surplus (or deficit) based on the difference between total wage and total expenses for the year.\nexpense_to_wage: the proportion of total expenses over total wages\nsurplus_to_wage: the proportion of total surplus over total wages\n\nfor our subsequent analysis.\nThe first 5 rows of the merged table are as follow:\n\n\nShow the code\nmerged <- survey %>% \n  inner_join(select(wages_880, participantId, total_wages), by = \"participantId\") %>%\n  inner_join(select(expenses_880, participantId, total_expenses), by = \"participantId\") %>%\n  mutate(total_surplus = total_wages + total_expenses,\n         expense_to_wage = round(abs(total_expenses/total_wages),3),\n         surplus_to_wage = round(total_surplus/total_wages,3)\n  )\n  \ndatatable(head(merged, n=5), options = list(dom='t'), \n              caption = \"Table 7: First 5 records of the Merged Demgraphic and Financial Data Table\",\n              rownames = FALSE)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#by-education-level",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#by-education-level",
    "title": "The City of Engagement: Undestanding the Demographics and Financial Characteristics of its Residents",
    "section": "4.1 By Education Level",
    "text": "4.1 By Education Level\n\n4.1.1 Breakdown on the proportion of participants by Education Level\n\n\n\n\n\n\nDesign Considerations for the Bar Plots\n\n\n\n\nSet a color scheme for each education level for consistency across mutliple charts\nProvide informative statistics about the participants for each education level in the tooltip. The content of the tips is customized through the aes() function within the geom_bar layer.\nUse plotly() for interactivity and displaying dynamic tooltip\n\n\n\n\n\nShow the code\nedu_prop <- merged %>%\n  group_by(educationLevel) %>%\n  summarise(count = n(),\n            avg_age = round(mean(age),0),\n            avg_hh = round(mean(householdSize),1),\n            avg_joviality = round(mean(joviality),3)) %>%\n  ungroup() %>%\n  mutate(proportion = count/sum(count))\n\n# Set a color scheme for Education Level analysis for consistency\ncolors <- c('#CCCCCC', '#808585', '#9067a7', '#ab6857')\n\nedu_col <- ggplot(edu_prop,aes(x = educationLevel, y = proportion, fill=educationLevel) ) +\n  geom_bar(stat = \"identity\",\n           aes(text = paste(\"Education Level:\", educationLevel,\n                              \"<br>Count:\", count,\n                              \"<br>Proportion:\", round(proportion*100,1), '%',\n                              \"<br>Average Age:\",avg_age,\n                              \"<br>Average Household Size:\", avg_hh,\n                              \"<br>Average Joviality:\", avg_joviality))) +\n  labs(x = \"Education Level\", \n       y = \"Proportion\", \n       title = \"93.6% of Participants receive College or Higher Level Education\",\n       subtitle = \"Most participants surveyed are literate\") +\n  # Set y-axis range from 0 to 0.50\n  scale_y_continuous(limits = c(0, 0.50)) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_fill_manual(values = colors) \n\n\nggplotly(edu_col, tooltip = 'text')\n\n\n\n\n\n\nFrom the above, we observe that majority of the participants have at least high school or college education. High literacy rate often correlates with higher knowledge-based economy and greater interest for cultural and intellectual activities. As we mouse over each column, we notice the following:\n\nAverage age across all education levels ranges from 38-40 years old\nAverage household size is around 2 persons\nHigher degree of joviality seems to be associated with higher education level.\n\n\n\n4.1.2 Distribution of total wages for the year by Education Level\n\n\n\n\n\n\nDesign Considerations for the Distribution Plots\n\n\n\n\nWith survey data, it‚Äôs useful to check the distribution of the data points for total wage and total expenses to get an intuition of how they differ for each education level. stat_halfeye() function is used to create half-density distribution for every education level\nA boxplot, created using geom_boxplot(), shows the median and mean values on the chart to complements the analysis\nThe x- and y- axes are switched around using the coord_flip()as it is easier to compare values horizontally\nThe stat_summary() function is used to add the marker for mean and text labels for median and mean values onto the chart\n\n\n\n\n\nShow the code\nggplot(merged, \n       aes(x = educationLevel, \n           y = total_wages)) +\n  stat_halfeye(aes(fill=educationLevel),\n               adjust = 1.0,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA,\n               alpha=0.5) +\n  coord_flip() +\n  theme_minimal() +\n  # Add geom_text layer for displaying median values\n  stat_summary(fun = median, geom = \"text\", aes(label = round(after_stat(y), 0)),\n               position = position_nudge(x = 0.15), vjust = -0.5,size=3) +\n  # Add geom_text layer for displaying mean dot in red\n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 3, color = \"#B00000\",\n               position = position_nudge(x = 0.0)) +\n  # Add geom_text layer for displaying mean values in red\n  stat_summary(fun = mean, geom = \"text\", aes(label = round(after_stat(y), 0)),\n               position = position_nudge(x = 0.15), vjust = 4, color = \"#B00000\",size=3) +\n  scale_fill_manual(values = colors) + \n  theme(legend.position = \"none\") + \n  scale_y_continuous(limits = c(0, 125000)) +\n  labs(x = \"Education Level\", \n       y = \"Total Wages for the Year($)\", \n       title = \"Participants who receive tertiary education earn considerably more\",\n       subtitle = \"\\n(Black fonts: Median Wage; Red fonts: Mean Wage)\") \n\n\n\n\n\nIt‚Äôs not surprising that the distribution of wages across all education levels are left-skewed. However, it‚Äôs worth noting that the wage distribution of participants who have tertiary education appear to be more evenly distributed than those who only receive College or lower education.\n\n\n4.1.3 Distribution of total expenses for the year by Education Level\nWith more income, does it mean that participants with tertiary education would spend more? Let‚Äôs find out!\n\n\nShow the code\nggplot(merged, \n       aes(x = educationLevel, \n           y = abs(total_expenses))) +\n  stat_halfeye(aes(fill=educationLevel),\n               adjust = 1.0,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA,\n               alpha = 0.5) +\n  # stat_dots(side = \"left\", \n  #           justification = 1.2, \n  #           binwidth = .5,\n  #           dotsize = 1.5) +\n  # Add'l codes from the previous plot\n  coord_flip() +\n  theme_minimal() +\n  # Add geom_text layer for displaying median values\n  stat_summary(fun = median, geom = \"text\", aes(label = round(after_stat(y), )),\n               position = position_nudge(x = 0.15), vjust = -0.5,size=3) +\n  # Add geom_text layer for displaying mean dot in red\n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 3, color = \"#B00000\",\n               position = position_nudge(x = 0.0)) +\n  stat_summary(fun = mean, geom = \"text\", aes(label = round(after_stat(y), 0)),\n               position = position_nudge(x = 0.15), vjust = 4, color = \"#B00000\",size=3) +\n  scale_fill_manual(values = colors) + \n  theme(legend.position = \"none\") + \n  labs(x = \"Education Level\", \n       y = \"Total Expenses for the Year($)\", \n       title = \"Participants Across All Education Levels Spend\\nbetween $15k and $17.5k on average\",\n       subtitle = \"\\n(Black fonts: Median Expenses; Red fonts: Mean Expenses)\") \n\n\n\n\n\nThe gap in expenses between the tertiary and non-tertiary educated participants is not as wide as their wages."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#by-household-size-and-kids",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#by-household-size-and-kids",
    "title": "The City of Engagement: Undestanding the Demographics and Financial Characteristics of its Residents",
    "section": "4.2 By Household Size and Kids",
    "text": "4.2 By Household Size and Kids\nLet‚Äôs check out some basic statistics on household size.\n\n4.2.1 Breakdown on the proportion of participants by Household Size\n\n\nShow the code\n# Set a color scheme for Household Size Level analysis for consistency\ncolors2 <- c('#CC9881', '#FADFC1', '#E3A6AB', '#A87B9F')\n\nhh_table <- merged %>%\n  group_by(householdSize,haveKids) %>%\n  summarise(count = n(),\n            avg_age = round(mean(age),0),\n            avg_joviality = round(mean(joviality),3)) %>%\n  ungroup() %>%\n  mutate(proportion = round(proportions(count),2)) %>%\n  select(householdSize,haveKids,count,proportion,avg_age,avg_joviality) %>%\n  mutate(householdSize = as.factor(householdSize))\n\nhh_bar <- ggplot(hh_table,aes(x = householdSize, y = proportion, fill=householdSize) ) +\n  geom_bar(stat = \"identity\",\n           aes(text = paste(\"Household Size:\", householdSize,\n                              \"<br>Have Kids:\", haveKids,\n                              \"<br>Count:\", count,\n                              \"<br>Proportion:\", round(proportion*100,1), '%',\n                              \"<br>Average Age:\",avg_age,\n                              \"<br>Average Joviality:\", avg_joviality))) +\n  labs(x = \"Household Size\", \n       y = \"Proportion\", \n       title = \"Fairly Proportionate Household Size with 1 to 3 Members\",\n       subtitle = \"\") +\n  # Set y-axis range from 0 to 0.40\n  scale_y_continuous(limits = c(0, 0.4)) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_fill_manual(values = colors2) \n\n\nggplotly(hh_bar, tooltip = 'text')\n\n\n\n\n\n\nFrom the above, we observe that household size is small and the distribution of family sizes is also fairly proportionate, with about one-third of households having 1, 2 or 3 members each. As we mouse over the each column, we notice the following:\n\nOnly households with the size of 3 have kids\nJoviality appears to be declining as the household size increases from 1 to 3\nSimilar to the split by education level, average age of participants across different household sizes ranges from 38 to 40 years old. This indicates that the age of participants in the survey is likely to be uniformly or normally distributed.\n\n\n\n4.2.2 Distribution of total wages for the year by Household Size\nLet‚Äôs examine the distribution of the participants‚Äô wages across different household sizes.\n\n\nShow the code\nggplot(merged %>%\n      mutate(householdSize = as.factor(householdSize)), \n      aes(x = householdSize, \n           y = total_wages)) +\n  stat_halfeye(aes(fill=householdSize),\n               adjust = 1.0,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA,\n               alpha=0.5) +\n  coord_flip() +\n  theme_minimal() +\n  # Add geom_text layer for displaying median values\n  stat_summary(fun = median, geom = \"text\", aes(label = round(after_stat(y), 0)),\n               position = position_nudge(x = 0.15), vjust = -0.5,size=3) +\n  # Add geom_text layer for displaying mean dot in red\n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 3, color = \"#B00000\",\n               position = position_nudge(x = 0.0)) +\n  # Add geom_text layer for displaying mean values in red\n  stat_summary(fun = mean, geom = \"text\", aes(label = round(after_stat(y), 0)),\n               position = position_nudge(x = 0.15), vjust = 4, color = \"#B00000\",size=3) +\n  scale_fill_manual(values = colors2) + \n  theme(legend.position = \"none\") + \n  scale_y_continuous(limits = c(0, 125000)) +\n  labs(x = \"Household Size\", \n       y = \"Total Wages for the Year($)\", \n       title = \"Participants from Larger Houesholds Tend to Earn More\",\n       subtitle = \"\\n(Black fonts: Median Wage; Red fonts: Mean Wage)\") \n\n\n\n\n\nWhile larger households earn more, it is interesting to note that the average wage for 2- and 3-member households are comparable.\n\n\n4.2.3 Distribution of total expenses for the year by Household Size\nLet‚Äôs also find out if the participants from bigger households spend more as well.\n\n\nShow the code\nggplot(merged %>%\n      mutate(householdSize = as.factor(householdSize)),\n     aes(x = householdSize, \n           y = abs(total_expenses))) +\n  stat_halfeye(aes(fill=householdSize),\n               adjust = 1.0,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA,\n               alpha = 0.5) +\n  coord_flip() +\n  theme_minimal() +\n  # Add geom_text layer for displaying median values\n  stat_summary(fun = median, geom = \"text\", aes(label = round(after_stat(y), )),\n               position = position_nudge(x = 0.15), vjust = -0.5,size=3) +\n  # Add geom_text layer for displaying mean dot in red\n  stat_summary(fun = mean, geom = \"point\", shape = 16, size = 3, color = \"#B00000\",\n               position = position_nudge(x = 0.0)) +\n  stat_summary(fun = mean, geom = \"text\", aes(label = round(after_stat(y), 0)),\n               position = position_nudge(x = 0.15), vjust = 4, color = \"#B00000\",size=3) +\n  scale_fill_manual(values = colors2) + \n  theme(legend.position = \"none\") + \n  labs(x = \"Household Size\", \n       y = \"Total Expenses for the Year($)\", \n       title = \"Participants from Larger Houesholds Tend to Spend More As Well\",\n       subtitle = \"\\n(Black fonts: Median Expenses; Red fonts: Mean Expenses)\") \n\n\n\n\n\nSimilar to the observation for wages, 2- and 3- member households average spend is close to each other."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#overall-age-distribution",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#overall-age-distribution",
    "title": "The City of Engagement: Undestanding the Demographics and Financial Characteristics of its Residents",
    "section": "4.3 Overall Age Distribution",
    "text": "4.3 Overall Age Distribution\nEarlier on, we observed that the average age of participants ranges from 38-40 regardless of whether we slice the data by Education Level or Household Size. Visualising the age distribution of the participants would give us a sense of whether the population of the city is growing or ageing, which in turn has a social and economic implications to the city‚Äôs development.\n\n\n\n\n\n\nDesign Considerations for Histograms\n\n\n\n\nApply a simple design so that we focus on appreciating the ‚Äúshape‚Äù of the distribution and get the ‚Äúbig‚Äù picture first.\nggplotly() is used to provide interactivity and details on the data.\n\n\n\n\n\nShow the code\nage_dist <- ggplot(data=merged, \n       aes(x= age)) +\n  geom_histogram(binwidth = 5, \n                 color=\"black\", \n                 fill='lightgray',\n                 linewidth = 0.1) +\n  xlim(c(15, 60)) +\n  ylim(c(0,150)) +\n  geom_vline(aes(xintercept=mean(age)),\n             color=\"red\", \n             linetype=\"dashed\", \n             size=1) +\n  geom_vline(aes(xintercept=median(age)),\n             color=\"grey30\",\n             linetype=\"dashed\", \n             size=1) +\n            xlab('Age') +\n            ylab('Count') +\n  ggtitle(\"Slow Population Growth in the City\") +\n  theme_minimal()\n\nggplotly(age_dist)\n\n\n\n\n\n\nThe relatively flat distribution for the age histogram above indicates slow population growth in the city, with the residents just reproducing enough to replace itself. Nonetheless, we have to check the survey‚Äôs participant identification criteria to ensure that the result above is not due to selection bias."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#overall-distribution-of-joviality-index",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#overall-distribution-of-joviality-index",
    "title": "The City of Engagement: Undestanding the Demographics and Financial Characteristics of its Residents",
    "section": "4.4 Overall Distribution of Joviality Index",
    "text": "4.4 Overall Distribution of Joviality Index\nIt is meaningful to assess the sentiments of the 880 participants who have been around in the past year, as this provides valuable insights into the well-being and quality of life of the participants. The joviality measure can be used to identify factors that contribute to happiness, thus allowing city planners to prioritise resources in areas that have a positive impact on residents‚Äô well-being.\n\n\nShow the code\njoviality_dist <- ggplot(data=merged, \n       aes(x= joviality)) +\n  geom_histogram(bins = 20, \n                 color=\"black\", \n                 fill='lightgray',\n                 linewidth = 0.1) +\n  geom_vline(aes(xintercept=mean(joviality)),\n             color=\"red\", \n             linetype=\"dashed\", \n             size=1) +\n  geom_vline(aes(xintercept=median(joviality)),\n             color=\"grey30\",\n             linetype=\"dashed\", \n             size=1) +\n            xlab('Joviality Index') +\n            ylab('Count') +\n  ggtitle(\"The 880 Pariticipants are Moderately Unhappy üòê\") +\n  theme_minimal()\n\nggplotly(joviality_dist)\n\n\n\n\n\n\nWe can see that distribution of the joviality index is left-skewed and both the median and mean index values are at 0.44 and 0.47 respectively. This indicates that those surveyed are not too pleased with their quality of live in the city and it is worth investigating the factors that influence the index.\nHow about the 131 survey participants who were only around in Mar 2022? Did they leave because they were unhappy too?\nLet‚Äôs do a similar plot for them as well.\n\n\nShow the code\njoviality_dist_131 <- ggplot(data = survey %>%\n                               # Include this line to retrieve the records of the missing 131 participants\n                               filter(participantId %in% ids_to_exclude),\n                             aes(x = joviality)) +\n  geom_histogram(bins = 20, \n                 color = \"black\", \n                 fill = 'lightgray',\n                 linewidth = 0.1) +\n  geom_vline(aes(xintercept = mean(joviality)),\n             color = \"red\", \n             linetype = \"dashed\", \n             size = 1) +\n  geom_vline(aes(xintercept = median(joviality)),\n             color = \"grey30\",\n             linetype = \"dashed\", \n             size = 1) +\n  xlab('Joviality Index') +\n  ylab('Count') +\n  ggtitle(\"The 131 Missing Participants were Happier!!üòï \") +\n  theme_minimal()\n\n\nggplotly(joviality_dist_131)\n\n\n\n\n\n\nSurprisingly, the median and mean joviality index of the 131 participants who went ‚Äúmissing‚Äù after Mar 2022 is much higher. It‚Äôs definitely worth taking a deeper look into why this group of participants was happier."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#whether-a-higher-education-qualification-results-in-more-wages",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#whether-a-higher-education-qualification-results-in-more-wages",
    "title": "The City of Engagement: Undestanding the Demographics and Financial Characteristics of its Residents",
    "section": "5.1 Whether a higher education qualification results in more wages",
    "text": "5.1 Whether a higher education qualification results in more wages\nBefore we start validating our hypotheses, we will develop a function that will help us visualise if the data is normally distributed using a Quantile-Quantile (QQ) Plot. At the same time, we also want to conduct a Shapiro-Wilk Test for Normality to confirm that the data points really follow a normal distribution. The function will make the repetitive task of generating a normality test result for each data set easier.\n\n\n\n\n\n\nDesign Considerations for the Normality Test function\n\n\n\n\nAccept inputs for a dataframe, categorical variable and a continuous measure that we will be applying the normality test on.\nAccept inputs for the plot title and y-axis label.\nConvert non-categorical variable into one that is categorical and also obtain the unique values of the variable\nDraw a QQ plot based on the inputs\nCompute the p-value for the Shapiro-Wilk Test and indicate if the data set is normally distributed based on the significance level of 0.05.\nAnnotate the Shapiro-Wilk Test result on the top-left of the QQ Plot . To reduce clutter, it is sufficient to just show the p-value and the test outcome on the plot\nArrange all output plots in 2 columns\n\n\n\n\n\nShow the code\nnormality_test <- function(df, \n                            column, \n                            measure,\n                            title,\n                            ylabel) {\n  \n  category <- if (is.factor(df[[column]])) {\n    levels(df[[column]])\n  } else if (is.character(df[[column]])) {\n    sort(unique(df[[column]]))\n  } else {\n    sort(unique(as.character(df[[column]])))\n  }\n  \n  plots <- list()\n  color_counter <- 1\n  \n  for (class in category) {\n    \n    subset_df <- filter(df, !!sym(column) == class)\n    \n    shapiro_res <- shapiro.test(subset_df[[measure]])\n    \n    p_value <- format(shapiro_res$p.value, digits = 3)\n    is_normal <- ifelse(shapiro_res$p.value > 0.05, \"Yes\", \"No\")\n    \n    g <- ggplot(subset_df, aes(sample = !!sym(measure))) +\n      stat_qq() +\n      stat_qq_line() +\n      ggtitle(paste(title, class)) +\n      xlab(paste(column,':',class)) + \n      ylab(ylabel) +\n      theme_light() +\n      annotate(\"label\", x = -0.5, y = Inf, hjust = 1, vjust = 1.5, \n               label = paste(\"Shapiro-Wilk Normality Test:\", \"\\n\",\n                             \"p-value = \", p_value, \"\\n\",\n                             \"Is normal distn? \", is_normal),\n                fill = \"white\"\n                              ) \n    \n    plots[[class]] <- g\n  }\n  \n  grid.arrange(grobs = plots, ncol = 2)\n}\n\n\nStep1: For the Shapiro-Wilk Test, we hypothesize,\n\nH0 = The data is normally distributed at 0.05 level of significance\n\nWe run the function by providing the relevant parameters:\n\ndf : merged\ncolumn: ‚ÄòeducationLevel‚Äô\nmeasure: ‚Äòtotal_wages‚Äô\ntitle: ‚ÄúNormality Test on Total Wages‚Äô\nylabel: ‚ÄòTotal Wages ($)‚Äô\n\n\n\nShow the code\nnormality_test(merged, 'educationLevel','total_wages',\"Normality Test on Total Wages for\",\"Total Wages($)\")\n\n\n\n\n\nThe results confirm that we can‚Äôt assume Normality for the data and we will proceed to apply a non-parametric test to check if the participants‚Äô wages are different based on their education level.\nStep 2: For the nonparametric Kruskal-Willis Test, we hypothesize,\n\nH0 = There is no difference between the median total wage of the participants with different education level at 0.05 level of significance.\n\n\n\n\n\n\n\nDesign Consideration for the ANOVA Plot\n\n\n\n\nSet a non-parametric test\nEnable pairwise comparison and only display significant results\nDisable the display of Bayes Factor to reduce clutter\nEnable notch and set notch width since we are using Median\n\n\n\n\n\nShow the code\nset.seed(123)\n\n# Use a color scheme for Education Level analysis for consistency\ncolors <- c('#CCCCCC', '#808585', '#9067a7', '#ab6857')\n\nggbetweenstats(\n  data = merged,\n  x = educationLevel, \n  y = total_wages,\n  type = \"np\",\n  pairwise_comparisons=TRUE,\n  pairwise.display = \"s\",\n  bf.message = FALSE,\n  notch = TRUE,\n  notchwidth = 0.1,\n  ylab = \"Total Wages ($)\",\n  title = \"One-Way ANOVA on Total Wages \",\n  messages = FALSE\n)  +\n  scale_color_manual(values = colors) +\n  scale_y_continuous(limits = c(0, 300000))\n\n\n\n\n\nThe p-value of the Kruskal-Willis Test is < 0.05 and there is a statistically significant difference in total wages. This is further supported by the pairwise comparison of total wages between the education levels which shows significant differences in their wages among all levels.\nWith the above, we reject the null hypothesis and conclude that there is sufficient statistical evidence that the wages of participants with different education levels are different."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#whether-participants-with-higher-education-qualification-are-happier",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#whether-participants-with-higher-education-qualification-are-happier",
    "title": "The City of Engagement: Undestanding the Demographics and Financial Characteristics of its Residents",
    "section": "5.2 Whether participants with higher education qualification are happier",
    "text": "5.2 Whether participants with higher education qualification are happier\nWhile participants with higher education qualification receive more wages, does a better academic qualification translate to happiness?\nStep1: For the Shapiro-Wilk Test, we hypothesize,\n\nH0 = The Joviality Index data for each Education Level is normally distributed at 0.05 level of significance\n\n\n\nShow the code\nnormality_test(merged, 'educationLevel','joviality',\"Normality Test on Joviality Index for\",\"Joviality Index\")\n\n\n\n\n\nThe results confirm that we can‚Äôt assume Normality for the data and we will proceed to apply a non-parametric test.\nStep 2: For the nonparametric Kruskal-Willis Test, we hypothesize,\n\nH0 = There is no difference between the median joviality index of the participants with different education level at 0.05 level of significance.\n\n\n\nShow the code\nset.seed(123)\n\nggbetweenstats(\n  data = merged,\n  x = educationLevel, \n  y = joviality,\n  type = \"np\",\n  pairwise_comparisons=TRUE,\n  pairwise.display = \"s\",\n  bf.message = FALSE,\n  notch = TRUE,\n  notchwidth = 0.1,\n  ylab = \"Joviality Index\",\n  title = \"One-Way ANOVA on Joviality\",\n  messages = FALSE\n)  +\n  scale_color_manual(values = colors) +\n  # Increase the ylim > 1.0 so that the pairwise results can be displayed\n  scale_y_continuous(limits = c(0, 1.25))\n\n\n\n\n\nAlthough ANOVA test result show a p-value < 0.005 indicating that there is a difference in joviality among participants with different education levels and permitting us to reject the null hypothesis, this is mainly attributed to the significant difference between the joviality of participants who attended high school and graduate school."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#whether-having-children-result-in-higher-expenditure",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#whether-having-children-result-in-higher-expenditure",
    "title": "The City of Engagement: Undestanding the Demographics and Financial Characteristics of its Residents",
    "section": "5.3 Whether having children result in higher expenditure",
    "text": "5.3 Whether having children result in higher expenditure\nThis is a follow-up to the observation in Section 4.2 where we noticed households with kids have higher expenditures. We will proceed to validate this observation.\nStep1: For the Shapiro-Wilk Test, we hypothesize,\n\nH0 = The total expenses for households with and without kids is normally distributed at 0.05 level of significance\n\n\n\nShow the code\nnormality_test(merged, 'haveKids','total_expenses',\"Normality Test on Total Expenses Index for\",\"Total Expenses($)\")\n\n\n\n\n\nThe results confirm that we can‚Äôt assume Normality for the data and we will proceed to apply a non-parametric test for the 2 samples.\nStep 2: For the non-parametric Mann-Whitney Test, we hypothesize,\n\nH0 = There is no difference between the total expenses for households with and without kids at 0.05 level of significance.\n\n\n\nShow the code\nset.seed(123)\n\n# Set a color scheme for Household Size Level analysis for consistency\ncolors2 <- c('#FADFC1', '#E3A6AB', '#A87B9F')\n\nggbetweenstats(\n  data = merged %>%\n    mutate(total_expenses = abs(total_expenses)),\n  x = haveKids, \n  y = total_expenses,\n  type = \"np\",\n  pairwise_comparisons=TRUE,\n  pairwise.display = \"s\",\n  bf.message = FALSE,\n  notch = TRUE,\n  notchwidth = 0.1,\n  ylab = \"Total Expenses($)\",\n  title = \"2-Sample Non-Parametric Test on Total Expenses\",\n  messages = FALSE\n)  +\n  scale_color_manual(values = colors2) \n\n\n\n\n\nThe test results reveal that there is indeed a significant difference between the total expenses of participants who have kids and those who don‚Äôt and we will reject the null hypothesis.\nExpenses are made up of Education, Shelter, Food and Recreation. We will dive deeper to understand which of these expense types resulted in the significant difference in the expenditure of the 2 groups.\nTo do this, we have to first extract the individual expenses items of the 880 participants for the year before charting an interval plot by expense type.\n\n\n\n\n\n\nDesign Considerations for Interval Plot\n\n\n\n\nFor each expense item, we want to compare the spent by households with and without kids. To place each pair of interval lines together, we have to set the fill argument of the aes() function to the haveKids attribute, and set the position argument in the stat_pointinterval() function to ‚Äôposition_dodge()‚Äô.\nSet point_interval argument as median quartile since data is not normally distributed\nAlign the color scheme with the previous plot by setting the values argument within the scale_fill_manual() function for consistency.\n\n\n\n\n\nShow the code\n# Extract the individual expenses items of the 880 participants for the year\n\nexpenses_by_id_category <- financials_880 %>%\n  filter(category != 'Wage') %>%\n  group_by(participantId, category) %>%\n  summarise(total_spent = sum(abs(amount))) %>%\n  inner_join(select(survey, participantId, haveKids), by = \"participantId\") \n\n# Prepare the interval plot\n\np<- expenses_by_id_category %>% \n  ggplot(aes(category, total_spent,fill=haveKids)) +\n  stat_pointinterval(\n    aes(interval_color = stat(fill)),\n    position = position_dodge(),\n    point_interval = median_qi,\n    .width = 0.95,\n    show.legend = TRUE\n    ) +\n  ylab(\"Total Spent ($)\") +\n  xlab(\"Expense Category\") +\n  theme_minimal() +\n  # Align the color scheme with the previou plot\n  scale_fill_manual(\n    values = c(\"#FADFC1\", \"#E3A6AB\"),\n    labels = c(\"FALSE\", \"TRUE\"),\n    guide = guide_legend(title = \"haveKids\"),\n    aesthetics = \"interval_color\") +\n  #Title, subtitle, and caption\n  labs(title = 'Visualising Uncertainty in Median Spend of Participants by Expense Type',\n  subtitle = '95% Quantiles intervals of median spend between participants With and WithOut kids') +\n  theme(plot.title = element_text(face = \"bold\", size = 14),\n        plot.subtitle = element_text(size = 10),\n        legend.position = \"top\") \n\np\n\n\n\n\n\nBased on the interval plot above, we see that all participants (with and without kids) have overlapping interval lines for food, recreation and shelter, indicating there is uncertainty on whether this is a significant difference in the median spend at 95% quantile for the 2 groups of participants on the 3 expense types.\nIt is also apparent that the spending on education , probably for the kids, is an expense item that participants without kids do not have to incur."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#whether-bigger-households-face-a-higher-burden-in-cost-of-living",
    "href": "Take-Home_Ex/Take-Home_Ex01/Take-Home_Ex1.html#whether-bigger-households-face-a-higher-burden-in-cost-of-living",
    "title": "The City of Engagement: Undestanding the Demographics and Financial Characteristics of its Residents",
    "section": "5.4 Whether bigger households face a higher burden in cost of living",
    "text": "5.4 Whether bigger households face a higher burden in cost of living\nWe will use the total expense to total wage ratio as a proxy measure for the cost of living in the city.\nStep1: For the Shapiro-Wilk Test, we hypothesize,\n\nH0 = The expense-to-wage ratio for different household sizes is normally distributed at 0.05 level of significance\n\n\n\nShow the code\nnormality_test(merged, 'householdSize','expense_to_wage',\"Normality Test on Expense To Wage Ratio for Household Size\",\"Expense-to-Wage Ratio\")\n\n\n\n\n\nSince we cannot assume Normality for the data based on the results of Step 1, we will apply non-parametric test to test the significance of the difference.\nStep 2: For the nonparametric Kruskal-Willis Test, we hypothesize,\n\nH0 = There is no difference between the expense-to-wage ratio of the participants with different household size at 0.05 level of significance.\n\n\n\nShow the code\nset.seed(123)\n\n# Set a color scheme for Household Size Level analysis for consistency\ncolors2 <- c('#CC9881', '#FADFC1', '#E3A6AB', '#A87B9F')\n\nggbetweenstats(\n  data = merged,\n  x = householdSize, \n  y = expense_to_wage,\n  type = \"np\",\n  pairwise_comparisons=TRUE,\n  pairwise.display = \"s\",\n  bf.message = FALSE,\n  notch = TRUE,\n  notchwidth = 0.1,\n  ylab = \"Expense-to-Wage Ratio\",\n  title = \"One-Way ANOVA on Expense-to-Wage Ratio\",\n  messages = FALSE\n)  +\n  scale_color_manual(values = colors2) \n\n\n\n\n\nThe p-value is > 0.05 indicating that there is insufficient evidence to reject the null hypothesis that the cost-to-wage ratio for larger households are different. Hence, we can also infer that even though we noticed families with kids incur higher expenses in Section 5.3, generally, the residents of these bigger households will earn more to defray the higher cost of living."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html",
    "title": "FishEye Knowldge Graph: Identify Temporal Patterns of individual entities and between entities",
    "section": "",
    "text": "(First Published: Jun 04, 2023)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#setting-the-scene",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#setting-the-scene",
    "title": "FishEye Knowldge Graph: Identify Temporal Patterns of individual entities and between entities",
    "section": "1.1 Setting the Scene",
    "text": "1.1 Setting the Scene\nThe country of Oceanus has sought FishEye International‚Äôs help in identifying companies possibly engaged in illegal, unreported, and unregulated (IUU) fishing. As part of the collaboration, FishEye‚Äôs analysts received import/export data for Oceanus‚Äô marine and fishing industries. However, Oceanus has informed FishEye that the data is incomplete. To facilitate their analysis, FishEye transformed the trade data into a knowledge graph. Using this knowledge graph, it hopes to understand business relationships, including finding links that will help stop IUU fishing and protect marine species that are affected by it."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#our-task",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#our-task",
    "title": "FishEye Knowldge Graph: Identify Temporal Patterns of individual entities and between entities",
    "section": "1.2 Our Task",
    "text": "1.2 Our Task\nIn response to Question 1 of VAST Chaellenge 2023: Mini-Challenge 2, our tasks are to:\n\nUse visual analytics to identify temporal patterns for individual entities and between entities in the knowledge graph FishEye created from trade records, and\nCategorize the types of business relationship patterns we can identify."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#load-the-relevant-packages-into-the-r-environment",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#load-the-relevant-packages-into-the-r-environment",
    "title": "FishEye Knowldge Graph: Identify Temporal Patterns of individual entities and between entities",
    "section": "2.1 Load the relevant packages into the R environment",
    "text": "2.1 Load the relevant packages into the R environment\nWe use the pacman::p_load() function to load the required R packages into our working environment. The loaded packages are:\n\nigraph : provides functions for creating, analyzing, and visualizing graphs\nggraph: creates visualizations of graphs using the grammar of graphics approach\nvisNetwork : creates interactive network visualizations\ngraphlayouts : provides layout algorithms for graph visualization\njsonlite : for working with JSON (JavaScript Object Notation) data\nplotly : for creating interactive web-based graphs\npatchwork : for combining multiple plots into a single layout\nknitr: for dynamic report generation\nkableExtra : provides additional customization options for tables created with the knitr package,\nDT : creates interactive tables using the DataTables JavaScript library\ntreemap : for creating treemaps\n\n\n\nShow the code\npacman::p_load(igraph, tidygraph, ggraph, visNetwork, tidyverse, graphlayouts,jsonlite, plotly, patchwork, knitr, kableExtra, DT,treemap)\n\n# Set the default display settings for numeric values to see large numbers in full\noptions(scipen = 999, digits = 15)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#import-and-extract-the-data",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#import-and-extract-the-data",
    "title": "FishEye Knowldge Graph: Identify Temporal Patterns of individual entities and between entities",
    "section": "2.2 Import and Extract the data",
    "text": "2.2 Import and Extract the data\nThe given data is a directed knowledge graph provided in json format. It contains 2 sets of information _ Nodes and Edges attributes .\n\nWe first imported data as assign it to a variable mc2.\n\n\n\nShow the code\nmc2 <- fromJSON(\"data/mc2_challenge_graph.json\")\n\n\n\nNext. we extracted the nodes information from mc2 data frame\n\n\n\nShow the code\nmc2_nodes <- as_tibble(mc2$nodes) %>%\n  select(id, shpcountry, rcvcountry)\n\n\nThe nodes data frame contains the following attributes:\n\nid -- Name of the company that originated (or received) the shipment\nshpcountry -- Country the company most often associated with when shipping\nrcvcountry -- Country the company most often associated with when receiving\n\n\nThen we extracted the edges info from mc2 data frame\n\n\n\nShow the code\nmc2_edges <- as_tibble(mc2$links) %>%\n  select(source, target, arrivaldate, hscode, valueofgoods_omu, volumeteu, weightkg,  valueofgoodsusd)\n\n\nThe edges data frame contains the following attributes:\n\narrivaldate -- Date the shipment arrived at port in YYYY-MM-DD format.\nhscode -- Harmonized System code for the shipment. Can be joined with the hscodes table to get additional details.\nvalueofgoods_omu -- Customs-declared value of the total shipment, in Oceanus Monetary Units (OMU)\nvolumeteu -- The volume of the shipment in ‚ÄòTwenty-foot equivalent units‚Äô, roughly how many 20-foot standard containers would be required. (Actual number of containers may have been different as there are 20ft and 40ft standard containers and tankers that do not use containers)\nweightkg -- The weight of the shipment in kilograms (if known)\n\nSince these working files are huge, we stored the mc2 nodes and edges data frames in rds format for ease of subsequent retrieval. This code need only be executed once. Thereafter we reloaded the mc2_nodes and edges data frames for data wrangling.\n\n\nShow the code\n# write_rds(mc2_nodes, \"data/mc2_nodes.rds\")\nmc2_nodes = read_rds(\"data/mc2_nodes.rds\")\n\n#write_rds(mc2_edges, \"data/mc2_edges.rds\")\n# Read arrivaldate as data format\nmc2_edges <- read_rds(\"data/mc2_edges.rds\") %>%\n  mutate(arrivaldate = as.Date(arrivaldate, format = \"%Y-%m-%d\"))"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#data-preparation",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#data-preparation",
    "title": "FishEye Knowldge Graph: Identify Temporal Patterns of individual entities and between entities",
    "section": "2.3 Data Preparation",
    "text": "2.3 Data Preparation\n\n2.3.1 Edge Data Frame\nInspect the data frame\nWe gott some summary statistics to understand the edge data.\n\n\nShow the code\nsummary(mc2_edges)\n\n\n    source             target           arrivaldate            hscode         \n Length:5464378     Length:5464378     Min.   :2028-01-01   Length:5464378    \n Class :character   Class :character   1st Qu.:2029-09-11   Class :character  \n Mode  :character   Mode  :character   Median :2031-04-30   Mode  :character  \n                                       Mean   :2031-05-31                     \n                                       3rd Qu.:2033-02-25                     \n                                       Max.   :2034-12-30                     \n                                                                              \n valueofgoods_omu           volumeteu                 weightkg               \n Min.   :    1100.00000   Min.   :   0.000000000   Min.   :        0.000000  \n 1st Qu.:  148130.00000   1st Qu.:   0.000000000   1st Qu.:     3060.000000  \n Median :  504485.00000   Median :   0.000000000   Median :    10300.000000  \n Mean   : 1665142.29537   Mean   :   1.471786376   Mean   :    37265.707968  \n 3rd Qu.: 1202560.00000   3rd Qu.:   0.000000000   3rd Qu.:    19730.000000  \n Max.   :44744530.00000   Max.   :1215.000000000   Max.   :495492485.000000  \n NA's   :5464097          NA's   :520933                                     \n valueofgoodsusd            \n Min.   :           0.0000  \n 1st Qu.:       26815.0000  \n Median :       72040.0000  \n Mean   :      865446.5412  \n 3rd Qu.:      158030.0000  \n Max.   :225833730200.0000  \n NA's   :3017844            \n\n\nThe following are noted:\n\nThere are 7 years of transactions, ranging from 1-Jan-2028 to 30-Dec-2034\nValueofgood_omu, volumeteu and valueofgoodusd attributes contain a lot of ‚ÄôNA‚Äôs. These attributes are not useful for our analysis.\n\nCheck for the presence of duplicate records\n\n\nShow the code\n# Check for the presence of duplicate edge \npaste(sum(duplicated(mc2_edges)),\"duplicate records.\")\n\n\nWe were unsure of the reasons behind the duplicate records and did not discount the possibility that they could be genuine. On balance, we found it unlikely that duplicate records exist for every day, with same weight and among the same pair of entities. Hence, we used the distinct() function on the edge data to only retain only unique edge records for our analysis.\n\n\nShow the code\nmc2_edges_unique <- mc2_edges %>%\n  distinct()\n\n\n\n\n2.3.2 Identify edge records that relate to the fishing industry\nWe referred to the HS Nomenclature 2022 available at World Customs Organisation (WCO)‚Äôs website, and the HSN Code List that is provided by Connect2India on their website. Based on these sources, we identified the following HS codes that correspond to different categories of fishery and seafood items.\n\n\n\nHS Code\nDescription\n\n\n\n\n3-digit code\n\n\n\n- 301\nLive fish\n\n\n- 302\nFish, fresh or chilled, whole\n\n\n- 303\nFish, frozen, whole\n\n\n- 304\nFish fillets, fish meat, mince except liver, roe\n\n\n- 305\nFish,cured, smoked, fish meal for human consumption\n\n\n- 306\nCrustaceans\n\n\n- 307\nMolluscs\n\n\n- 308\nFish and crustaceans, molluscs and other aquatic\n\n\n\nNext, we imported the list of relevant HS codes and the descriptions into our work environment and used the information to extract records related to the fishery products\n\n\nShow the code\n# Import the relevant HS codes\nhscode_fish <- read_csv('data/lookup_hscode.csv', show_col_types = FALSE ) %>%\n  mutate(hscode = as.character(hscode))\n\n# Filter by 3-digit and 4-digit HS codes for fishery products\nmc2_edges_fish <- mc2_edges_unique %>%\n  filter(hscode %in% hscode_fish$hscode) %>%\n  filter(substr(hscode,start = 1,stop=3) %in% c('301','302','303','304','305','306','307','308')\n           )\n\n\nWe did a frequency count by hscode and list the top 20 transacted HS codes to gain a better understanding of the number of transactions and the quantities that were involved.\n\n\nShow the code\nfreq_count_fish <- mc2_edges_fish %>%\n  group_by(hscode) %>%\n  summarise(count = n(),\n            sum_weight = sum(weightkg)) %>%\n  inner_join(hscode_fish,select(hscode,short_desc),by = 'hscode')\n\ntop_10 <- freq_count_fish %>%\n  arrange(desc(count)) %>%\n  head(10) %>%\n  mutate(short_desc = as.character(short_desc))\n\nrest <- freq_count_fish %>%\n  filter(!(hscode %in% top_10$hscode)) %>%\n  summarise(count = sum(count),\n            sum_weight = sum(sum_weight)) %>%\n  mutate(hscode = \"others\",\n         short_desc = \"Other fishery products\")\n\nfinal_df <- bind_rows(top_10, rest)\n\nggplot(final_df, aes(x = reorder(short_desc, -count), y = count)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", alpha = 0.8) +\n  labs(x = \"HS Code Short Description\", y = \"Count\",\n       title = \"Top 10 Fishery Products Transacted between 2028 And 2034\",\n       caption = \"Transactions for the other 148 6-digit HS Codes are categorised under 'Other fishery products'\") +\n  theme_minimal() +\n  coord_flip() +\n  theme(axis.text.x = element_text(hjust = 1),\n        plot.title = element_text(hjust = 0, margin = margin(t = 20, r = 0, b = 10, l = 0)))"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#community-detection",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#community-detection",
    "title": "FishEye Knowldge Graph: Identify Temporal Patterns of individual entities and between entities",
    "section": "2.4.Community Detection",
    "text": "2.4.Community Detection\nThe filtered edge records contain 8.9k entities with 538.2k links. A network of this size was too complex for us to conduct a meaningful analysis visually. As nodes within the same community tend to have more interactions among themselves than with nodes in other communities, we partitioned them into communities and then select one community to study. This made it easier to analyze the chosen community‚Äôs transformation over time and interpret the network‚Äôs organization,\n\n2.4.1 Identify the community of interest for our subsequent analysis\n\nTo begin, we identified all unique pairs of transacting entities from the 538.2k links by removing self-links (i.e.¬†source = target) and filtering away transactions that occurred fewer than 3 times over the 7 years (i.e < 1 transaction a year on average).\n\n\n\nShow the code\nmc2_edges_aggregated2 <- mc2_edges_fish %>%\n  mutate(weeknumber = isoweek(arrivaldate),\n         year = year(arrivaldate)) %>%\n  group_by(source,target) %>%\n  summarise(weight=n(),weight_sum = sum(weightkg)) %>%\n  filter(source != target) %>%\n  # filter away edge pairs that only had 3 transactions over the 7 years\n  filter(weight >3) %>%\n  ungroup()\n\n\n\nNext, we prepared the nodes data using the unique pairs of transacting entities.\n\n\n\nShow the code\nmc2_nodes_aggregated2 <- mc2_nodes %>%\n  filter(id  %in% c(mc2_edges_aggregated2$source, mc2_edges_aggregated2$target)) %>%\n  # Duplicate the id column as this name will be replaced once we convert to when we apply tbl_graph()\n  mutate(label = id) %>%\n  arrange(label) %>%\n  mutate(row_id = row_number()) %>%\n  distinct() \n\n\n\nThereafter, we prepared the graph object, and apply the Walktrap Algorithm for community detection.\n\n\n\nShow the code\n# Create graph object\n# Note that we are using an directed graph for this analysis and we can't use the popular louvain algo as the latter only applies to undirected graphs in r. \nmc2_graph2 <- tbl_graph(nodes=mc2_nodes_aggregated2,edges = mc2_edges_aggregated2, directed = T)\n\n\nset.seed(1234)\n\n# Convert ggraph to igraph object\nigraph_obj <- as.igraph(mc2_graph2)\n\n# Detection algo. This algo accepts directed graph and is fast\ncommunity <- cluster_walktrap(igraph_obj, weights= E(igraph_obj)$weight)\nmembership <- community$membership\n\n\nResults ==> 2.4k communities were detected with only the top 28 communities having more than 10 entities.\n\n\nShow the code\n# Count the occurrences of each membership value\nmembership_counts <- table(membership)\n\n# Sort the table by membership counts in decreasing order\nsorted_membership <- sort(membership_counts, decreasing = TRUE)\n\n# Create a data frame from the sorted membership data\nmembership_data <- data.frame(Community_Id = names(sorted_membership[1:30]),\n                              Count = as.numeric(sorted_membership[1:30]))\n\n# Create a DT table from the membership data frame\nmembership_table <- datatable(membership_data, \n                              options = list(pageLength = 5\n                                            ),\n                              caption = \"Table 1: Top 30 Communities Detected and their membership size\") %>%\n  formatStyle(1,\n                target = 'row',\n                backgroundColor = styleEqual(c(14), c('#c7e9c0')))\n\n# Display the DT table\nmembership_table\n\n\n\n\n\n\n\nWe chose the community with id=14 for our analysis, which consists of 164 nodes/entities. This community is the second largest one in our dataset and it offers a good trade-off between complexity and clarity. We wanted to avoid a graph that is too dense or too sparse for our study.\n\n\nShow the code\n# Extract community id 14 from the main graph\nsubgraph_3 <-as.directed(induced_subgraph(igraph_obj, community$membership==14))\n\n# Extract nodes of the community\nsubgraph_nodes_info <- subgraph_3 %>%\n  as_tbl_graph() %>%\n  activate(nodes) %>%\n  select(-id) %>%\n  mutate(id = row_number()) %>%\n  as_tibble() \n\n# Extract edges of the community\nsubgraph_edge_info <- subgraph_3 %>%\n  as_tbl_graph() %>%\n  activate(edges) %>%\n  inner_join(subgraph_nodes_info,select(id,label), by=c('from'='id')) %>%\n  rename(source = label) %>%\n    inner_join(subgraph_nodes_info,select(id,label), by=c('to'='id')) %>%\n  rename(target = label) %>%\n  select(source,target) %>%\n  as_tibble()\n\n\n\n\n2.4.2 Extract the nodes and edge information of the selected community\nWe then extracted the edges records for the unique pairs of transacting entities in community id = 14 from the m2_edges_fish (this is data frame that we extracted in Section 2.3.2. after extracting the desired the HS Codes) as we needed to retrieve their yearly transactions. At the same time, we re-labeled and replaced the values in the id column as the entity names within the column could be overwritten when we generate the graph object.\n\n\nShow the code\n# Extract the edge info from original graph\nsubgraph_edges2 <- mc2_edges_fish %>%\n  mutate(weeknumber = isoweek(arrivaldate),\n         year = year(arrivaldate)) %>%\n  filter(source  %in% subgraph_edge_info$source, target  %in% subgraph_edge_info$target) %>%\n  group_by(source,target,year,hscode) %>%\n  summarise(weight=n(),\n            sum_weight = sum(weightkg)) %>%\n  ungroup() %>%\n  inner_join(select(hscode_fish, hscode, short_desc), by = \"hscode\")\n\n# Re-label and replace the values in the id column\nsubgraph_nodes2 <- mc2_nodes %>%\n  filter(id  %in% c(subgraph_edges2$source, subgraph_edges2$target)) %>%\n  # Duplicate the id column as this name will be replaced once we convert to when we apply tbl_graph()\n  mutate(label = id) %>%\n  arrange(label) %>%\n  mutate(row_id = row_number()) %>%\n  distinct() \n\n# Replace with NA values in shpcountry and rcvcountry to prevent downstream issue\nsubgraph_nodes2 <- replace(subgraph_nodes2, is.na(subgraph_nodes2), 'unknown')\n\n\nFinally, we generated the graph object.\n\n\nShow the code\nsubgraph_obj <- tbl_graph(nodes=subgraph_nodes2,\n                          edges=subgraph_edges2,\n                          directed = T)\n\n\nAt this point, we had the following objects for Community id 14:\n\nsubgraph_edge2\nsubgraph_nodes2\nsubgraph_obj: A tbl_graph() object created using the nodes and edges information\n\n\n\n2.4.3 Exploratory Data Analysis of the Community\n\nYear-on-Year trend of the number and quantity of fishery products transactions\n\n\n\nShow the code\n# Y-oY plot by weight\nweight_yoy <- subgraph_edges2 %>%\n  group_by(year) %>%\n  summarise(sum_weight2=round(sum(sum_weight)/1000,0)) %>%\n  ungroup()\n\nggplot(weight_yoy, aes(x = year, y = sum_weight2)) +\n  geom_line(color = \"#1f77b4\", size = 1.5) +\n  geom_text(aes(label = sum_weight2), vjust = -0.5) +\n  scale_y_continuous(limits = c(0000, max(weight_yoy$sum_weight2) * 1.1)) +\n  theme_minimal() +\n  labs(x = \"Year\", y = \"Total Weight\\n(in tonnes)\",\n       title = 'Total Weight by Year') +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\nShow the code\n# Y-oY plot by count\ncount_yoy <- subgraph_edges2 %>%\n  group_by(year) %>%\n  summarise(count=sum(weight)) %>%\n  ungroup()\n\nggplot(count_yoy, aes(x = year, y = count)) +\n  geom_line(color = \"#aec7e8\", size = 1.5) +\n  geom_text(aes(label = count), vjust = -0.5) +\n  scale_y_continuous(limits = c(0, max(count_yoy$count) * 1.1)) +\n  theme_minimal() +\n  labs(x = \"Year\", y = \"No. of Transactions\",\n       title = 'Transactions by Year') +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\n\n\nObservations:\n\n\n\n\nThere is a general uptrend in the number and quantity of fishery products transacted from Year 2028 to Year 2034\nNoticeable surge in trade in Year 2033.\n\n\n\n\nCompute Centrality Measures for the Nodes\n\n\n\nShow the code\nsubgraph_obj <- subgraph_obj %>%\n  activate(nodes) %>%\n  mutate(\n          in_deg_centrality = round(centrality_degree(weights = weight, mode = \"in\", loops = FALSE),3),\n          out_deg_centrality = round(centrality_degree(weights = weight, mode = \"out\", loops = FALSE),3),\n          out_deg_closeness = round(centrality_closeness(weights=weight,mode='out',normalized = TRUE),3),\n          in_deg_closeness = round(centrality_closeness(weights=weight,mode='in',normalized = TRUE),3),\n          between_centrality = round(centrality_betweenness(weights = weight, directed = T),3)\n          ) %>%\n  mutate(in_deg_norm = round(ifelse(in_deg_centrality == 0, 0, (in_deg_centrality - min(in_deg_centrality)) / (max(in_deg_centrality) - min(in_deg_centrality))),3),\n      out_deg_norm = round(ifelse(out_deg_centrality == 0, 0, ((out_deg_centrality - min(out_deg_centrality)) / (max(out_deg_centrality) - min(out_deg_centrality)))),3)\n    )\n\n# Create a tibble for display\nnodes_stats <- subgraph_obj %>%\n  activate(nodes) %>%\n  as_tibble() %>%\n  select(id,shpcountry,rcvcountry,in_deg_norm,out_deg_norm,between_centrality) \n\n# Display the centrality measures for the nodes\n\ndatatable(nodes_stats, class = \"compact\", options = list(pageLength = 8), \n              caption = \"Table 2: Centrality Measures of Entities in Community\",\n              rownames = FALSE)\n\n\n\n\n\n\n\n\nPlot the static graph for the community\n\n\n\nShow the code\nset.seed(123)\n\ng2_graph <- subgraph_obj %>%\n  ggraph(layout = 'nicely') +\n  geom_edge_link(aes(width=weight),\n                 alpha=0.8) +\n  scale_edge_width(range = c(0.05, 0.2)) +\n  geom_node_point(aes(color=in_deg_norm, size = out_deg_norm, alpha=0.3)) +  \n  theme_graph() +\n    labs(title = \"Network Graph of Community Id 14 from Year 2028 to 2034\") +\n  theme(legend.position = \"bottom\") \n  \ng2_graph"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#undetstand-the-visual-cues",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#undetstand-the-visual-cues",
    "title": "FishEye Knowldge Graph: Identify Temporal Patterns of individual entities and between entities",
    "section": "3.1 Undetstand the Visual Cues",
    "text": "3.1 Undetstand the Visual Cues\n\n\n\n\n\n\n\n\nVisual Cue\nSample Image\nWhat it means\n\n\n\n\nEdges\n\n\n\n\n- Arrow head\n\nRefers to the direction in which shipment was made, A sends goods to B.\n\n\n- Edge width\n\nThicker edge means there were more transactions between A and D than between A and B.\n\n\n- Edge color\n\nDifferent colored edges mean different seafood products (based on HS Code) were transacted between A and B, and A and D.\n\n\nNodes\n\n\n\n\n- Color\n\nEntities that supply seafood do not usually re-sell them. Hence, most nodes were either a supplier or buyer. Bright green refers to suppliers with higher out-degrees and most would sell more fishery products by weight. At the other spectrum is the bright red nodes which relates bigger buyer with higher in-degrees. Again, most buyers (not all) would purchase more fishery products by weight.\n\n\n- Label\n\nThe label in the node is a reference for the entity involved. Only more active nodes (based on in-degree and out-degree centrality) are labelled.\n\n\n- Edge Color\n\nA node with a pink ring indicates it is a re-seller, and possibly a transshipping entity.\n\n\n\nFor each year, we created 3 functions to:\n\nPlot the network graph\nCompute the number of nodes and links involved\nTreemaps which encode the number of transaction (size) and quantities involved (by color intensity) for In-degree and Out-degree nodes\n\n\n\nShow the code\n# Function to create network based on a given year and plot title\n# Centrality measures for nodes are computed based on the year's network. Hence, these measures are not constant year on year\n\ncreate_graph <- function(data, range, title) {\n  \n  set.seed(123)\n  \n  g <- data %>%\n    #as_tbl_graph() %>%\n    activate(edges) %>%\n    filter(year %in% range) %>%\n    activate(nodes) %>%\n    mutate(\n        in_deg_centrality = round(centrality_degree(weights = weight, mode = \"in\", loops = FALSE),3),\n        out_deg_centrality = round(centrality_degree(weights = weight, mode = \"out\", loops = FALSE),3),\n        out_deg_closeness = round(centrality_closeness(weights=weight,mode='out',normalized = TRUE),3),\n        in_deg_closeness = round(centrality_closeness(weights=weight,mode='in',normalized = TRUE),3),\n        between_centrality = round(centrality_betweenness(weights = weight, directed = T),3)\n        ) %>%\n    mutate(\n      in_deg_norm = ifelse(in_deg_centrality == 0, 0, (in_deg_centrality - min(in_deg_centrality)) / (max(in_deg_centrality) - min(in_deg_centrality)) + 1),\n      out_deg_norm = ifelse(out_deg_centrality == 0, 0, (1 - (out_deg_centrality - min(out_deg_centrality)) / (max(out_deg_centrality) - min(out_deg_centrality))))\n    ) %>%\n    mutate(combined_in_out_deg = in_deg_norm + out_deg_norm,\n           bet_ind = ifelse(between_centrality >0, 1,0)) %>%\n    mutate(row_id = ifelse((combined_in_out_deg>=1.2) | (combined_in_out_deg <=0.8),row_id,\"\")) \n  \n  Isolated <- which(degree(g) == 0)\n  g <- delete.vertices(g, Isolated)\n  \n  g %>%\n    as_tbl_graph() %>%\n    ggraph(layout = 'nicely') +\n    geom_edge_link(aes(), alpha = 0.5) +\n    geom_edge_fan(\n      aes(color = short_desc),\n      arrow = arrow(length = unit(2, 'mm')),\n      end_cap = circle(3.3, 'mm'),\n      start_cap = circle(3, 'mm')\n    ) +\n    geom_node_circle(aes(fill = combined_in_out_deg,r = 0.3, colour = bet_ind),alpha=0.8) + \n    scale_fill_gradient2(low = \"#20E620\", mid = \"#666666\", high = \"#E00000\", midpoint = 1) +\n    scale_color_gradient(low = \"#666666\", high = \"pink\")+\n    geom_node_text(aes(label = row_id, size=0.5), colour = \"black\") +\n    theme_graph() +\n    theme(legend.position = \"none\") +\n    labs(title = title) +\n      coord_fixed()\n}\n\n\n\n\nShow the code\n# Function to compute the number of nodes and links\n\ncal_node_edges <- function(data, range) {\n  # Convert tbl_graph to igraph object\n  \n  g <- data %>%\n    #as_tbl_graph() %>%\n    activate(edges) %>%\n    filter(year %in% range)\n  \n  Isolated <- which(degree(g) == 0)\n  igraph_obj <- delete.vertices(g, Isolated)\n  \n  # Convert tbl_graph object to igraph\n  #igraph_obj = as.igraph(g)\n  \n  # Compute the number of nodes and edges\n  num_nodes <- vcount(igraph_obj)\n  num_edges <- ecount(igraph_obj)\n  \n  # Print the results\n  df <- data.frame(Num_of_Nodes = num_nodes, Num_of_Edges = num_edges)\n  \n  print(df)\n}\n\n\n\n\nShow the code\n# Create a function to create treemaps for in and out degree\n\ntreemaps <- function(data, range) {\n\n# Prepare the data\npivot_table <- data %>%\n  activate(edges)%>%\n  as.tibble() %>%\n  filter(year %in% range) %>%\n  group_by(from,to) %>%\n  summarise(count = n(),\n            sum_weight=sum(sum_weight)/1000) %>%\n  arrange(desc(sum_weight)) %>%\n  ungroup()\n\n  # Plot map for the suppliers\n  Suppliers <- treemap(pivot_table,\n          index=c(\"from\"),\n          vSize=\"count\",\n          vColor=\"sum_weight\",\n          type = \"value\",\n          title= paste(\"Entities That Supplied Fishery Products in\",range),\n          title.legend = \"Weight of Fishery Products Sold (in tonnes)\"\n          )\n  \n  # Plot map for the buyers\n  Buyers <- treemap(pivot_table,\n          index=c(\"to\"),\n          vSize=\"count\",\n          vColor=\"sum_weight\",\n          type = \"value\",\n          palette = \"-RdGy\",\n          title=paste(\"Entities That Acquired Fishery Products in\",range),\n          title.legend = \"Weight of Fishery Products Acquired (in tonnes)\"\n          )\n\n}"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#temporal-analysis-of-individuals-and-between-entities",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#temporal-analysis-of-individuals-and-between-entities",
    "title": "FishEye Knowldge Graph: Identify Temporal Patterns of individual entities and between entities",
    "section": "3.2 Temporal Analysis of Individuals and between entities",
    "text": "3.2 Temporal Analysis of Individuals and between entities\n\n2028202920302031203220332034\n\n\n\n\nShow the code\nrange <- c(2028)\ndata <- subgraph_obj\n\ncreate_graph(data,range,\"2028: 4 Main Groups Formed by Big\\nBuyers ID 28,46,128,132 were Observed\") +\n  labs(subtitle=\"A big buyer was typically surrounded by a few smaller suppliers\",\n       caption = \"Unconnected entities have been removed \" )\n\n\n\n\n\n\n\nShow the code\ncal_node_edges(data,range)\n\n\n  Num_of_Nodes Num_of_Edges\n1           84          145\n\n\nShow the code\ntreemaps(data,range)\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nrange <- c(2029)\n\ncreate_graph(data,range,\"2029: Big Buyers ID 128 and 132 were closer\\nas they were connected by Tier 2 Supplier ID 129\") +\n  labs(subtitle=\"Re-seller ID 84 and 151 were transacting actively as indicated by their high in-degree\",\n       caption = \"Unconnected entities have been removed \" )\n\n\n\n\n\n\n\nShow the code\ncal_node_edges(data,range)\n\n\n  Num_of_Nodes Num_of_Edges\n1           96          176\n\n\nShow the code\ntreemaps(data,range)\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nrange <- c(2030)\n\ncreate_graph(data,range,\"2030: Supplier ID 141 Sold More and Formed\\nIts Own Cluster\") +\n  labs(subtitle=\"A bigger supplier could also attract its share of  buyers\",\n       caption = \"Unconnected entities have been removed \" )\n\n\n\n\n\n\n\nShow the code\ncal_node_edges(data,range)\n\n\n  Num_of_Nodes Num_of_Edges\n1           93          150\n\n\nShow the code\ntreemaps(data,range)\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nrange <- c(2031)\n\ncreate_graph(data,range,\"2031: Supplier ID 139 Sold Much More Than Before\\nwhile Supplier ID 141 Scaled Down its Business\") +\n  labs(subtitle=\"The increase in ID 139's supplies correlates with the increase in ID 132's purchase\\nRe-seller ID 119 joined the network\",\n       caption = \"Unconnected entities have been removed \" )\n\n\n\n\n\n\n\nShow the code\ncal_node_edges(data,range)\n\n\n  Num_of_Nodes Num_of_Edges\n1           95          166\n\n\nShow the code\ntreemaps(data,range)\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nrange <- c(2032)\n\ncreate_graph(data,range,\"2032: Supplier ID 139 and Buyer ID 132\\nDoniminated and Took Centre-stage in the Network\") +\n  labs(subtitle=\"Bigger Supplier ID 95 which had around since 2028 left the network\\nRe-seller ID 84 disappeared and new re-sellers ID 32 and 144 joined\",\n       caption = \"Unconnected entities have been removed \" )\n\n\n\n\n\n\n\nShow the code\ncal_node_edges(data,range)\n\n\n  Num_of_Nodes Num_of_Edges\n1           89          134\n\n\nShow the code\ntreemaps(data,range)\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nrange <- c(2033)\n\ncreate_graph(data,range,\"2033: Resellers ID 32 and 144 Moved to the Centre\\nof the Network Trading Actively with Various Entities\")+\n  labs(subtitle=\"In contrast to what we observed in 2028, we have a big supplier (ID 139) surrounded\\nby a few smaller buyers\",\n       caption = \"Unconnected entities have been removed \" )\n\n\n\n\n\n\n\nShow the code\ncal_node_edges(data,range)\n\n\n  Num_of_Nodes Num_of_Edges\n1           97          155\n\n\nShow the code\ntreemaps(data,range)\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nrange <- c(2034)\n\ncreate_graph(data,range,\"2034: The 4 Main Buyer Groups Were Still Around Although\\nSome Major Players Have Changed\") +\n  labs(subtitle=\"Re-seller ID 32, which was active in the previous year, left the network\\nRe-seller ID 84 became active in the network again\")\n\n\n\n\n\n\n\nShow the code\ncal_node_edges(data,range)\n\n\n  Num_of_Nodes Num_of_Edges\n1           92          152\n\n\nShow the code\ntreemaps(data,range)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations:\n\n\n\n\nThe 4 main buyer groups (or clusters in the networ), lead by ID 28, 46, 128, 132), are probably well-established seafood importers supplying to the Oceanus.\nLarger buyers attract larger suppliers and will also acquire from smaller fisheries and vessels.\nRe-sellers ID 32 and 84 joined the network, traded actively with many parties and then left the network shortly. ID 84 was dormant in Year 2032 and 2033 and became active again in Year 2034. Given that re-sellers are often associated with transshipping entities and that non-compliant entities often close and open to avoid detection, it is worth further examining and assessing if their trade had been legitimate."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#main-players-in-each-business-group",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#main-players-in-each-business-group",
    "title": "FishEye Knowldge Graph: Identify Temporal Patterns of individual entities and between entities",
    "section": "4.1 Main players in each business group",
    "text": "4.1 Main players in each business group\nThe top 5 sellers in the network based on their out-degree centrality are:\n\n\nShow the code\nn <- 5\n\nnodes_active <- subgraph_obj %>%\n  as_tbl_graph() %>%\n  activate(nodes) %>%\n  mutate(\n        in_deg_centrality = round(centrality_degree(weights = weight, mode = \"in\", loops = FALSE),3),\n        out_deg_centrality = round(centrality_degree(weights = weight, mode = \"out\", loops = FALSE),3),\n        out_deg_closeness = round(centrality_closeness(weights=weight,mode='out',normalized = TRUE),3),\n        in_deg_closeness = round(centrality_closeness(weights=weight,mode='in',normalized = TRUE),3),\n        between_centrality = round(centrality_betweenness(weights = weight, directed = T),3)\n        ) %>%\n  mutate(\n      in_deg_norm = ifelse(in_deg_centrality == 0, 0, (in_deg_centrality - min(in_deg_centrality)) / (max(in_deg_centrality) - min(in_deg_centrality)) + 1),\n      out_deg_norm = ifelse(out_deg_centrality == 0, 0, (1 - (out_deg_centrality - min(out_deg_centrality)) / (max(out_deg_centrality) - min(out_deg_centrality))))\n    ) %>%\n    mutate(combined_in_out_deg = in_deg_norm + out_deg_norm) %>%\n  as_tibble() \n\n\nbigger_suppliers_cnt <- nodes_active %>%\n  arrange(desc(out_deg_centrality)) %>%\n  select(row_id,id, shpcountry, rcvcountry, out_deg_centrality) %>%\n  rename(Name = id,\n         ID = row_id) %>%\n  top_n(n,wt = out_deg_centrality)\n  \nkable(bigger_suppliers_cnt) %>%\n  kable_styling(full_width = FALSE) %>%\n  add_header_above(c(\"Table 3: Top 5 Suppliers by Frequency\" = 5))\n\n\n\n\n \nTable 3: Top 5 Suppliers by Frequency\n  \n    ID \n    Name \n    shpcountry \n    rcvcountry \n    out_deg_centrality \n  \n \n\n  \n    139 \n    Seashell Seekers LLC Delivery \n    Marebak \n    unknown \n    859 \n  \n  \n    95 \n    Mar del Norte United \n    Zawalinda \n    Oceanus \n    570 \n  \n  \n    126 \n    Rajasthan  Marine sanctuary Underwater \n    Oceanus \n    Oceanus \n    381 \n  \n  \n    27 \n    Black Sea Anchovy Pic Wharf \n    Vesperanda \n    Oceanus \n    269 \n  \n  \n    124 \n    Portuguese Sea Bass Ltd. Liability Co \n    Faraluna \n    Oceanus \n    264 \n  \n\n\n\n\n\nThe top 5 sellers in the network based on their in-degree centrality are:\n\n\nShow the code\nbigger_buyers_cnt <- nodes_active %>%\n  arrange(desc(in_deg_centrality)) %>%\n  select(row_id,id, shpcountry, rcvcountry, in_deg_centrality)  %>%\n  rename(Name = id,\n       ID = row_id) %>%\n  top_n(n,wt = in_deg_centrality)\n\nkable(bigger_buyers_cnt) %>%\n  kable_styling(full_width = FALSE) %>%\n  add_header_above(c(\"Table 4: Top 5 Buyers by Frequency\" = 5))\n\n\n\n\n \nTable 4: Top 5 Buyers by Frequency\n  \n    ID \n    Name \n    shpcountry \n    rcvcountry \n    in_deg_centrality \n  \n \n\n  \n    132 \n    Sailors and Surfers Incorporated Enterprises \n    Puerto Sol \n    Oceanus \n    1767 \n  \n  \n    128 \n    Rift Valley fishery Inc \n    Puerto del Mar \n    Oceanus \n    1647 \n  \n  \n    28 \n    Black Sea Tuna Sagl \n    Nalakond \n    Oceanus \n    771 \n  \n  \n    46 \n    David Ltd. Liability Co Forwading \n    Coralada \n    Oceanus \n    543 \n  \n  \n    37 \n    Coral Cove BV Delivery \n    Quornova \n    Oceanus \n    420 \n  \n\n\n\n\n\nThe top 5 re-sellers in the network based on their betweenness centrality are:\n\n\nShow the code\nbigger_resell_cnt <- nodes_active %>%\n  arrange(desc(between_centrality)) %>%\n  select(row_id,id, shpcountry, rcvcountry, between_centrality)  %>%\n  rename(Name = id,\n       ID = row_id) %>%\n  top_n(n,wt = between_centrality)\n\nkable(bigger_resell_cnt) %>%\n  kable_styling(full_width = FALSE) %>%\n  add_header_above(c(\"Table 5: Top 5 Re-sellers by Connectivity Score\" = 5))\n\n\n\n\n \nTable 5: Top 5 Re-sellers by Connectivity Score\n  \n    ID \n    Name \n    shpcountry \n    rcvcountry \n    between_centrality \n  \n \n\n  \n    84 \n    Ltd. Liability Co Corp \n    Coralmarica \n    Oceanus \n    13 \n  \n  \n    32 \n    Bu yu wang AG \n    Zawalinda \n    Oceanus \n    11 \n  \n  \n    109 \n    Ocean Oasis S.A. de C.V. Transport \n    Thessalandia \n    Oceanus \n    11 \n  \n  \n    119 \n    Playa Azul Sp Shipping \n    Arreciviento \n    Oceanus \n    8 \n  \n  \n    144 \n    Spanish Anchovy CJSC Marine \n    Merigrad \n    Oceanus \n    5 \n  \n  \n    159 \n    b√°i su≈ç wƒõn l√∫ S.p.A. \n    Syrithania \n    Oceanus \n    5 \n  \n\n\n\n\n\nWe will take 2 players from the each group to understand their business relationship and trading patterns through an interactive network.\nThe following visual cue will help us identify their role in the network.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nDon‚Äôt forget to mouse over the nodes and edges to see the tooltips for more info!\n\n\n\n\nShow the code\n# Prepare the edge data set\nedges_aggregated <- subgraph_obj %>%\n  as_tbl_graph() %>%\n  activate(edges) %>%\n  as_tibble() %>%\n  mutate(title = paste('Count = ',weight,'<br>HSCODE =', short_desc),\n         value = weight)\n         #value = weight, label = paste(short_desc)) \n\n  \n\n# define grpuping based on the value of combined_in_out_degree\n\ncut_breaks <- c(0, 0.6, 0.9,1.1, 1.4,2)\ncut_labels <- c('1 High Out-Degree','2 Medium Out-Degree','3 Low Degree','4 Medium In-Degree','5 High In-Degree' )\n\nnodes_active2 <- nodes_active[,-1] %>%\n  mutate(categories=cut(combined_in_out_deg, breaks = cut_breaks, labels = cut_labels, include.lowest = TRUE)) %>%\n  mutate(categories = ifelse(between_centrality > 0, \"6 Resell\\\\Tranship\", as.character(categories))) %>%\n  # Requirement of visNetwork to name grouping column as such\n  rename(group = categories) %>%\n  mutate(id = row_number()) %>%\n  mutate(title = paste(id,label,\"<br>Rcv Ctry =\", rcvcountry,'<br>Shp Ctry =',shpcountry))\n  \n# Plot the intereactive graph\nvisNetwork(nodes_active2,\n           edges_aggregated,\n          main = \"Transaction graph grouped by Deg centrality intervals\",\n           height = \"500px\", width = \"100%\") %>%\n  visIgraphLayout(layout = \"layout_nicely\") %>%\n  #visNodes(shape = 'dot', value = 'pagerank') %>%\n  visEdges(arrows = 'to',\n           smooth = list(enables = TRUE,\n                         type= 'continuous'),\n           shadow = FALSE,\n           dash = FALSE) %>%\n  visOptions(highlightNearest = list(enabled = T, degree = 1, hover = T),\n             nodesIdSelection = TRUE,\n             selectedBy = \"group\") %>%\n  visGroups(\n    groupname = '1 High Out-Degree',\n    color = \"#20E620\")  %>%\n  visGroups(\n    groupname = '2 Medium Out-Degree',\n    color = \"#B6D7A8\") %>%\n  visGroups(\n    groupname = '3 Low Degree',\n    color = \"#666666\") %>%\n  visGroups(\n    groupname = '4 Medium In-Degree',\n    color = \"#EA9999\")  %>%\n  visGroups(\n    groupname = '5 High In-Degree',\n    color = \"#E00000\")  %>%\n  visInteraction(hideEdgesOnDrag = TRUE) %>%\n  visLegend(enabled=F) %>%\n  visLayout(randomSeed = 123)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#the-bigger-sellers",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#the-bigger-sellers",
    "title": "FishEye Knowldge Graph: Identify Temporal Patterns of individual entities and between entities",
    "section": "4.2 The Bigger Sellers",
    "text": "4.2 The Bigger Sellers\nBig Seller 1: 139, Seashell Seekers LLC Delivery (Seashell)\nBusiness relationship patterns: Supplied fishery products to customers of various sizes, including big buyers such as Sailors and Surfers and Rift Valley fishery Inc.\n(Image 1: Direct Trading Network of Seashell)\n\nSeashell is a large overseas supplier of various seafood products selling to businesses in Oceanus. As shown in the temporal analysis, Seashell (ID 139) became a significant supplier in the Year 2031 and grew over the years. This corroborates with the EDA observation that there was a surge in transactions in Year 2031.\nVolume of Sales (in kg) to customers over the 7 years are as follows:\n\n\nShow the code\nbiz <- 'Seashell Seekers LLC Delivery'\nid <- 139\n\n# Identity unique customers\nunique_customers <- edges_aggregated %>%\n  filter(from == id) %>%\n  select(from, to) %>%\n  inner_join(nodes_active2,select(id,label),by= c('to'='id')) %>%\n  select(label) %>%\n  unique()\n  \n# Identity relevant transactions of customers\nedges_customers <- mc2_edges_fish %>%\n  filter(source == biz)%>%\n  filter(target %in% unique_customers$label) \n\n# Prepare data for plotting heatmap\nheatmap <- edges_customers %>%\n  mutate(month = floor_date(arrivaldate, unit = \"month\")) %>%\n  group_by(target, month) %>%\n  summarise(weight = n(),\n            sum_weight_ton = sum(weightkg)/1000) %>% \n  arrange(desc(target)) %>%\n  ungroup() %>%\n  mutate(target2 = ifelse(nchar(target)>40, substr(target, 1, 40),target))\n\ndt_from <- \"2028-01-01\"\ndt_to <- '2034-12-31'\n\n# Plot heatmap\nplot <- ggplot(heatmap, aes(x = month, y = reorder(target2, sum_weight_ton), fill = sum_weight_ton)) +\n  geom_tile(colour=\"black\", size=0.1, show.legend=F,\n            aes(text = paste(\"Name:\", target,\n                              \"<br>Month:\", month,\n                              \"<br>Count:\", weight,\n                              \"<br>Weight(ton):\",sum_weight_ton))) +\n  scale_fill_distiller(palette=\"RdPu\",\n                       direction = 1) +\n  scale_y_discrete(name=\"\", expand=c(0,0))+\n  scale_x_date(name=\"Arrival Date\", \n               limits=as.Date(c(dt_from, dt_to)), \n               expand=c(0,0),date_breaks = \"1 year\", \n               date_labels = \"%Y-%m\") +\n  labs(title= paste0(biz,\"'s \", 'Sales by Weight (in tonnes)'),\n       subtitle=paste0('Breakdown by Buyers from ', \n                       dt_from, ' to ',dt_to)) +\n  theme_classic()  \n\n\n#  theme(panel.background = element_rect(fill = \"gray\"))\n\n\nggplotly(plot, tooltip = 'text')\n\n\n\n\n\n\nBig Seller 2: 27,Black Sea Anchovy Pic Wharf (Black Sea)\nBusiness relationship patterns: Supplied and traded solely with one big buyer\n(Image 2: Direct Trading Network of Black Sea)\n\nBlack Sea‚Äôs sole customer in the network was Rift Valley fishery Inc, trading in shrimps and prawns. Black Sea fits the profile of a large fishing vessel or entity, supplying its catches to Rift Valley directly."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#the-bigger-buyers",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#the-bigger-buyers",
    "title": "FishEye Knowldge Graph: Identify Temporal Patterns of individual entities and between entities",
    "section": "4.3 The Bigger Buyers",
    "text": "4.3 The Bigger Buyers\nBig Buyer 1: 132,Sailors and Surfers Incorporated Enterprises (Sailors)\nBusiness relationship patterns: Sourced fishery produce from a large pool of suppliers\n(Image 3: Direct Trading Network of Sailors)\n\nSailors has been one of the 4 major buyers since Year 2028. It bought various seafood produce, including salmons and hakes. Given these, Sailors is likely a company based in Oceanus importing seafood and buying from local fisheries.\nVolume of supplies (in kg) from sellers over the 7 years are as follows:\n\n\nShow the code\nbiz <- 'Sailors and Surfers Incorporated Enterprises'\nid <- 132\n\n# Identity unique customers\nunique_customers <- edges_aggregated %>%\n  filter(to == id) %>%\n  select(c(from, to)) %>%\n  inner_join(nodes_active2,select(id,label),by= c('from'='id')) %>%\n  select(c(label)) %>%\n  unique()\n  \n# Identity relevant transactions of customers\nedges_customers <- mc2_edges_fish %>%\n  filter(target == biz)%>%\n  filter(source %in% unique_customers$label) \n\n# Prepare data for plotting heatmap\nheatmap_buy <- edges_customers %>%\n  mutate(month = floor_date(arrivaldate, unit = \"month\")) %>%\n  group_by(source, month) %>%\n  summarise(weight = n(),\n            sum_weight_ton = sum(weightkg)/1000) %>% \n  ungroup() %>%\n  mutate(source2 = ifelse(nchar(source)>40, substr(source, 1, 40),source)) \n\ndt_from <- \"2028-01-02\"\ndt_to <- '2034-12-31'\n\n# Plot heatmap\nplot <- ggplot(heatmap_buy, aes(x = month, y = reorder(source2,sum_weight_ton), fill = sum_weight_ton)) +\n  geom_tile(colour=\"White\", show.legend=F,\n            aes(text = paste(\"Name:\", source,\n                              \"<br>Month:\", month,\n                              \"<br>Count:\", weight,\n                              \"<br>Weight(ton):\",sum_weight_ton))) +\n  scale_fill_distiller(palette=\"Gn\",\n                       direction = 1) +\n  scale_y_discrete(name=\"\", expand=c(0,0))+\n  scale_x_date(name=\"Arrival Date\", \n               limits=as.Date(c(dt_from, dt_to)), \n               expand=c(0,0),date_breaks = \"1 year\", \n               date_labels = \"%Y-%m\") +\n  labs(title= paste0(biz,\"'s \", 'Purchases by Weight (in tonnes)'),\n       subtitle=paste0('Breakdown by Sellers from ', \n                       dt_from, ' to ',dt_to)) +\n  theme_classic() +\n  theme(legend.position='top',\n        plot.title.position=\"plot\",\n        axis.text.y=element_text(colour=\"Black\",size=5)) +\n  theme(panel.background = element_rect(fill = \"gray\"))\n\nggplotly(plot, tooltip = 'text')\n\n\n\n\n\n\nFrom the heatmap, we can see that Sailors ramped up its purchases in Year 2031 when it started to buy from The Sea Lion S.A. de C.V. Carriers and Mar de la Felicidad Co.\n\nBig Buyer 2: 46,David Ltd.¬†Liability Co Forwading (David)\nBusiness relationship pattern: Bought from a variety of sources, but mainly from smaller suppliers\n(Image 4: Direct Trading Network of David)\n\nAs seen from the image, David sourced its goods and products from a various suppliers, but mainly from smaller suppliers. It traded mainly in fishes as opposed to other types of seafood. From its business name, David was a forwarder; however, this was not apparent from network.\nVolume of supplies (in kg) from sellers over the 7 years are as follows:\n\n\nShow the code\nbiz <- 'David Ltd. Liability Co Forwading'\nid <- 46\n\n# Identity unique customers\nunique_customers <- edges_aggregated %>%\n  filter(to == id) %>%\n  select(c(from, to)) %>%\n  inner_join(nodes_active2,select(id,label),by= c('from'='id')) %>%\n  select(c(label)) %>%\n  unique()\n  \n# Identity relevant transactions of customers\nedges_customers <- mc2_edges_fish %>%\n  filter(target == biz)%>%\n  filter(source %in% unique_customers$label) \n\n# Prepare data for plotting heatmap\nheatmap_buy <- edges_customers %>%\n  mutate(month = floor_date(arrivaldate, unit = \"month\")) %>%\n  group_by(source, month) %>%\n  summarise(weight = n(),\n            sum_weight_ton = sum(weightkg)/1000) %>% \n  ungroup()  %>%\n  mutate(source2 = ifelse(nchar(source)>40, substr(source, 1, 40),source))\n\ndt_from <- \"2028-01-02\"\ndt_to <- '2034-12-31'\n\n# Plot heatmap\nplot <- ggplot(heatmap_buy, aes(x = month, y = reorder(source2,sum_weight_ton), fill = sum_weight_ton)) +\n  geom_tile(colour=\"White\", show.legend=F,\n            aes(text = paste(\"Name:\", source,\n                              \"<br>Month:\", month,\n                              \"<br>Count:\", weight,\n                              \"<br>Weight(ton):\",sum_weight_ton))) +\n  scale_fill_distiller(palette=\"Gn\",\n                       direction = 1) +\n  scale_y_discrete(name=\"\", expand=c(0,0))+\n  scale_x_date(name=\"Arrival Date\", \n               limits=as.Date(c(dt_from, dt_to)), \n               expand=c(0,0),date_breaks = \"1 year\", \n               date_labels = \"%Y-%m\") +\n  labs(title= paste0(biz,\"'s \", 'Purchases by Weight (in tonnes)'),\n       subtitle=paste0('Breakdown by Sellers from ', \n                       dt_from, ' to ',dt_to)) +\n  theme_classic() +\n  theme(panel.background = element_rect(fill = \"gray\"))\n\nggplotly(plot, tooltip = 'text')\n\n\n\n\n\n\nFor David, we will notice that prior to Year 2031, its major suppliers were Norwegian King Crab Dockyard, Rift Valley fishery OJSC and Chhattisgarh S.A. de C.V. The trade relationship with these 3 suppliers stopped from Year 2031 and since then, David appeared to have scaled down its operations and not buying as much as before."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#the-re-sellers-with-high-connectivity-scores",
    "href": "Take-Home_Ex/Take-Home_Ex02/Take-Home_Ex2.html#the-re-sellers-with-high-connectivity-scores",
    "title": "FishEye Knowldge Graph: Identify Temporal Patterns of individual entities and between entities",
    "section": "4.4 The Re-sellers with High Connectivity Scores",
    "text": "4.4 The Re-sellers with High Connectivity Scores\nReseller 1: 84,Ltd.¬†Liability Co Corp (Ltd)\nBusiness relationship pattern: Traded in various seafood produce as an re-seller\n(Image 5: Direct Trading Network of Ltd)\n\nLtd did not trade with with primary suppliers directly. Instead, it played an intermediary role, acquiring its goods and products from 2 other re-sellers before supplying them to end buyers. In the temporal analysis, we noted that Ltd existed from the community in Year 2032, 3 years after its appearance in Year 2029. Thereafter, it became active again in Year 2034.\nRe-seller 2: 144, Spanish Anchovy CJSC Marine (Spanish)\nBusiness relationship pattern: Trader of various seafood produce\n(Image 6: Direct Trading Network of Spanish)\n\nUnlike Ltd, Spanish sourced its goods direct from suppliers acting much like a seasfood trading company or brokerage. Seafood trading companies act as intermediaries between fishing boats, seafood suppliers, and customers. They purchase seafood directly from fishing boats locally and may also source seafood products from overseas suppliers. In addition, seafood trading companies often cater to a wide range of customers, including wholesalers, retailers, restaurants, and other food service providers."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex3.html",
    "href": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex3.html",
    "title": "Countering illegal fishing: Identify anomalies in the business groups",
    "section": "",
    "text": "(First Published: Jun 18, 2023)\n(Authorities have a challenging task of enforcing on IUU as many fishing companies and owners deliberately exploit a variety of complex company structures, with individual companies based across many jurisdictions, to own and run their operations.)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex3.html#setting-the-scene",
    "href": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex3.html#setting-the-scene",
    "title": "Countering illegal fishing: Identify anomalies in the business groups",
    "section": "1.1 Setting the Scene",
    "text": "1.1 Setting the Scene\nFishEye International, a non-profit focused on countering illegal, unreported, and unregulated (IUU) fishing, has been given access to an international finance corporation‚Äôs database on fishing related companies. In the past, FishEye has determined that companies with anomalous structures are far more likely to be involved in IUU (or other ‚Äúfishy‚Äù business). FishEye has transformed the database into a knowledge graph. It includes information about companies, owners, workers, and financial status. FishEye is aiming to use this graph to identify anomalies that could indicate a company is involved in IUU."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex3.html#our-task",
    "href": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex3.html#our-task",
    "title": "Countering illegal fishing: Identify anomalies in the business groups",
    "section": "1.2 Our Task",
    "text": "1.2 Our Task\nIn response to Question 1 of VAST Chaellenge 2023: Mini-Challenge 3, our task is to use visual analytics to identify anomalies in the business groups present in the given knowledge graph."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex3.html#import-and-extract-the-data",
    "href": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex3.html#import-and-extract-the-data",
    "title": "Countering illegal fishing: Identify anomalies in the business groups",
    "section": "2.2 Import and Extract the data",
    "text": "2.2 Import and Extract the data\nThe given data is an undirected knowledge graph provided in json format. It contains 2 sets of information - Nodes and Edges attributes .\n\nFirst, we imported the data and assigned it to a variable mc3.\n\n\n\nShow the code\nmc3 <- fromJSON(\"data/mc3.json\")\n\n\n\nNext, we extracted the nodes data frame from mc3.\n\nAt the same time, we applied distinct() to remove duplicate node records and rounded the revenue_omu values to the nearest whole unit so that it would be easier for us to work with the attribute given its small denomination .\n\n\nShow the code\n# Extract the nodes data\n\n# convert the fields to characters first to extract the information embedded as list\nmc3_nodes <- as_tibble(mc3$nodes) %>%\n  # mutate() and as.character() are used to convert the field data type from list to character\n  mutate(country = as.character(country),\n         id = as.character(id),\n         product_services = as.character(product_services),\n         revenue_omu = as.numeric(as.character(revenue_omu)),\n         type = as.character(type)) %>%\n  # Re-organise the columns\n  select(id,country,type,product_services,revenue_omu) %>%\n  # remove duplicate records\n  distinct() %>%\n  # omu is denominated in smaller currency units, so we will round all values to the nearest whole unit to make it easier to work with\n  mutate(revenue_omu = round(revenue_omu,0)) \n\n\n\nThen, we extracted the edges data frame from mc3.\n\n\n\nShow the code\n# Extract the edge data\nmc3_edges <- as_tibble(mc3$links) %>%\n  # remove the duplicates\n  distinct() %>%\n  #mutate() and as.character() are used to convert the field data type from list to character\n  mutate(source = as.character(source),\n         target = as.character(target),\n         type = as.character(type)) %>%\n  group_by(source, target, type) %>%\n  summarise(weight = n()) %>%\n  # Included to ensure self-links are excluded, although there was none found\n  filter(source!=target) %>%\n  ungroup\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nAlthough we included distinct() function to remove duplicate records in the codes above, there was no such records or self-links\nGrouping by source, target and type did not reduce the number of records, and the weight for all records show 1. This meant the edge information contain the relationships between the entities involved and might not connote the volume of transactions between them\n\n\n\nWe stored the extracted mc3 nodes and edges data frames in rds format for ease of subsequent retrieval. The following ‚Äúwrite‚Äù code lines need only be executed once. Thereafter we can reload the mc3_nodes and mc3_edges data frames for data wrangling.\n\n\nShow the code\n# write and load the mc3_nodes and edges dataframe. With this, we need not re-extract from the raw dataset subsequently\n\n#write_rds(mc3_nodes, \"data/mc3_nodes.rds\")\nmc3_nodes = read_rds(\"data/mc3_nodes.rds\")\n\n#write_rds(mc3_edges, \"data/mc3_edges.rds\")\nmc3_edges <- read_rds(\"data/mc3_edges.rds\")"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex3.html#data-preparation",
    "href": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex3.html#data-preparation",
    "title": "Countering illegal fishing: Identify anomalies in the business groups",
    "section": "2.3 Data Preparation",
    "text": "2.3 Data Preparation\n\n2.3.1 The Edges data frame\n\nWe started data perpetration by inspecting the mc3_edges data frame using skim()\n\n\n\nShow the code\nskim(mc3_edges)\n\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nmc3_edges\n\n\n\n\nNumber of rows\n\n\n24036\n\n\n\n\nNumber of columns\n\n\n4\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\ncharacter\n\n\n3\n\n\n\n\nnumeric\n\n\n1\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: character\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmin\n\n\nmax\n\n\nempty\n\n\nn_unique\n\n\nwhitespace\n\n\n\n\n\n\nsource\n\n\n0\n\n\n1\n\n\n6\n\n\n700\n\n\n0\n\n\n12856\n\n\n0\n\n\n\n\ntarget\n\n\n0\n\n\n1\n\n\n6\n\n\n28\n\n\n0\n\n\n21265\n\n\n0\n\n\n\n\ntype\n\n\n0\n\n\n1\n\n\n16\n\n\n16\n\n\n0\n\n\n2\n\n\n0\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\n\n\n\n\nweight\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÅ\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nThere was no field with missing value\nThere were 2 unique relationship types: Entity - Beneficial Owner (BO), Entity - Company Contact (CC). We assumed that the source entities were companies, the target entities were all individuals associated with the companies and their relationships were reflected under type column.\nThe source column has a maximum length of 700 characters, which was atypical of most entity names. This was examined in the next step.\n\n\n\n\nWe then examined records with lengthy text in the source column\n\nWe pulled out some records with more than 100 characters in the source column.\n\n\nShow the code\n# Set variable n for character limit \nn <- 100\n\n# filter such records\nfiltered_data <- mc3_edges %>%\n  filter(str_length(source) > n)\n \n\n# Inspect the filtered records\nkable(head(filtered_data, n=5)) %>%\n  kable_styling(full_width = FALSE) %>%\n  add_header_above(c(\"Table 1: Sample Records under the Source column with > 100 characters\" = 4))\n\n\n\n\n \nTable 1: Sample Records under the Source column with > 100 characters\n  \n    source \n    target \n    type \n    weight \n  \n \n\n  \n    c(\"1 Swordfish Ltd Solutions\", \"1 Swordfish Ltd Solutions\", \"Saharan Coast  BV Marine\", \"Olas del Sur Estuary\") \n    Daniel Reese \n    Company Contacts \n    1 \n  \n  \n    c(\"Adriatic Squid Ltd. Liability Co\", \"Brisa del Este Cargo Bonito\", \"Sea Harvest Marine conservation CJSC Marine\") \n    Angelica Wheeler \n    Beneficial Owner \n    1 \n  \n  \n    c(\"Adriatic Squid Ltd. Liability Co\", \"Brisa del Este Cargo Bonito\", \"Sea Harvest Marine conservation CJSC Marine\") \n    Shelly Strong \n    Company Contacts \n    1 \n  \n  \n    c(\"Ancla Azul –û–ê–û Holdings\", \"Ancla Azul –û–ê–û Holdings\", \"Ancla Azul –û–ê–û Holdings\", \"Playa de Arena Sagl\") \n    Jennifer Morales \n    Company Contacts \n    1 \n  \n  \n    c(\"Ancla del Este OJSC\", \"Irish Trout S.p.A. Carriers\", \"Irish Trout S.p.A. Carriers\", \"Irish Trout S.p.A. Carriers\", \"Irish Trout S.p.A. Carriers\", \"Irish Trout S.p.A. Carriers\") \n    Carlos Harvey \n    Company Contacts \n    1 \n  \n\n\n\n\n\nWe noticed that these rows contain list of entities in the source column, implying that there were records with many source entities to one single target entity. To flatten the records, we extracted and then converted such rows from the edge data frame to additional link records using the separate_rows() function to split each element in the embedded list into a separate row while repeating the values in other columns.\n\n\nShow the code\n# Extract records with lists in source column\nfiltered_data_list <- mc3_edges%>%\n  # filter records starting with 'c(\"' in the source column\n  filter(str_starts(source, '^c\\\\(\"')) %>%\n  # remove the first 2 character and last character of the source column\n  mutate(source = substr(source, 3, nchar(source) - 1)) %>%\n  # split each element in the list in source column to a new row\n  separate_rows(source, sep = \",\") %>%\n  # remove empty string at the start of the source columns\n  mutate(source = trimws(source)) %>%\n  # remove the opening and closing quotes from the source column\n  mutate(source = substr(source, 2, nchar(source) - 1))\n\n# Inspect the filtered records\nkable(slice(filtered_data_list, 3:6)) %>%\n  kable_styling(full_width = FALSE) %>%\n  add_header_above(c(\"Table 2: Extracted Edge records based on the 1 st record from Table 1\" = 4))\n\n\n\n\n \nTable 2: Extracted Edge records based on the 1 st record from Table 1\n  \n    source \n    target \n    type \n    weight \n  \n \n\n  \n    1 Swordfish Ltd Solutions \n    Daniel Reese \n    Company Contacts \n    1 \n  \n  \n    1 Swordfish Ltd Solutions \n    Daniel Reese \n    Company Contacts \n    1 \n  \n  \n    Saharan Coast  BV Marine \n    Daniel Reese \n    Company Contacts \n    1 \n  \n  \n    Olas del Sur Estuary \n    Daniel Reese \n    Company Contacts \n    1 \n  \n\n\n\n\n\nAfter we had flattened the records with embedded list in the source column, we combined these processed edge records with those records which did not have any embedded list in the source column originally.\n\n\nShow the code\n# Extract records which had list in their source column\nfiltered_data <- mc3_edges %>%\n  filter(str_starts(source, '^c\\\\(\"'))\n\n# Extact records which did not have  any list in the source column originally\nremaining_data <- mc3_edges %>%\n  anti_join(filtered_data)\n\n# Union remaining_data and desired_rows\nmc3_edges_flat <- bind_rows(remaining_data, filtered_data_list) %>%\n  # group to eliminate duplicate source, target, type records\n  group_by(source,target,type) %>%\n  mutate(weight=sum(weight)) %>%\n  ungroup() %>%\n  # remove repeated rows after grouping\n  distinct()\n\n\n\nNext, we visualised the count of edge records by type\n\n\n\nShow the code\nmc3_edges_flat %>%\n  group_by(type) %>%\n  summarise(count = n()) %>%\n  ggplot(aes(x = type, y = count)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\") +\n  geom_text(aes(label = format(count,big.mark=\",\")), vjust = -0.5) +\n  theme_minimal() +\n  labs(x = \"Edge Type\", y = \"No. of\\nTransactions\",\n       title = 'Count of Relationships by Type',\n       subtitle = \"Beneficial Owners accounted for the majority of the edge types\") +  # Add the subtitle\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5, hjust = 1)) +\n  ylim(0, 20000) \n\n\n\n\n\n\nWe prepared the nodes data frame using records from the edges data frame\n\n(i) We extracted and combined the entities listed in the source and target columns into an id column. For the target entities, we retained edge type information based on the provided edge records since it was the role of the target entity. For the the source entities, we defaulted their type as Company (this has been discussed in the observation note to Step 1 above). The process resulted in 35,386 unique (id, type) pairs.\n\n\nShow the code\n# Prepare the nodes information using the source and target information in the edge data frame\nid1 <- mc3_edges_flat %>%\n  select(source) %>%\n  mutate(type = \"Company\") %>%\n  rename(id = source)\n  \nid2 <- mc3_edges_flat %>%\n  select(target, type) %>%\n  rename(id = target)\n\n# Get unique nodes from source and target columns of edge records\nmc3_nodes_fr_edges <- rbind(id1, id2) %>%\n  distinct()\n\n\n(ii) Next, we left joined the mc3_nodes_fr_edges with the mc3_nodes data frame by id and type to get more information about the entities. The information was stored in the mc3_nodes_combined data frame.\n\n\nShow the code\n# Get unique nodes from source and target columns of edge records\nmc3_nodes_combined <- left_join(mc3_nodes_fr_edges, mc3_nodes, by = c(\"id\", \"type\"))\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nThe number of node records increased from 35,386 to 35,806. This meant that there were duplicate node records with same id and type (but have different country, revenue_omu or product_services data) in the mc3_nodes dataframe. We further treated these records in Section 2.3.2.\nThe left join step above also meant entity records from the mc3_nodes data frame that did not have a matching type would be excluded in our subsequent analysis.\n\n\n\n\nQuick visualisation of the network for nodes with high Betweenness Centrality\n\n(i) We computed centrality measures of the nodes. To limit the number of entities to be displayed, we extracted those with Betweenness Centrality scores of >= 100,000.\n\n\nShow the code\n# compute the centrality measures for nodes\nmc3_graph <- tbl_graph(nodes = mc3_nodes_combined,\n                       edges = mc3_edges_flat,\n                       directed = FALSE) %>%\n  mutate(betweenness_centrality = centrality_betweenness(),\n         closeness_centrality = centrality_closeness())\n\n# set random seed for consistency\nset.seed(123)\n\n# plot the graph\nmc3_graph %>%\n  # we only plot nodes with high betweeness_centrality\n  filter(betweenness_centrality >= 100000) %>%\nggraph(layout = \"fr\") +\n  geom_edge_link(aes(alpha=0.5)) +\n  geom_node_point(aes(\n    linewidth = betweenness_centrality,\n    alpha = 0.5)) +\n  scale_size_continuous(range=c(0.01,0.5))+\n  theme_graph() +\n  labs(title = \"Initial Network Plot based on Edge Data\")+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n2.3.2 The Nodes data frame\n\nStatus Check\n\nAfter performing Step 4(ii) of Section 2.3.1 above, there were entities in the mc3_nodes_combined data frame with > 1 record.\n\nWe aggregated information of entities with multiple records in the mc3_nodes_combined data frame such that there was only 1 node record per entity.\n\nThis was achieved by the following steps:\ni. For records with same id, type and country information\nWe concatenated the product_services information across different rows and sum up their revenue_omu information. This was on the assumption that a company only has 1 record per country.\nii. For records with the same id and type information\nWe concatenated the country information and also tracked the count of countries involved. We tracked companies with presence in multiple countries as this did not appear to be the norm for the data set.\n\n\nShow the code\n# Step 2(i)\n# Identify records with same id and type information\nresult <- mc3_nodes_combined %>% \n  filter(duplicated(mc3_nodes_combined[, c(\"id\", \"type\")]) | duplicated(mc3_nodes_combined[, c(\"id\", \"type\")], fromLast = TRUE))\n\n# Concatenate text values in product_services and sum revenue_omu for records with the same id, type, and country\nresult_same_coy <- result %>%\n  group_by(id, type, country) %>%\n  summarize(product_services = paste(product_services, collapse = \", \"),\n            revenue_omu = sum(revenue_omu)) %>%\n  ungroup()\n\n# Step 2(ii)\n# Arrange alphabetically in the country column, then concatenate text values for records with the same id, type\nresult2 <- result_same_coy %>%\n  arrange(country) %>%\n  group_by(id, type) %>%\n  summarize(country = paste(country, collapse = \", \"),\n            country_count = n(),\n            product_services = paste(product_services, collapse = \", \"),\n            revenue_omu = sum(revenue_omu))\n\n\n\nWe removed records with duplicate Ids from the mc3_nodes_combined data frame and combined the aggregated node records in Step 2 back with mc3_nodes_combined data frame. There were 35,386 unique (id,type) pairs, the same number we got from the m3_edge data frame in Step 4(i) of Section 2.3.1.\n\n\n\nShow the code\nmc3_nodes_cleaned <- mc3_nodes_combined %>%\n  anti_join(result) %>%\n  mutate(country_count = 1)\n  \nmc3_nodes_cleaned <- bind_rows(mc3_nodes_cleaned , result2)  \n\n\n\nFinally, we aggregated the entities by their Id, resulting in 34,422 unique entity records.\n\nThe following logic were applied during the aggregation for rows with same id:\n\nThe node type , country and product_services values were concatenated across rows\ntype_count was created to track the number of node types\nThe country_count and revenue_omu values were sum-up across rows\n\n\n\nShow the code\nmc3_nodes_cleaned <- mc3_nodes_cleaned %>%\n  arrange(type,country,product_services) %>%\n  group_by(id) %>%\n  summarize(type = paste(type,collapse =\", \"),\n            type_count = n(),\n            country = paste(country, collapse = \", \"),\n            country_count = max(country_count),\n            product_services = paste(product_services, collapse = \", \"),\n            revenue_omu = sum(revenue_omu)) %>%\n  # clean up the country and product_services column after concatenation\n  mutate(country = ifelse(country %in% c('NA',\"NA, NA\"), NA, country),\n         product_services = ifelse(product_services %in% c('NA',\"NA, NA\"), NA, product_services))\n\n\n\nWe visualised the count of node records by type\n\n\n\nShow the code\nmc3_nodes_cleaned %>% \n  group_by(type) %>%\n  summarise(count = n()) %>%\n  ggplot(aes(x = type, y = count)) +\n  geom_bar(stat = \"identity\", fill='lightblue') +\n  geom_text(aes(label = format(count,big.mark=\",\")), vjust = -0.5) +\n  theme_minimal() +\n  labs(x = \"Node Type\", y = \"No. of\\nEntities\",\n       title = 'Count of Nodes by Type',\n       subtitle ='There were individuals who had roles as Benenficial Owners and Company Contacts') +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5, hjust = 1)) +\n  ylim(0, 15000) \n\n\n\n\n\n\nWe also visualised the Top 8 countries where the nodes were domiciled in\n\n\n\nShow the code\nmc3_nodes_cleaned %>%\n  filter(!is.na(country)) %>%\n  group_by(country) %>%\n  summarise(count = n()) %>%\n  top_n(10) %>%\n  arrange(desc(count)) %>%\n  ggplot(aes(x = reorder(country, -count), y = count)) +\n  geom_bar(stat = \"identity\", fill = '#3498db') +\n  geom_text(aes(label = format(count,big.mark=\",\")), vjust = -0.5) +\n  theme_minimal() +\n  labs(x = \"Country\", y = \"No. of\\nNodes\",\n       title = 'Count of Nodes by Country',\n       subtitle = 'Oceanus, Marebak and ZH were the top 3 countries where most nodes domiciled') +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5, hjust = 1)) \n\n\n\n\n\n\nWe noticed that values for the product_services column were very different across rows and decided to check the frequency of each value.\n\n\n\nShow the code\n# Get the freq count of records by product_services column \nfreq_count_pdt_svcs <- mc3_nodes_cleaned %>% \n  group_by(product_services) %>%\n  summarise(count = n()) %>%\n  arrange(desc(count))\n\ndatatable(freq_count_pdt_svcs, class = \"compact\", options = list(pageLength = 5), \n              caption = \"Table 3: Frequency Count of Values in product_services Column\",\n              rownames = FALSE)\n\n\n\n\n\n\n\n\nWe saw from the there was a large number of the records with empty strings . ‚ÄúUnknown‚Äù or ‚ÄúUnknown, Unknown‚Äù. The last category was due to the concatenation we did earlier to aggregate the information for nodes with multiple records. We re-coded ‚ÄúUnknown‚Äù and ‚ÄúUnknown, Unknown‚Äù to NA.\n\n\n\nShow the code\nmc3_nodes_cleaned2 <- mc3_nodes_cleaned %>%\n  mutate(product_services = trimws(product_services))%>%\n  mutate(product_services = replace(product_services, product_services == \"Unknown\", NA)) %>%\n  mutate(product_services = replace(product_services, product_services == \"Unknown, Unknown\", NA))"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex3.html#categorise-entities-by-their-products-and-services",
    "href": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex3.html#categorise-entities-by-their-products-and-services",
    "title": "Countering illegal fishing: Identify anomalies in the business groups",
    "section": "2.4 Categorise Entities by their products and services",
    "text": "2.4 Categorise Entities by their products and services\nThere were still round 2,170 different types of product and services that the entities offered. The information within the product_services column were highly varied and we performed text sensing to identify entities in the fishing industry, which was the industry of interest for us.\n\nTokenised the words used under the product_services column\n\n\n\nShow the code\ntoken_nodes <- mc3_nodes_cleaned2 %>%\n  unnest_tokens(word, \n                product_services)\n\n\n\nRemoved common words that did not have much differentiating power\n\n\n\nShow the code\n# Inclded our own list of stopwords\nstopwords = c(NA,'products','services','unknown','related','canned')\n\n# Remove stopwords\nstopwords_removed <- token_nodes %>% \n  # Remove default stopwords from tidytext\n  anti_join(stop_words) %>%\n  # Exclude common words that we defined above\n  filter(!word %in% stopwords)\n\n\n\nPerformed lemmatisation to convert all words to their root form\n\n\n\nShow the code\nstopwords_removed <- stopwords_removed %>%\n  mutate(word = lemmatize_words(word))\n\n\n\nVisualised the frequency count of the words\n\n\n\nShow the code\nstopwords_removed %>%\n  count(word, sort = TRUE) %>%\n  top_n(15) %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(x = word, y = n)) +\n  geom_col(stat = \"identity\",fill=\"#6baed6\") +\n  geom_text(aes(label = n), vjust = 0.4, hjust=-0.1) +\n  xlab(NULL) +\n  coord_flip() +\n      labs(x = \"Count\",\n      y = \"Unique words\",\n      title = \"Count of unique words found in product_services field\",subtitle = 'Words relating to fish and seafood produce were prominent') +\n  theme_minimal() +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5, hjust = 1))\n\n\n\n\n\n\nWe generated a word_count table and exported it to manually identify keywords of various industries for manual categorization. This helped to ensure that we could capture words related to the fishing industry more accurately.\n\n\n\nShow the code\n# Append a cummulative count column into the stopwords_removed data frame\nword_count <-stopwords_removed %>%\n  count(word, sort = TRUE) %>%\n  mutate(cumulative_percent = cumsum(n) / sum(n) * 100)\n\n# Export the word_count list to csv\n# write_csv(word_count,\"data/word_count.csv\")\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nThere were 4,700 unique words\nThe first 700 unique words covered 70% of all the text used under the product_servcies column.\n\n\n\n\nThrough manual inspection of the first 700 most frequent words, we identified the following list of seafood related terms\n\n‚Äôfish, seafood, salmon, shrimp, fillet, crab, tuna, shellfish, squid, cod, clam, pollock, lobster, octopus, oyster, scallop, sockeye, crustacean, mackerel, roe, mollusk, mussel, groundfish, cuttlefish, rockfish, caviar, eel, haddock, crayfish, sardine, seabass, catfish, finfish, mollusc, trout‚Äù\n\nWe also identified some keywords to categorise other businesses.\n\nAlthough this approach was mechanical and not the most comprehensive, it would help the us gain a quick understanding of the business activities of the entity. The 9 main business activities identified were: (i) seafood, (ii) fruits, vegetables and other food, (iii) machinery and equipment, (iv) consumer goods, (v) meat and dairy, (vi) freight and transport, (vii) energy and fuel, (viii) metals, and (ix) chemical and plastic.\n\n\nShow the code\n# Group the prossed words in Step 3 by id\n# The tokenised words, separated by commas, for each entity is now appended in product_services2 column\nprocessed_text <- stopwords_removed %>%\n  group_by(id) %>%\n  summarize(product_services2 = paste(word, collapse = \", \"))\n\n# left join mc3_nodes_cleaned2 with the processed text\nmc3_nodes_cleaned2 <- left_join(mc3_nodes_cleaned2, processed_text, by = c(\"id\"))\n\n# create list of keywords to identify various businesses\n\nseafood <- as.list(c('fish', 'seafood', 'salmon', 'shrimp', 'fillet', ' crab', ' tuna', 'shellfish', 'squid', ' cod', 'clam', 'pollock', 'lobster', 'octopus', 'oyster', 'scallop', 'sockeye', 'crustacean', 'mackerel', ' roe', 'mollusk', 'mussel', 'groundfish', 'cuttlefish', 'rockfish', 'caviar', ' eel', 'haddock', 'crayfish', 'sardine', 'seabass', 'catfish', 'finfish', 'mollusc'))\n\nfruits <- as.list(c('food', 'fruit', 'vegetable', 'vegetables', 'tomato',' gelatine','gelatin','salt','coffee'))\n\nmachinery <- as.list(c('equipment', 'machine', 'machinery', 'automation'))\nconsumer <- as.list(c('accessory', 'fabric', 'adhesive', 'paper', 'clothing', 'stationery', 'toy', 'yarn', 'dress', 'pencil', 'shirt', 'pens',' \nfootwear','workwear',   'apparel','footwear','shoe','sandal','bag','grocery'))\n\nmeat <-as.list(c('meat', 'steak', 'milk', 'poultry', 'beef', 'chicken', 'pork', 'lamb', 'dairy'))\nfreight <- as.list(c('freight', 'transportation', 'logistic', 'cargo', 'transport', 'warehouse', 'shipping', 'truck', 'trucking', 'forwarding','boat charter','automobile'))\n\nenergy <- as.list(c('oil', 'gas', 'energy','electricity'))\n\nmetals <- as.list(c('metal', 'steel', 'aluminum','aluminium', 'copper', 'alloy', 'metals'))\n\nchemicals <- as.list(c('chemical','plastic','ammonia'))\n\n# Create a new column 'category' and assign the entity to a industry based on the keywords defined above\nmc3_nodes_cleaned3 <- mc3_nodes_cleaned2 %>%\n  mutate(category = \"other\") %>%\n  mutate(category = if_else(str_detect(product_services2, paste(fruits, collapse = \"|\")), \"fruits, vegetables and other food\", category)) %>%\n  mutate(category = if_else(str_detect(product_services2, paste(machinery, collapse = \"|\")), \"machinery and equipment\", category)) %>%\n  mutate(category = if_else(str_detect(product_services2, paste(consumer, collapse = \"|\")), \"consumer goods\", category)) %>%\n    mutate(category = if_else(str_detect(product_services2, paste(meat, collapse = \"|\")), \"meat and dairy\", category)) %>%\n    mutate(category = if_else(str_detect(product_services2, paste(freight, collapse = \"|\")), \"freight and transport\", category)) %>%\n      mutate(category = if_else(str_detect(product_services2, paste(energy, collapse = \"|\")), \"energy and fuel\", category)) %>%\n      mutate(category = if_else(str_detect(product_services2, paste(metals, collapse = \"|\")), \"metals\", category)) %>%\n        mutate(category = if_else(str_detect(product_services2, paste(chemicals, collapse = \"|\")), \"chemical and plastic\", category)) %>%\n  # Seafood category was placed last to override earlier categorisation\n  mutate(category = if_else(str_detect(product_services2, paste(seafood, collapse = \"|\")), \"seafood\", category))\n\n\n\nPerformed a frequency code to find out the number of entities categorised as seafood\n\n\n\nShow the code\nfreq_industry <- mc3_nodes_cleaned3 %>%\n  filter(!is.na(category)) %>%\n  group_by(category) %>%\n  summarise(count = n(),\n            total_revenue =format(sum(revenue_omu, na.rm=TRUE), big.mark=\",\"),\n            avg_revenue = format(round(mean(revenue_omu, na.rm=TRUE),0), big.mark=\",\")\n            ) %>%\n  arrange(desc(count)) %>%\n  ungroup()\n\n\n# Inspect the filtered records\nkable(freq_industry) %>%\n  kable_styling(full_width = FALSE) %>%\n  add_header_above(c(\"Table 4: Frequency Count and Revenue of Entities By Industry\" = 4))\n\n\n\n\n \nTable 4: Frequency Count and Revenue of Entities By Industry\n  \n    category \n    count \n    total_revenue \n    avg_revenue \n  \n \n\n  \n    seafood \n    694 \n    225,860,850 \n    388,077 \n  \n  \n    other \n    567 \n    556,995,019 \n    1,218,807 \n  \n  \n    consumer goods \n    331 \n    211,739,917 \n    778,456 \n  \n  \n    freight and transport \n    187 \n    421,253,488 \n    2,632,834 \n  \n  \n    fruits, vegetables and other food \n    160 \n    33,876,949 \n    245,485 \n  \n  \n    chemical and plastic \n    156 \n    279,194,590 \n    2,215,830 \n  \n  \n    metals \n    152 \n    59,611,880 \n    518,364 \n  \n  \n    machinery and equipment \n    142 \n    84,695,891 \n    730,137 \n  \n  \n    energy and fuel \n    64 \n    114,295,659 \n    2,241,091 \n  \n  \n    meat and dairy \n    60 \n    33,166,401 \n    637,815 \n  \n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\nSeafood industry had the most number of entities and the avearge revenue per entity was the second lowest (after fruits, vegetables and other food) among the 9 industry sectors featured."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex3.html#business-groups-with-high-network-diameter",
    "href": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex3.html#business-groups-with-high-network-diameter",
    "title": "Countering illegal fishing: Identify anomalies in the business groups",
    "section": "3.1 Business Groups with high network diameter",
    "text": "3.1 Business Groups with high network diameter\n\nTo identify the relevant business groups with at least 1 seafood entity, we extracted the seafood_nodes and edge records of seafood entities for network examination\n\n\n\nShow the code\n# extract the entities categorised as seafood\nseafood_entities <- mc3_nodes_cleaned3 %>%\n  filter(category=='seafood')\n\n# extract the edge link records related to these seafood entities\nseafood_edges <- mc3_edges_flat[mc3_edges_flat$source %in% seafood_entities$id | mc3_edges_flat$target %in% seafood_entities$id, ]\n\n# extract the seafood_nodes records using the edge information\nid1 <- seafood_edges %>%\n  select(source) %>%\n  rename(id = source)\n  \nid2 <- seafood_edges %>%\n  select(target) %>%\n  rename(id = target)\n\n# Get unique nodes from source and target columns of edge records\n# left join with the mc3_nodes_cleaned3 dataset to get the attributes for the nodes\nseafood_nodes <- rbind(id1, id2) %>%\n  distinct() %>%\n  left_join(mc3_nodes_cleaned3,by=c('id'))\n\n\n\nNext, we prepared the tbl_graph object for network computation and plotting\n\n\n\nShow the code\nseafood_graph<- tbl_graph(nodes=seafood_nodes,\n                          edges = seafood_edges,\n                          directed = FALSE)\n\nseafood_graph\n\n\n# A tbl_graph: 3369 nodes and 2721 edges\n#\n# An unrooted forest with 648 trees\n#\n# A tibble: 3,369 √ó 9\n  id         type  type_count country country_count product_services revenue_omu\n  <chr>      <chr>      <int> <chr>           <dbl> <chr>                  <dbl>\n1 2 Limited‚Ä¶ Comp‚Ä¶          1 Marebak             1 Canning, proces‚Ä¶          NA\n2 9 Charter‚Ä¶ Comp‚Ä¶          1 Marebak             1 Fish and fish p‚Ä¶       36658\n3 Adair S.A‚Ä¶ Comp‚Ä¶          1 Mawazam             1 Frozen cephalop‚Ä¶       33309\n4 Adams Gro‚Ä¶ Comp‚Ä¶          1 ZH                  1 A range of fish‚Ä¶        9056\n5 Adriatic ‚Ä¶ Comp‚Ä¶          1 Nalako‚Ä¶             1 Fish and seafoo‚Ä¶      113379\n6 Adriatic ‚Ä¶ Comp‚Ä¶          1 Nalako‚Ä¶             1 Canned seafood ‚Ä¶       16452\n# ‚Ñπ 3,363 more rows\n# ‚Ñπ 2 more variables: product_services2 <chr>, category <chr>\n#\n# A tibble: 2,721 √ó 4\n   from    to type             weight\n  <int> <int> <chr>             <int>\n1     1   695 Beneficial Owner      1\n2     1   696 Company Contacts      1\n3     1   697 Company Contacts      1\n# ‚Ñπ 2,718 more rows\n\n\n\n\n\n\n\n\nObservations:\n\n\n\n\nThere were 648 subgraphs within the seafood_graph\n\n\n\n\nWe derived the degree, betweenness centrality measures and subgraph group id of each node\n\n\n\nShow the code\nseafood_graph <- seafood_graph %>% \n  activate(nodes) %>%\n  mutate(degree_centrality = centrality_degree(),\n         betweenness_centrality = centrality_betweenness()) %>%\n  # get subgraph id for the node\n  mutate(group_id = components(seafood_graph)$membership)\n\n\n\nWe visualised the distribution of the subgraph size\n\n\n\nShow the code\nfreq_count <- seafood_graph %>%\n  activate(nodes) %>%\n  as.tibble() %>%\n  arrange(group_id) %>%\n  group_by(group_id) %>%\n  summarise(count = n()) \n\n\nggplot(freq_count, aes(x = count)) +\n  geom_histogram(binwidth = 1, fill = \"steelblue\", color = \"white\") +\n  # The follow chunk is disabled as the labels cluttered the plot\n  geom_text(\n    stat = \"count\",\n    aes(label = ..count..),\n    vjust = -0.5,\n    color = \"black\",\n    size = 2.5\n  ) +\n  labs(x = \"Number of entities in the Subgraph (or Subgraph Size)\",\n      y = \"Count by Subgraph size\",\n      title = \"Distribution of Subgraph Size\",\n      subtitle = 'Majority of Subgraphs had fewer than 10 entities') +\n  theme_minimal()\n\n\n\n\n\n\nWe computed the network diameter of the 648 business groups in the Seafood Network\n\nTo have a layered business structure within a subgraph, we will need the sub-graph‚Äôs network diameter to have a minimum value of 2 and above. The larger the diameter, the more complex the structure of the business group is.\n\n\nShow the code\n# get the list of group_ids for each subgraph in the network\nsubgraph_ids <- unique(freq_count$group_id)\n\n# assign a diameter_list to store the results\ndiameter_list <- list()\n\n# for each group_id\nfor (x in subgraph_ids) {\n  \n  # filter the relevant nodes for the group id\n  nodes <- seafood_graph %>%\n    activate(nodes) %>%\n    as.tibble() %>%\n    filter(group_id == x)\n  \n  # extract the relevant edge records   \n  edges <- seafood_edges %>%\n    filter(source %in% nodes$id| target %in% nodes$id)\n  \n\n  # construct the subgraph\n  subgraph <- tbl_graph(nodes = nodes, \n                        edges = edges, \n                        directed = FALSE)\n  \n  # calculate the network diamter\n  diameter <- with_graph(subgraph, graph_diameter())\n  \n  # append output to list\n  diameter_list[[as.numeric(x)]] <- diameter\n  \n}\n\n# \nnetwork_diameter <- tibble(\n  group_id = subgraph_ids,\n  network_diameter = unlist(diameter_list)\n)\n\n\n\nWe plotted the distribution of the network diameter of the 648 groups\n\n\n\nShow the code\ndiameter <- network_diameter %>%\n  count(network_diameter, sort = TRUE) %>%\n  ggplot(aes(x = as.factor(network_diameter), y = n)) +\n  geom_col(fill='#08519c') +\n  xlab(NULL) +\n  labs(x = \"Network Diameter\",\n       y = \"Count\",\n       title = \"Distribution of Subgraphs by Network Diameter\",\n       subtitle = '4 Subgraphs had a diameterof 6 and above') +\n  scale_x_discrete(breaks = 1:7) +\n  geom_text(aes(label = n), vjust = -0.5) +\n  theme_minimal() + \n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5, hjust = 1))\n\ndiameter\n\n\n\n\n\n\nWe combined the 2 sets of information obtained from Steps 5 and 6 on the subgraphs and displayed each group in a Jitterplot with Subgraph Size vs Network Diameter\n\n\n\nShow the code\n# Combine the subgraph size and the network_diamter into a single data frame\nratio <- freq_count %>%\n  inner_join(network_diameter,by=c('group_id')) %>%\n  mutate(size_to_diameter_ratio = round(count/network_diameter,2)) %>%\n  arrange(desc(size_to_diameter_ratio))\n\n# Display the information in a jitterplot\ngg <- ggplot(ratio, \n            aes(x = network_diameter, \n                y = count,\n                colour = network_diameter,\n                tooltip = paste0('group id ',group_id,\n                                 '<br> Group Size: ',count,\n                                 '<br> Network Diameter: ',network_diameter),\n                data_id = group_id)\n            )+\n  geom_jitter_interactive() +\n  xlab(\"Network Diameter\") +\n  ylab(\"Subgraph Size\") +\n  ggtitle(\"Jitterplot of Subgraph Size vs Network Diameter\") +\n  theme_minimal() +\n  scale_x_continuous(breaks = c(1,2,3,4,5,6,7)) +\n  theme(legend.position = \"none\")\n \ngirafe(                                  \n  ggobj = gg,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)\n\n\n\n\n\n\n\n\n\n\n\n\nObservations:\n\n\n\n\n2 subgraphs, at the extreme ends of the jitterplot, grabbed our attention:\n\nGroup ID 102: Short Network Diameter (2), High Subsgraph Size (61)\nGroup ID 210: Long Network Diameter (7), relatively High Subgraph Szie (40)\n\n\n\n\nBefore we proceeded to review the network diagram of these subgraphs, we created 3 functions to help us with our analysis:\n(i) Function to extract the nodes and edges records of entities for a given subgraph.\n\n\nShow the code\ncreateNE_by_Group <- function(groupid) {\n\n  \n  relevant_entities <- seafood_graph %>%\n    activate(nodes) %>%\n    as.tibble()%>%\n    arrange(id) %>%\n    filter(group_id == groupid)\n\n\n  relevant_edges <- mc3_edges_flat %>%\n    filter(source %in% relevant_entities$id| target %in% relevant_entities$id) %>%\n    rename(from = source) %>% \n    rename(to = target) %>% \n    mutate(title = type)\n\n  # extract the seafood_nodes records using the edge information\n  Cid1 <- relevant_edges %>%\n    select(from) %>%\n    rename(id = from)\n  \n  Cid2 <- relevant_edges %>%\n    select(to) %>%\n    rename(id = to)\n\n  # Get unique nodes from source and target columns of edge records\n  # left join with the mc3_nodes_cleaned3 dataset to get the attributes for the nodes\n  relevant_nodes <- rbind(Cid1, Cid2) %>%\n    distinct() %>%\n    left_join(mc3_nodes_cleaned3,by=c('id')) %>%\n    arrange(id) %>%\n    mutate(label = id) %>%\n    mutate(group = type) %>% \n    mutate(title = paste('id = ',id, \"<br>Country =\",country, '<br>Entity Type =',type,'<br>Revenue (omu) =',revenue_omu,'<br>Biz Category =',category,'<br>Biz Activity =',product_services))\n  \n  title = paste(\"Subsidiary Group ID\",groupid)\n  \n  list(relevant_edges = relevant_edges, relevant_nodes = relevant_nodes, title = title)\n   \n}\n\n\n(ii) Function to first identify the subgraph group id of a given entity and then extract the nodes and edges entity‚Äôs subgraph\nThis generates the nodes and edges data frames of a 2-hop network graph of the given entity which would facilitate our examination of the entity.\n\n\nShow the code\ncreateNE_by_id <- function(entityid) {\n  \n  # obtain the subgraph group id of the entity\n  groupid <- seafood_graph %>% \n  activate(nodes) %>%\n  filter(id == entityid) %>%\n    select(group_id) %>%\n    as.tibble()\n  \n  # assign the group id to a variable\n  groupid <- groupid$group_id[1]\n  \n  # pass the group id into the createNE_by_Group() function to generate the subgraph\n  output <- createNE_by_Group(groupid)\n  \n  # return the nodes and edges data frames for graphing\n  list(relevant_edges = output$relevant_edges,relevant_nodes = output$relevant_nodes,title = entityid)\n   \n}\n\n\n(ii) Function to plot an interactive Network graph with the given nodes and edges data\n\n\nShow the code\ncreateGraph <- function(r_nodes , r_edges,title) {\n  visNetwork(nodes = r_nodes,   # Visualize the nodes\n             edges = r_edges,   # Visualize the edges\n             main = paste(\"Network graph of\", title),\n             height = \"500px\", width = \"100%\") %>%\n    visIgraphLayout(layout = \"layout_nicely\") %>%\n    visEdges(smooth = list(enables = TRUE, type = 'straightCross'),  # Customize the appearance of edges\n             shadow = FALSE,\n             dash = FALSE) %>%\n    visGroups(groupname = \"Company\", shape = \"icon\", \n              icon = list(code = \"f1ad\", size = 75)) %>%  # Define a group with icon shape for companies\n    visGroups(groupname = \"Beneficial Owner\", shape = \"icon\", \n              icon = list(code = \"f007\", color = \"red\")) %>%  # Define a group with red icon shape for beneficial owners\n    visGroups(groupname = \"Company Contacts\", shape = \"icon\", \n              icon = list(code = \"f007\", color = \"blue\")) %>%  # Define a group with blue icon shape for company contacts\n    visGroups(groupname = \"Beneficial Owner, Company Contacts\", shape = \"icon\", \n              icon = list(code = \"f007\", color = \"purple\")) %>%  # Define a group with purple icon shape for both beneficial owners and company contacts\n    addFontAwesome() %>%  # Add Font Awesome icons to the visualization\n    visOptions(highlightNearest = list(enabled = TRUE, degree = 1, hover = TRUE),  # Enable highlighting of nearest nodes on hover\n               nodesIdSelection = TRUE,\n               selectedBy = \"group\") %>%  # Enable node selection by group\n    visInteraction(hideEdgesOnDrag = TRUE) %>%  # Hide edges while dragging nodes\n    visLegend() %>%  # Display legend\n    visLayout(randomSeed = 123)  # Set a random seed for consistent layout\n}\n\n\n\n3.1.1 Subgraph Group ID 102\nFirst, we plotted the interactive network of Subgraph Group 102.\n\n\nShow the code\nresult <- createNE_by_Group(102)\ncreateGraph(result$relevant_nodes,result$relevant_edges, result$title) \n\n\n\n\n\n\nCongo had many BOs and we wondered if this was normal for companies in the edge data frame that was provided. As such, we derived the 95% Quantile Interval (QI) for number of BOs that companies had and noticed that 95% of the companies only had 1 to10 BOs.\n\n\nShow the code\navg_BOs <- mc3_edges_flat %>%\n  filter(type == 'Beneficial Owner') %>%\n  group_by(source) %>%\n  summarise(BO_count = n()) \n  \n# Calculate the 95% quantile interval\nlower_ci <- quantile(avg_BOs$BO_count, 0.025)\nupper_ci <- quantile(avg_BOs$BO_count, 0.975)\n\n# Create the histogram\nggplot(avg_BOs, aes(x = BO_count)) +\n  geom_histogram(binwidth = 1, fill = \"lightblue\", color = \"black\") +\n  labs(title = \"Histogram of BO_count\", x = \"BO_count\", y = \"Frequency\") +\n  theme_minimal() +\n  annotate(\"text\", x = mean(avg_BOs$BO_count), y = 10,\n           label = paste(\"95% QI:\", lower_ci, \"-\", upper_ci), hjust = -0.1, vjust=-5, color = \"red\") +\n  xlim(0, 40)\n\n\n\n\n\nAnomaly 1: Multiple Beneficial Owners surrounding a Seafood Company, Congo Rapids Ltd.¬†Corporation (Congo)\nCogno had 58 BOs, way beyond what a typical company had. One reasonable explanation, judging from the the range of products and services that Congo offered, could be that it was a large scale co-operation with many subsidiaries. However, it could also be a red-flag üö© for IUU as this would allow vessel owners to shop and select the vessel flag state of a BO that would facilitate their illicit activities, such as gaining access to fisheries resources which are reserved for vessels owned by a resident BO.\n\n\n3.1.2 Subgraph Group ID 210\nSimilarly, we started our review with the network diagram of Subgraph Group 210.\n\n\nShow the code\nresult <- createNE_by_Group(210)\ncreateGraph(result$relevant_nodes,result$relevant_edges, result$title)\n\n\n\n\n\n\nGroup ID 210 also shared the same observation as Group ID 102 of having multiple BOs for its seafood companies (Kerala Market SRL Wave and The Salted Pearl - Oyj Marine conservation), although the extent of having multiple BOs was not as serious as Congo‚Äôs. Both companies had 16 BOs.\nHowever, what struck us was the ownership structure of 2 entities in the network, The Salted Pearl - Oyj Marine conservation and SeaScape Foods Ltd Freight.\nAnomaly 2: Multiple and Same Set of Individuals Associated with 2 Companies\nIn the image on the right below, we saw that The Salted Pearl - Oyj Marine Conservation and SeaScape Foods Ltd Freight sharing the same set of 16 BOs and 1 Company Contact. This appeared to be a deliberate arrangement to perpetuate a scheme üö© .\nA more typical arrangement for 2 associated companies would be that they may share some common owners or contact persons, but not exactly the same set of individuals, similar to the setup for 2 seafood companies on the left image below.\n\n\n\n\n\n\n\nCommon: 2 companies may share some common owners or contact persons, but not exactly the same set of individuals\nUncommon: Multiple and Same Set of Individuals for 2 companies"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex3.html#companies-with-extraordinary-revenue",
    "href": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex3.html#companies-with-extraordinary-revenue",
    "title": "Countering illegal fishing: Identify anomalies in the business groups",
    "section": "3.2 Companies with ‚Äúextraordinary‚Äù revenue",
    "text": "3.2 Companies with ‚Äúextraordinary‚Äù revenue\nCompanies exist to create profit for their owners. A larger company would generate more revenue and in our context, we used the number of BOs as a proxy for the company size. Let us check out the distribution of revenue generated per BO for each company in the Seafood Network.\n\n\nShow the code\n# find out the number of BOs (Company Contacts excluded) for every company\nBO_for_company <- seafood_edges %>%\n  filter(!type == 'Company Contacts') %>%\n  group_by(source) %>%\n  summarise(BO_count = n())\n\n# compute revenue (in OMU) per BO per company\nrevenue_per_BO <- seafood_nodes %>%\n  inner_join(BO_for_company, by=c(\"id\"=\"source\")) %>%\n  # for this analysis, we will impute revenue_omu as 1 so that they can be considered in the analysis\n  mutate(revenue_per_BO = ifelse(is.na(revenue_omu), 1,round(revenue_omu/BO_count,0)),\n         # given the wide range of revenue, we applied log on the revenue_per_BO\n         log_revenue = log(revenue_per_BO))\n\n# Boxplot of the log_revenue of the Revenue per BO per company\np <- revenue_per_BO %>%\n  ggplot(aes(x=type, y = log_revenue)) +\n  geom_boxplot_interactive(\n            aes(fill=category,\n                group = paste(type,category),\n                data_id = category,\n                tooltip = after_stat({\n                                      paste0(\n                                        \"Quartile (in Revenue_OMU):\",\n                                        \"\\nQ1: \", exp(.data$ymin),\n                                        \"\\nQ3: \", exp(.data$ymax),\n                                        \"\\nmedian: \", round(exp(.data$middle),0)\n                                        )\n                                      }),\n                outlier.tooltip = paste(\"id:\", id,\n                                    \"<br>Country:\", country,\n                                    \"<br>Revenue:\", revenue_omu,\n                                    \"<br>No. of BOs:\", BO_count,\n                                    \"<br>Revenue Per BO:\", revenue_per_BO,\n                                    \"<br>Biz Category:\", category,\n                                    \"<br>Biz Acitivity:\",product_services)\n               ),\n              outlier.colour='red') +\n              theme(axis.title.x=element_blank(),\n                  axis.text.x=element_blank(),\n                  axis.ticks.x=element_blank()) +\n              theme_minimal() + \n              coord_flip() +\n  ggtitle(\"Boxplot of Revenue Per BO Per Seafood Company\") +\n  theme(axis.title.y = element_text(angle = 0, vjust = 0.5, hjust = 1))\n          \n\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)                                        \n\n\n\n\n\n\n\nMousing over the red outlier points on the right side of the plot, we have 3 entities with exceptionally high Revenue per BO. They were Baker and Sons, Barron LLC, Caracola del Mar NV Family. Let‚Äôs take a look at their networks.\n\n3.2.1 Network Graph Comparison of Top 3 Entities with Highest Revenue Per BO\nWe plotted the network graph for Baker and Sons below and did the same for the other 2 entities.\n\n\nShow the code\nresult<-createNE_by_id(\"Baker and Sons\")\ncreateGraph(result$relevant_nodes,result$relevant_edges,result$title)\n\n\n\n\n\n\nAnomaly 3: BO of a seafood company, Baker and Sons, having a number of other unrelated businesses\nOther than owning another company (Faroe Islands Ltd Express) associated with seafood, we noticed one BO of Baker and Sons, Michael Johnson, owned a number of other companies that were unrelated to the seafood businesses. This structure was in contrast with the other 2 high revenue per BO entities, Barron LLC and Caracola del Mar NV Family as shown below, where the BOs only had 1 company to manage.\n\n\n\n\n\n\n\nBarron LLC\nCaracola del Mar NV Family\n\n\n\n\n\n\n\n\n\nWhile having a BO with numerous businesses unrelated to fishing does not directly imply involvement in illegal fishing, there‚Äôs a possibility that such unrelated business could be a front to launder illicit gains from IUU or act as a false front for other illegal activities üö©. Fisheye could consider analysing the entity‚Äôs financial transactions, including sources of funding, payment flows,to look for any suspicious patterns, such as large amounts of cash transactions or frequent transfers to offshore jurisdictions known for illegal fishing activities.\n\n\n3.2.2 Network Graph of Company with Low Revenue Per BO\nUsing the revenue per BO information that we generated earlier, we charted a scatter plot using Log Revenue Vs Number of BOs of Companies to identify companies with very low revenue per BO. Such entities would appear very close to the x-axis of the plot.\n\n\nShow the code\nplot_ly(data = revenue_per_BO %>%\n          # Excluded the 3 entities with high revenue per BO to see the plot\n          filter(log_revenue<=13.41), \n        x = ~BO_count, \n        y = ~revenue_omu,\n        text = ~paste(\"Entity:\", id,     \n                      \"<br>Country:\", country,\n                      \"<br>Revenue:\", revenue_omu,\n                      \"<br>No. of BOs:\", BO_count,\n                      \"<br>Revenue Per BO:\", revenue_per_BO,\n                      \"<br>Biz Category:\", category),\n                        \n        color = ~revenue_per_BO, \n        colors = colorRampPalette(c(\"blue\", \"red\"))(nrow(revenue_per_BO)))  %>%\n      layout(title = 'Log Revenue vs No. Of BOs of Companies')\n\n\n\n\n\n\nWe would expect companies with lower revenue to be of smaller operating scale and had fewer BOs as seen from the clustered points at the point of origin in the scatter plot above. One such exception was Rufiji Delta GmbH Express (Rufiji).\nAnomaly 4: Companies with little revenue and yet a substantial number of BOs.\nRufiji was an ‚Äúexceptional‚Äù entity among the companies in the seafood sector, with a remarkably high number of BOs. It had 39 BOs while its revenue was only OMU 6137. This meant that each BO received an average of OMU 157, which was far below the median revenue per BO of OMU 12,429 for the sector (check out the boxplot above). The reasons for this unusual situation were not clear from the data.\nLet‚Äôs check out the subgraph of Rufiji.\n\n\nShow the code\nresult<-createNE_by_id(\"Rufiji Delta  GmbH Express\")\ncreateGraph(result$relevant_nodes,result$relevant_edges,result$title)\n\n\n\n\n\n\n\nFrom the network, only 7 out of 39 BOs had an alternate source of income from another company. There was no other data that discloses how the rest of the BOs earned their living. This raised some questions about their economic situation and sustainability, and if there were commercially justifiable reasons for having a large pool of BOs üö© ."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex3.html#business-groups-with-presence-in-multiple-countries",
    "href": "Take-Home_Ex/Take-Home_Ex03/Take-Home_Ex3.html#business-groups-with-presence-in-multiple-countries",
    "title": "Countering illegal fishing: Identify anomalies in the business groups",
    "section": "3.3 Business Groups with Presence in Multiple Countries",
    "text": "3.3 Business Groups with Presence in Multiple Countries\nFor each business group, we computed the number of countries in which the entities in the group had presence in. Thereafter, we constructed a jitterplot using Subgraph Size vs Number of Countries that the business groups operated in.\n\n\nShow the code\n# Combine the countries for each business group\ncountry_count <- seafood_graph %>%\n  activate(nodes) %>%\n  as_tibble() %>%\n  group_by(group_id) %>%\n  summarise(country = paste(country, collapse = \", \"),\n            size = n()) %>%\n  ungroup()\n\n# Function to count unique words in a country column\nunique_words <- function(text) {\n  text_words <- str_split(text, \",\\\\s*\")[[1]]\n  text_words <- unique(text_words)\n  text_words <- text_words[text_words != \"NA\"]\n  unique_text_words <- unique(text_words)\n  return(unique_text_words)\n}\n\n# Derive the unique contries and number of unique countries for each group\ncountry_count <- country_count %>%\n  mutate(unique_countries = map(country, unique_words)) %>%\n  mutate(country_count = lengths(unique_countries))\n\n\n# Display the information in a jitterplot\ngg <- ggplot(country_count, \n            aes(x = country_count, \n                y = size,\n                colour = country_count,\n                tooltip = paste0('group id ',group_id,\n                                 '<br> Group Size: ',size,\n                                 '<br> Unique Countries Count: ',country_count,\n                                  '<br> Unique Countries: ', unique_countries),\n                data_id = group_id)\n            )+\n  geom_jitter_interactive() +\n  xlab(\"No of Unique Countries that the Business Group Operated In\") +\n  ylab(\"Subgraph Size\") +\n  ggtitle(\"Jitterplot of Subgraph Size vs Countries of Operation\") +\n  theme_minimal() +\n  scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10)) +\n  theme(legend.position = \"none\")\n \ngirafe(                                  \n  ggobj = gg,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)\n\n\n\n\n\n\nThe obvious outlier in the plot was Subgraph Group ID 29, which operated in 10 countries.\n\n3.3.1 Subgraph Group ID 29\nThe network graph for Subgraph Group ID 29 is as shown.\n\n\nShow the code\nresult <- createNE_by_Group(29)\ncreateGraph(result$relevant_nodes,result$relevant_edges, result$title)\n\n\n\n\n\n\n\nThe main entity in subgraph was Aqua Aura SE Marine life (Aqua) which had presence in 9 countries. It not only operated in multiple countries, it also had a high number of beneficial owners (36 BOs) and contact persons (10 company contacts). There was no revenue information on Aqua even though one country which it traded in was Ocenanus.\nAnomaly 5: Companies with presence in a high number of countries\nAqua was involved in wide range of business activities and hence, the large pool of individuals and high number of countries which it was associated with appeared legititmate. Nonetheless, business groups that operated in multiple countries and involving numerous individuals could create a complex and opaque structure, making it challenging to track and regulate the group‚Äôs fishing activities effectively. This structual complexity could be exploited to engage in illegal practices, such as evading regulations, concealing illegal fishing operations, or engaging in illicit activities along the seafood supply chain. As such, they could be of concern as well üö©."
  }
]